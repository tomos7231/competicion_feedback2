{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1661218651317,"user":{"displayName":"Tomoya Yanagi","userId":"11157828680567606809"},"user_tz":-540},"id":"k3QT3hl827yR","outputId":"d4a21f02-790f-4275-f650-c89dbbd00c73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Aug 23 01:37:30 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["! nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iri53t0J3Ma0"},"outputs":[],"source":["import os\n","\n","class Config:\n","    AUTHOR = \"wanwan7123\"\n","\n","    NAME = \"feedback-Exp042-essay-deberta-large\"\n","    MODEL_PATH = \"microsoft/deberta-large\"\n","    DATASET_PATH = []\n","\n","    COMPETITION = \"feedback-prize-effectiveness\"\n","    COLAB_PATH = \"/content/drive/MyDrive/DataAnalysis/competicion/competicion_feedback\" \n","    DRIVE_PATH = os.path.join(COLAB_PATH, AUTHOR)\n","\n","    api_path = \"/content/drive/MyDrive/kaggle.json\"\n","\n","    seed = 42\n","    num_fold = 5\n","    trn_fold = [0, 1, 2, 3, 4]\n","    batch_size = 4\n","    n_epochs = 5\n","    \n","    fc_dropout = 0.1\n","    weight_decay = 0.001\n","    beta = (0.9, 0.98)\n","    lr = 5e-6\n","    eval_steps = 499\n","    num_warmup_steps_rate = 0.01\n","    clip_grad_norm = None\n","    gradient_accumulation_steps = 1\n","    \n","    # GPU Optimize Settings\n","    gpu_optimize_config= {\n","        \"fp16\": True,\n","        \"freezing\": True,\n","        \"optim8bit\": True,\n","        \"gradient_checkpoint\": True\n","    }\n","\n","    upload_from_colab = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3920,"status":"ok","timestamp":1661218658562,"user":{"displayName":"Tomoya Yanagi","userId":"11157828680567606809"},"user_tz":-540},"id":"Y3qpAE-53Teb","outputId":"248f836f-8601-49f1-affa-3a8a33e280c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch==1.10 in /usr/local/lib/python3.7/dist-packages (1.10.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10) (4.1.1)\n"]}],"source":["import os\n","import re\n","import gc\n","import sys\n","import json\n","import time\n","import shutil\n","import joblib\n","import random\n","import requests\n","import warnings\n","warnings.filterwarnings('ignore')\n","from ast import literal_eval\n","from tqdm import tqdm\n","from pathlib import Path\n","from glob import glob\n","\n","import numpy as np\n","import pandas as pd\n","import scipy \n","import itertools\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import (\n","    StratifiedKFold, \n","    KFold, \n","    GroupKFold,\n","    StratifiedGroupKFold\n",")\n","from sklearn.metrics import log_loss\n","!pip install torch==1.10\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.utils.checkpoint import checkpoint\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zS5FvS83UY_"},"outputs":[],"source":["def setup(cfg):\n","    cfg.COLAB = 'google.colab' in sys.modules\n","    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    if cfg.COLAB:\n","        print('This environment is Google Colab')\n","\n","        # mount\n","        from google.colab import drive\n","        if not os.path.isdir('/content/drive'):\n","            drive.mount('/content/drive') \n","\n","        # pip install\n","        ! pip install transformers==4.16.2\n","        ! pip install tokenizers==0.11.6\n","        ! pip install transformers[sentencepiece]\n","\n","        # use kaggle api (need kaggle token)\n","        f = open(cfg.api_path, 'r')\n","        json_data = json.load(f) \n","        os.environ['KAGGLE_USERNAME'] = json_data['username']\n","        os.environ['KAGGLE_KEY'] = json_data['key']\n","\n","        # set dirs\n","        cfg.DRIVE = cfg.DRIVE_PATH\n","        cfg.EXP = (cfg.NAME if cfg.NAME is not None \n","            else requests.get('http://172.28.0.2:9000/api/sessions').json()[0]['name'][:-6]\n","        )\n","        cfg.INPUT = os.path.join(cfg.DRIVE, 'Input')\n","        cfg.OUTPUT = os.path.join(cfg.DRIVE, 'Output')\n","        cfg.SUBMISSION = os.path.join(cfg.DRIVE, 'Submission')\n","        cfg.DATASET = os.path.join(cfg.DRIVE, 'Dataset')\n","\n","        cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, cfg.EXP) \n","        cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, 'model')\n","        cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, 'fig')\n","        cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, 'preds')\n","\n","        # make dirs\n","        for d in [cfg.INPUT, cfg.SUBMISSION, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n","            os.makedirs(d, exist_ok=True)\n","        \n","        if not os.path.isfile(os.path.join(cfg.INPUT, 'train.csv')):\n","            # load dataset\n","            ! pip install --upgrade --force-reinstall --no-deps kaggle\n","            ! kaggle competitions download -c $cfg.COMPETITION -p $cfg.INPUT\n","            filepath = os.path.join(cfg.INPUT,cfg.COMPETITION+'.zip')\n","            ! unzip -d $cfg.INPUT $filepath\n","            \n","        \n","        for path in cfg.DATASET_PATH:\n","            datasetpath = os.path.join(cfg.DATASET,  path.split('/')[1])\n","            if not os.path.exists(datasetpath):\n","                os.makedirs(datasetpath, exist_ok=True)\n","                ! kaggle datasets download $path -p $datasetpath\n","                filepath = os.path.join(datasetpath, path.split(\"/\")[1]+'.zip')\n","                ! unzip -d $datasetpath $filepath\n","\n","    else:\n","        print('This environment is Kaggle Kernel')\n","\n","        # set dirs\n","        cfg.INPUT = f'../input/{cfg.COMPETITION}'\n","        cfg.EXP = cfg.NAME\n","        cfg.OUTPUT_EXP = cfg.NAME\n","        cfg.SUBMISSION = './'\n","        cfg.DATASET = '../input/'\n","        \n","        cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n","        cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n","        cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n","\n","        # make dirs\n","        for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n","            os.makedirs(d, exist_ok=True)\n","    return cfg\n","\n","\n","def dataset_create_new(dataset_name, upload_dir):\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = dataset_name\n","    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","    api = KaggleApi()\n","    api.authenticate()\n","    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQS0vaZd8A8-"},"outputs":[],"source":["# =====================\n","# Utils\n","# =====================\n","# Seed\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","# KFold\n","def get_kfold(train, n_splits, seed):\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    generator = kf.split(train)\n","    fold_series = []\n","    for fold, (idx_train, idx_valid) in enumerate(generator):\n","        fold_series.append(pd.Series(fold, index=idx_valid))\n","    fold_series = pd.concat(fold_series).sort_index()\n","    return fold_series\n","\n","def get_stratifiedkfold(train, target_col, n_splits, seed):\n","    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    generator = kf.split(train, train[target_col])\n","    fold_series = []\n","    for fold, (idx_train, idx_valid) in enumerate(generator):\n","        fold_series.append(pd.Series(fold, index=idx_valid))\n","    fold_series = pd.concat(fold_series).sort_index()\n","    return fold_series\n","\n","def get_groupkfold(train, target_col, group_col, n_splits):\n","    kf = GroupKFold(n_splits=n_splits)\n","    generator = kf.split(train, train[target_col], train[group_col])\n","    fold_series = []\n","    for fold, (idx_train, idx_valid) in enumerate(generator):\n","        fold_series.append(pd.Series(fold, index=idx_valid))\n","    fold_series = pd.concat(fold_series).sort_index()\n","    return fold_series\n","\n","def get_groupstratifiedkfold(train, target_col, group_col, n_splits, seed):\n","    kf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    generator = kf.split(train, train[target_col], train[group_col])\n","    fold_series = []\n","    for fold, (idx_train, idx_valid) in enumerate(generator):\n","        fold_series.append(pd.Series(fold, index=idx_valid))\n","    fold_series = pd.concat(fold_series).sort_index()\n","    train['fold'] = fold_series\n","    return train, fold_series"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0EEPotHB-SJ"},"outputs":[],"source":["# 文章のバグを治す\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","import codecs\n","\n","def replace_encoding_with_utf8(error: UnicodeError) -\u003e Tuple[bytes, int]:\n","    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n","\n","\n","def replace_decoding_with_cp1252(error: UnicodeError) -\u003e Tuple[str, int]:\n","    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n","\n","# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n","codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n","codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n","\n","def resolve_encodings_and_normalize(text: str) -\u003e str:\n","    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n","    text = (\n","        text.encode(\"raw_unicode_escape\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n","        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","    )\n","    text = unidecode(text)\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9c3uiSU3mDy"},"outputs":[],"source":["def flatten(_list):\n","    return list(itertools.chain.from_iterable(_list))\n","\n","def even_split(input_ids):\n","    best_idx = None\n","    best_len = 100000\n","    for i in range(1, len(input_ids)):\n","        x_len = len(flatten(input_ids[:i]))\n","        y_len = len(flatten(input_ids[i:]))\n","        diff = abs(x_len - y_len)\n","        \n","        if best_len \u003e diff:\n","            best_len = diff\n","            best_idx = i\n","    \n","    return best_idx\n","\n","def preprocess_df(df, tokenizer, max_length=198, total_max_length:int=1024):\n","    df['discourse_text'] = df['discourse_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n","    df['discourse_start'] = df['discourse_type'].apply(lambda x: f'\u003c{x} start\u003e')\n","    df['discourse_end'] = df['discourse_type'].apply(lambda x: f'\u003c{x} end\u003e')\n","    df['input_text'] = df['discourse_start'] + \" \" + df['discourse_text'] + \" \" + df['discourse_end']\n","\n","    # one-hot型の準備\n","    label_ar = df['label'].values\n","    onehot_ar = np.eye(3)[label_ar] \n","    df['Ineffective'] = onehot_ar[:, 0]\n","    df['Adequate'] = onehot_ar[:, 1]\n","    df['Effective'] = onehot_ar[:, 2]\n","    df['label_list'] = df[['Ineffective', 'Adequate', 'Effective']].values.tolist()\n","\n","\n","    gdf = df.groupby(\"essay_id\")\n","    fold_df = df.groupby('essay_id')['fold'].apply(lambda x: list(x)[0])\n","    \n","    essay_inputs = df.groupby(\"essay_id\")[\"input_text\"].apply(list)\n","    essay_ids = essay_inputs.index.tolist()\n","    \n","    labels = gdf[\"label_list\"].apply(list)\n","    discourse_ids = gdf[\"discourse_id\"].apply(list)\n","    \n","    rows = []\n","    for i in tqdm(range(len(essay_inputs))):\n","        # まず全体をtokenizeして1024に収まっていれば、各テキストをtruncationしておく必要はない\n","        input_ids = tokenizer.batch_encode_plus(essay_inputs[i], max_length=total_max_length, truncation=True)[\"input_ids\"]\n","        \n","        if len(flatten(input_ids)) \u003e total_max_length:\n","            split_idx = even_split(input_ids)\n","            \n","            first = input_ids[:split_idx]\n","            first_seq_ids = [[seq_ids]*len(ids) for seq_ids, ids in enumerate(first)]\n","\n","            second = input_ids[split_idx:]\n","            second_seq_ids = [[seq_ids]*len(ids) for seq_ids, ids in enumerate(second)]\n","            essay_id = essay_ids[i]\n","            \n","            rows.append({\n","                \"essay_id\":essay_ids[i],\n","                \"group\":1,\n","                \"discourse_id\": discourse_ids[i][:split_idx ],\n","                \"label\":labels[i][:split_idx],\n","                \"input_ids\":flatten(first),\n","                \"seq_ids\":flatten(first_seq_ids),\n","                \"fold\":fold_df[essay_id],\n","            })\n","        \n","            rows.append({\n","                \"essay_id\":essay_ids[i],\n","                \"group\":2,\n","                \"discourse_id\": discourse_ids[i][split_idx:],\n","                \"label\":labels[i][split_idx:],\n","                \"input_ids\":flatten(second),\n","                \"seq_ids\":flatten(second_seq_ids),\n","                \"fold\":fold_df[essay_id],\n","            })\n","        else:\n","            # もしかしたら一つのtextで1024超える場合もある？\n","            seq_ids = [[seq_ids]*len(ids) for seq_ids, ids in enumerate(input_ids)]\n","            input_ids = flatten(input_ids)\n","            essay_id = essay_ids[i]\n","            \n","            rows.append({\n","                \"essay_id\":essay_ids[i],\n","                \"group\":1,\n","                \"discourse_id\": discourse_ids[i],\n","                \"label\":labels[i],\n","                \"input_ids\":input_ids,\n","                \"seq_ids\":flatten(seq_ids),\n","                \"fold\":fold_df[essay_id],\n","            })\n","            \n","    return pd.DataFrame(rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53jHZmig2OH2"},"outputs":[],"source":["class TrainDataset(Dataset):\n","    def __init__(self, df, tokenizer):\n","        super().__init__()\n","        self.df = df\n","        self.tokenizer = tokenizer\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        \n","        inputs = {}\n","        \n","        input_ids = np.array(row[\"input_ids\"])\n","        seq_ids = np.array(row[\"seq_ids\"])\n","        label = np.array(row['label'])\n","        \n","        attention_mask = np.array([1 if id != self.tokenizer.pad_token_id else 0 for id in input_ids], dtype=np.int64)\n","        \n","        inputs = {\n","            \"discourse_id\":row[\"discourse_id\"],\n","            \"input_ids\":input_ids,\n","            \"attention_mask\":attention_mask,\n","            \"seq_ids\":seq_ids,\n","            \"label\":label,\n","        }\n","        \n","        return inputs\n","\n","# バッチごとにパディング操作を行う\n","class Collator:\n","    def __init__(self, tokenizer, input_cols, meta_cols=None):\n","        self.tokenizer = tokenizer\n","        self.input_cols = input_cols\n","        \n","        self.meta_cols = meta_cols if meta_cols is not None else []\n","        \n","        self.padding = True\n","        self.max_length: Optional[int] = None\n","        self.pad_to_multiple_of: Optional[int] = None\n","    \n","    def __call__(self, features):\n","        first = features[0]\n","        \n","        input_features = []\n","        meta_features = {meta_col:[] for meta_col in self.meta_cols}\n","        \n","        for f in features:\n","            input_features.append({col:f[col] for col in self.input_cols})\n","            for meta_col in self.meta_cols:\n","                meta_features[meta_col].extend(f[meta_col])\n","            \n","        batch = self.tokenizer.pad(\n","            input_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",\n","        )\n","        \n","        if \"label\" in first:\n","            batch[\"labels\"] = pad_sequence([torch.tensor(f[\"label\"], dtype=torch.float) for f in features], batch_first=True, padding_value=-1)\n","\n","        if \"seq_ids\" in first:\n","            batch[\"seq_ids\"] = pad_sequence([torch.tensor(f[\"seq_ids\"], dtype=torch.long) for f in features], batch_first=True, padding_value=-1)\n","            \n","        if self.meta_cols is not None:\n","            batch[\"meta\"] = meta_features\n","\n","        \n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDh1Mz2G2Cnm"},"outputs":[],"source":["class SimpleHeader(nn.Module):\n","    def __init__(self, input_size:int, num_labels):\n","        super().__init__()\n","        self.fc = nn.Linear(input_size, num_labels)\n","\n","        self.cls_dropouts = nn.Sequential(\n","            nn.Dropout(0.1),\n","            nn.Dropout(0.2),\n","            nn.Dropout(0.3),\n","            nn.Dropout(0.4),\n","            nn.Dropout(0.5),\n","        )\n","\n","    def forward(self, x):\n","        output = torch.mean(\n","            torch.stack(\n","                [self.fc(self.cls_dropouts[i](x)) for i in range(5)],\n","                dim=0,\n","            ),\n","            dim=0,\n","        )\n","        return output\n","\n","class LitModel(nn.Module):\n","    def __init__(self,cfg):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.gpu_optimize_config = cfg.gpu_optimize_config\n","        self.config = AutoConfig.from_pretrained(\n","            cfg.MODEL_PATH,\n","            output_hidden_states=True\n","        )\n","        self.config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": 0.1,\n","                \"layer_norm_eps\": 1e-7,\n","                \"add_pooling_layer\": False,\n","                \"num_labels\": 3,\n","            }\n","        )\n","        self.backbone = AutoModel.from_pretrained(\n","            cfg.MODEL_PATH,\n","            config=self.config\n","        )   \n","        self.hidden_size = self.config.hidden_size\n","        self.fc = nn.Linear(self.config.hidden_size, 3)\n","        self.header = SimpleHeader(input_size=self.config.hidden_size, num_labels=3)\n","        self._init_weights(self.header.fc)\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.config.hidden_size, 512),\n","            nn.Tanh(),\n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=0)\n","        )\n","        self._init_weights(self.attention)\n","\n","        # Gradient Checkpointing\n","        if self.gpu_optimize_config['gradient_checkpoint']:\n","            self.backbone.gradient_checkpointing_enable()\n","\n","    @property\n","    def device(self):\n","        return self.backbone.device\n","            \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def feature(self, last_hidden_state):\n","        weights = self.attention(last_hidden_state)\n","        feature = torch.sum(weights * last_hidden_state, dim=0)\n","        return feature\n","\n","    # トークンごとに平均取って潰す\n","    def sequence_mean(self, logits, batch):\n","        batch_seq_mean = []\n","        batch_size = batch[\"input_ids\"].shape[0]\n","        seq_lens = []\n","        # バッチサイズごとに\n","        for i in range(batch_size):\n","            seq_mean = []\n","            # バッチの長さに応じて処理\n","            # iはバッチを表すので、そのバッチで何個文章がくっついてるかをjは表す\n","            for j in range(max(batch[\"seq_ids\"][i])+1):\n","                # i, j成分（該当するdiscourse）を取り出す\n","                idx = batch[\"seq_ids\"][i]==j\n","                idx = idx.nonzero().reshape(-1)\n","                # idxでtensorの抜き出しを行う\n","                seq_tensor = torch.index_select(logits[i], 0, idx)\n","                seq_tensor = self.feature(seq_tensor)\n","                seq_mean.append(seq_tensor)\n","\n","            seq_lens.append(len(seq_mean))\n","            batch_seq_mean.append(torch.vstack(seq_mean))\n","\n","        return batch_seq_mean, seq_lens\n","            \n","    def forward(self, input_dict, labels):\n","        # batch, len, hidden_size\n","        output = self.backbone(\n","            input_ids=input_dict[\"input_ids\"],\n","            attention_mask=input_dict[\"attention_mask\"]\n","        )[\"last_hidden_state\"]\n","    \n","        # discourse, hidden_size\n","        output, seq_lens = self.sequence_mean(output, input_dict)\n","        # discourse, hidden_size\n","        output = torch.vstack(output)\n","        output = self.header(output)\n","\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(output, labels)\n","            return loss, output\n","        else:\n","            return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6wGuA-o0eL8"},"outputs":[],"source":["# FGM\n","# https://www.kaggle.com/competitions/tweet-sentiment-extraction/discussion/143764#809408\n","\n","class FGM():\n","    def __init__(self, model):\n","        self.model = model\n","        self.backup = {}\n","\n","    def attack(self, epsilon=1.0, emb_name='word_embeddings'):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and emb_name in name:\n","                self.backup[name] = param.data.clone()\n","                norm = torch.norm(param.grad)\n","                if norm != 0:\n","                    r_at = epsilon * param.grad / norm\n","                    param.data.add_(r_at)\n","\n","    def restore(self, emb_name='word_embeddings'):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and emb_name in name:\n","                assert name in self.backup\n","                param.data = self.backup[name]\n","            self.backup = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-kDMGg-8i-I"},"outputs":[],"source":["def training(cfg, train):\n","    # =====================\n","    # Training\n","    # =====================\n","    set_seed(cfg.seed)\n","    oof_all_df = pd.DataFrame()\n","    for fold in cfg.trn_fold:\n","        # dataset, dataloader\n","        train_df = train.loc[train['fold']!=fold]\n","        valid_df = train.loc[train['fold']==fold]\n","\n","        # psuedo_dataの追加\n","        psuedo_df = pd.read_csv(f'/content/drive/MyDrive/DataAnalysis/competicion/competicion_feedback/wanwan7123/Input/zaccheaus_exp016/zac_exp016_psuedo_sample_fold{fold}.csv')\n","        psuedo_df['discourse_id'] = psuedo_df['discourse_id'].apply(lambda x: eval(str(x)))\n","        psuedo_df['label'] = psuedo_df['label'].apply(lambda x: eval(str(x)))\n","        psuedo_df['input_ids'] = psuedo_df['input_ids'].apply(lambda x: eval(str(x)))\n","        psuedo_df['seq_ids'] = psuedo_df['seq_ids'].apply(lambda x: eval(str(x)))\n","        train_df = pd.concat([train_df, psuedo_df]).reset_index(drop=True)\n","\n","        train_idx = list(train_df.index)\n","        valid_idx = list(valid_df.index)\n","\n","        # Datasetの設定\n","        train_dataset = TrainDataset(train_df, cfg.tokenizer)\n","        valid_dataset = TrainDataset(valid_df, cfg.tokenizer)\n","        train_loader = DataLoader(\n","            dataset=train_dataset, \n","            batch_size=cfg.batch_size, \n","            shuffle=True,\n","            pin_memory=True,\n","            drop_last=True,\n","            collate_fn = Collator(cfg.tokenizer, input_cols = [\"input_ids\", \"attention_mask\"], meta_cols = [\"discourse_id\"])\n","        )\n","        valid_loader = DataLoader(\n","            dataset=valid_dataset,\n","            batch_size=cfg.batch_size,\n","            shuffle=False,\n","            pin_memory=True,\n","            drop_last=False,\n","            collate_fn = Collator(cfg.tokenizer, input_cols = [\"input_ids\", \"attention_mask\"], meta_cols = [\"discourse_id\"])\n","        )\n","\n","        # model\n","        model = LitModel(cfg)\n","        torch.save(model.config, cfg.EXP_MODEL+'config.pth')\n","        model = model.to(cfg.device)\n","\n","        # optimizer, scheduler\n","        param_optimizer = list(model.named_parameters())\n","        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {\n","                'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n","                'weight_decay': cfg.weight_decay\n","            },\n","            {\n","                'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n","                'weight_decay': 0.0\n","            }\n","        ]\n","\n","        # optimizer\n","        optimizer = AdamW(\n","            optimizer_grouped_parameters,\n","            lr=cfg.lr,\n","            betas=cfg.beta,\n","            weight_decay=cfg.weight_decay,\n","        )\n","        \n","        # scaler\n","        scaler = GradScaler()\n","\n","        # enable FGM\n","        fgm = FGM(model)\n","\n","        num_train_optimization_steps = int(\n","            len(train_loader) * cfg.n_epochs // cfg.gradient_accumulation_steps\n","        )\n","        num_warmup_steps = int(num_train_optimization_steps * cfg.num_warmup_steps_rate)\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_train_optimization_steps\n","        )\n","\n","        # model-training\n","        criterion = nn.CrossEntropyLoss()\n","        best_val_score = 9999\n","        \n","        for epoch in range(cfg.n_epochs):\n","            # training\n","            print(f\"# ============ start epoch:{epoch} ============== #\")\n","            train_losses = []\n","            train_nums = []\n","            model.train() \n","            val_losses_batch = []\n","            ids = []\n","            # dataloader回して予測\n","            with tqdm(train_loader, total=len(train_loader)) as pbar:\n","                for step, (inputs) in enumerate(pbar):\n","\n","                    meta = inputs.pop(\"meta\", None)\n","                    inputs = {k:v.to(cfg.device) for k, v in inputs.items()}\n","                    labels = inputs['labels'][torch.where(inputs['labels'] != -1)].reshape(-1, 3)\n","                    optimizer.zero_grad()\n","\n","                    with autocast():\n","                        loss, output = model(inputs, labels)\n","                    \n","                    ids.extend(meta[\"discourse_id\"])\n","                    pbar.set_postfix({\n","                        'loss': loss.item(),\n","                        'lr': scheduler.get_lr()[0]\n","                    })\n","                    train_losses.append(loss.item() * len(labels))\n","                    train_nums.append(len(labels))\n","\n","                    if cfg.gradient_accumulation_steps \u003e 1:\n","                        loss = loss / cfg.gradient_accumulation_steps\n","\n","                    scaler.scale(loss).backward()\n","\n","                    # FGM attack\n","                    fgm.attack()\n","                    with autocast():\n","                        loss_adv, _ = model(inputs, labels)\n","                    scaler.scale(loss_adv).backward()\n","                    fgm.restore()\n","                    \n","                    if cfg.clip_grad_norm is not None:\n","                        torch.nn.utils.clip_grad_norm_(\n","                            model.parameters(), \n","                            cfg.clip_grad_norm\n","                        )\n","                    if (step+1) % cfg.gradient_accumulation_steps == 0:\n","                        scaler.step(optimizer)\n","                        scaler.update()\n","                        scheduler.step()\n","\n","                    # evaluating\n","                    if ((step+1) % cfg.eval_steps == 0):\n","                        print('===========steps：', step, '==========')\n","                        val_preds = []\n","                        val_losses = []\n","                        val_nums = []\n","                        ids = []\n","                        model.eval()\n","                        with torch.no_grad():\n","                            with tqdm(valid_loader, total=len(valid_loader)) as pbar:\n","                                for steps, (inputs) in enumerate(pbar):\n","\n","                                    meta = inputs.pop(\"meta\", None)\n","                                    inputs = {k:v.to(cfg.device) for k, v in inputs.items()}\n","                                    labels = inputs['labels'][torch.where(inputs['labels'] != -1)].reshape(-1, 3)\n","\n","                                    with autocast():\n","                                        loss, output = model(inputs, labels)\n","                                    ids.extend(meta[\"discourse_id\"])\n","\n","                                    output = output.detach().cpu().numpy()\n","                                    val_preds.append(output)\n","                                    val_losses.append(loss.item() * len(labels))\n","                                    val_nums.append(len(labels))\n","                                    pbar.set_postfix({\n","                                        'val_loss': loss.item()\n","                                    })\n","\n","                        val_preds = np.vstack(val_preds)\n","                        val_loss = sum(val_losses) / sum(val_nums)\n","\n","                        val_log = {\n","                            'val_loss': val_loss\n","                        }\n","                        display(val_log)\n","\n","                        if best_val_score \u003e val_loss:\n","                            print(\"save model weight\")\n","                            best_val_preds = val_preds\n","                            best_val_score = val_loss\n","                            torch.save(\n","                                model.state_dict(), \n","                                os.path.join(cfg.EXP_MODEL, f\"fold{fold}.pth\")\n","                            )\n","                        \n","                        model.train()\n","\n","            train_loss = sum(train_losses)/sum(train_nums)\n","            train_log = {\n","                'train_loss':train_loss\n","            }\n","            display(train_log)\n","\n","            # evaluating(per epoch)\n","            print(' ==========end epoch ==========')\n","            val_preds = []\n","            val_losses = []\n","            val_nums = []\n","            ids = []\n","            model.eval()\n","            with torch.no_grad():\n","                with tqdm(valid_loader, total=len(valid_loader)) as pbar:\n","                    for steps, (inputs) in enumerate(pbar):\n","\n","                        meta = inputs.pop(\"meta\", None)\n","                        inputs = {k:v.to(cfg.device) for k, v in inputs.items()}\n","                        labels = inputs['labels'][torch.where(inputs['labels'] != -1)].reshape(-1, 3)\n","\n","                        with autocast():\n","                            loss, output = model(inputs, labels)\n","                        ids.extend(meta[\"discourse_id\"])\n","\n","                        output = output.detach().cpu().numpy()\n","                        val_preds.append(output)\n","                        val_losses.append(loss.item() * len(labels))\n","                        val_nums.append(len(labels))\n","                        pbar.set_postfix({\n","                            'val_loss': loss.item()\n","                        })\n","\n","            val_preds = np.vstack(val_preds)\n","            val_loss = sum(val_losses) / sum(val_nums)\n","\n","            val_log = {\n","                'val_loss': val_loss\n","            }\n","            display(val_log)\n","\n","            if best_val_score \u003e val_loss:\n","                print(\"save model weight\")\n","                best_val_preds = val_preds\n","                best_val_score = val_loss\n","                torch.save(\n","                    model.state_dict(), \n","                    os.path.join(cfg.EXP_MODEL, f\"fold{fold}.pth\")\n","                )\n","\n","        oof_df = pd.DataFrame(ids, columns=['discourse_id'])\n","        oof_df['Ineffective'] = best_val_preds[:, 0]\n","        oof_df['Adequate'] = best_val_preds[:, 1]\n","        oof_df['Effective'] = best_val_preds[:, 2]\n","        oof_df.to_csv(os.path.join(cfg.EXP_PREDS, f'oof_pred_fold{fold}.csv'))\n","        oof_all_df = pd.concat([oof_all_df, oof_df], axis=0)\n","        del model; gc.collect()\n","\n","    oof_all_df.to_csv(os.path.join(cfg.EXP_PREDS, 'oof_pred.csv'))\n","\n","    # =====================\n","    # scoring\n","    # =====================\n","    '''\n","    metric = nn.CrossEntropyLoss()\n","    score = metric(torch.from_numpy(oof_pred), torch.from_numpy(train['label'].values))\n","    print('CV:', score.to('cpu').detach().numpy())\n","    '''\n","    return oof_all_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"gSrrvDWVpxN_"},"outputs":[{"name":"stdout","output_type":"stream","text":["This environment is Google Colab\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==4.16.2 in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.8.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.6)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.0.53)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.8.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.64.0)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.11.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.12.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2022.6.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers==4.16.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers==4.16.2) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers==4.16.2) (3.8.1)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (1.24.3)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.16.2) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.16.2) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tokenizers==0.11.6 in /usr/local/lib/python3.7/dist-packages (0.11.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n","Requirement already satisfied: tokenizers!=0.11.3,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.11.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.8.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.8.1)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.0.53)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.12.0)\n","Requirement already satisfied: sentencepiece!=0.1.92,\u003e=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.97)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers[sentencepiece]) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers[sentencepiece]) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers[sentencepiece]) (3.8.1)\n","Requirement already satisfied: six\u003e=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf-\u003etransformers[sentencepiece]) (1.15.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers[sentencepiece]) (2022.6.15)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers[sentencepiece]) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers[sentencepiece]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers[sentencepiece]) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers[sentencepiece]) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers[sentencepiece]) (1.1.0)\n","env: TOKENIZERS_PARALLELISM=true\n","tokenizers.__version__: 0.11.6\n","transformers.__version__: 4.16.2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4191/4191 [00:06\u003c00:00, 643.49it/s]\n"]},{"data":{"text/html":["\n","  \u003cdiv id=\"df-c8fe2439-cb70-4d54-b40c-55083933a886\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eessay_id\u003c/th\u003e\n","      \u003cth\u003egroup\u003c/th\u003e\n","      \u003cth\u003ediscourse_id\u003c/th\u003e\n","      \u003cth\u003elabel\u003c/th\u003e\n","      \u003cth\u003einput_ids\u003c/th\u003e\n","      \u003cth\u003eseq_ids\u003c/th\u003e\n","      \u003cth\u003efold\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e00066EA9880D\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e[fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...\u003c/td\u003e\n","      \u003ctd\u003e[[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...\u003c/td\u003e\n","      \u003ctd\u003e[1, 41552, 32258, 386, 15698, 16870, 1672, 167...\u003c/td\u003e\n","      \u003ctd\u003e[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e000E6DE9E817\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e[695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...\u003c/td\u003e\n","      \u003ctd\u003e[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, ...\u003c/td\u003e\n","      \u003ctd\u003e[1, 41552, 46884, 386, 15698, 38, 524, 7594, 1...\u003c/td\u003e\n","      \u003ctd\u003e[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e0016926B079C\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e[89304284cef1, 4f2e871a4908, a885c3aa214b, 953...\u003c/td\u003e\n","      \u003ctd\u003e[[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, ...\u003c/td\u003e\n","      \u003ctd\u003e[1, 41552, 46884, 386, 15698, 38, 206, 14, 521...\u003c/td\u003e\n","      \u003ctd\u003e[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e00203C45FC55\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e[a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...\u003c/td\u003e\n","      \u003ctd\u003e[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...\u003c/td\u003e\n","      \u003ctd\u003e[1, 41552, 32258, 386, 15698, 85, 16, 358, 129...\u003c/td\u003e\n","      \u003ctd\u003e[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e00203C45FC55\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e[44458755d833, e0be5aef7bed, f8a81ae083d4, 248...\u003c/td\u003e\n","      \u003ctd\u003e[[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...\u003c/td\u003e\n","      \u003ctd\u003e[1, 41552, 44057, 31628, 386, 15698, 635, 6, 1...\u003c/td\u003e\n","      \u003ctd\u003e[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8fe2439-cb70-4d54-b40c-55083933a886')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-c8fe2439-cb70-4d54-b40c-55083933a886 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c8fe2439-cb70-4d54-b40c-55083933a886');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["       essay_id  group                                       discourse_id  \\\n","0  00066EA9880D      1  [fe6dfbd53216, ca9e1b60c9fb, 6cf2157f4f19, d92...   \n","1  000E6DE9E817      1  [695d181861a1, cd97ee1cc0ad, 1b775274990b, 567...   \n","2  0016926B079C      1  [89304284cef1, 4f2e871a4908, a885c3aa214b, 953...   \n","3  00203C45FC55      1  [a713d0f6dc68, 2fd9bb2bfedf, 0e5ecdf1516e, 499...   \n","4  00203C45FC55      2  [44458755d833, e0be5aef7bed, f8a81ae083d4, 248...   \n","\n","                                               label  \\\n","0  [[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...   \n","1  [[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, ...   \n","2  [[0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, ...   \n","3  [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...   \n","4  [[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, ...   \n","\n","                                           input_ids  \\\n","0  [1, 41552, 32258, 386, 15698, 16870, 1672, 167...   \n","1  [1, 41552, 46884, 386, 15698, 38, 524, 7594, 1...   \n","2  [1, 41552, 46884, 386, 15698, 38, 206, 14, 521...   \n","3  [1, 41552, 32258, 386, 15698, 85, 16, 358, 129...   \n","4  [1, 41552, 44057, 31628, 386, 15698, 635, 6, 1...   \n","\n","                                             seq_ids  fold  \n","0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     2  \n","1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     0  \n","2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n","3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  \n","4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     1  "]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n","- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["# ============ start epoch:0 ============== #\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|████▊     | 498/1026 [1:43:57\u003c1:46:22, 12.09s/it, loss=0.473, lr=4.91e-6]"]},{"name":"stdout","output_type":"stream","text":["===========steps： 498 ==========\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/224 [00:00\u003c?, ?it/s]\u001b[A\n","  0%|          | 0/224 [00:09\u003c?, ?it/s, val_loss=0.523]\u001b[A\n","  0%|          | 1/224 [00:09\u003c36:53,  9.93s/it, val_loss=0.523]\u001b[A\n","  0%|          | 1/224 [00:19\u003c36:53,  9.93s/it, val_loss=0.713]\u001b[A\n","  1%|          | 2/224 [00:19\u003c36:40,  9.91s/it, val_loss=0.713]\u001b[A\n","  1%|          | 2/224 [00:32\u003c36:40,  9.91s/it, val_loss=0.656]\u001b[A\n","  1%|▏         | 3/224 [00:32\u003c41:18, 11.22s/it, val_loss=0.656]\u001b[A\n","  1%|▏         | 3/224 [00:43\u003c41:18, 11.22s/it, val_loss=0.581]\u001b[A\n","  2%|▏         | 4/224 [00:43\u003c40:52, 11.15s/it, val_loss=0.581]\u001b[A\n","  2%|▏         | 4/224 [00:56\u003c40:52, 11.15s/it, val_loss=0.528]\u001b[A\n","  2%|▏         | 5/224 [00:56\u003c43:14, 11.85s/it, val_loss=0.528]\u001b[A\n","  2%|▏         | 5/224 [01:11\u003c43:14, 11.85s/it, val_loss=0.809]\u001b[A\n","  3%|▎         | 6/224 [01:11\u003c46:20, 12.76s/it, val_loss=0.809]\u001b[A\n","  3%|▎         | 6/224 [01:21\u003c46:20, 12.76s/it, val_loss=0.811]\u001b[A\n","  3%|▎         | 7/224 [01:21\u003c42:35, 11.78s/it, val_loss=0.811]\u001b[A\n","  3%|▎         | 7/224 [01:34\u003c42:35, 11.78s/it, val_loss=0.554]\u001b[A\n","  4%|▎         | 8/224 [01:34\u003c44:29, 12.36s/it, val_loss=0.554]\u001b[A\n","  4%|▎         | 8/224 [01:40\u003c44:29, 12.36s/it, val_loss=0.502]\u001b[A\n","  4%|▍         | 9/224 [01:40\u003c37:33, 10.48s/it, val_loss=0.502]\u001b[A\n","  4%|▍         | 9/224 [01:54\u003c37:33, 10.48s/it, val_loss=0.983]\u001b[A\n","  4%|▍         | 10/224 [01:54\u003c41:07, 11.53s/it, val_loss=0.983]\u001b[A\n","  4%|▍         | 10/224 [02:04\u003c41:07, 11.53s/it, val_loss=0.68] \u001b[A\n","  5%|▍         | 11/224 [02:04\u003c39:10, 11.04s/it, val_loss=0.68]\u001b[A\n","  5%|▍         | 11/224 [02:16\u003c39:10, 11.04s/it, val_loss=0.516]\u001b[A\n","  5%|▌         | 12/224 [02:16\u003c39:33, 11.20s/it, val_loss=0.516]\u001b[A\n","  5%|▌         | 12/224 [02:25\u003c39:33, 11.20s/it, val_loss=0.609]\u001b[A\n","  6%|▌         | 13/224 [02:25\u003c36:41, 10.44s/it, val_loss=0.609]\u001b[A\n","  6%|▌         | 13/224 [02:36\u003c36:41, 10.44s/it, val_loss=0.541]\u001b[A\n","  6%|▋         | 14/224 [02:36\u003c37:15, 10.64s/it, val_loss=0.541]\u001b[A\n","  6%|▋         | 14/224 [02:43\u003c37:15, 10.64s/it, val_loss=0.611]\u001b[A\n","  7%|▋         | 15/224 [02:43\u003c33:59,  9.76s/it, val_loss=0.611]\u001b[A\n","  7%|▋         | 15/224 [02:55\u003c33:59,  9.76s/it, val_loss=0.783]\u001b[A\n","  7%|▋         | 16/224 [02:55\u003c35:46, 10.32s/it, val_loss=0.783]\u001b[A\n","  7%|▋         | 16/224 [03:10\u003c35:46, 10.32s/it, val_loss=0.644]\u001b[A\n","  8%|▊         | 17/224 [03:10\u003c39:59, 11.59s/it, val_loss=0.644]\u001b[A\n","  8%|▊         | 17/224 [03:18\u003c39:59, 11.59s/it, val_loss=0.679]\u001b[A\n","  8%|▊         | 18/224 [03:18\u003c36:12, 10.54s/it, val_loss=0.679]\u001b[A\n","  8%|▊         | 18/224 [03:30\u003c36:12, 10.54s/it, val_loss=0.961]\u001b[A\n","  8%|▊         | 19/224 [03:30\u003c37:38, 11.02s/it, val_loss=0.961]\u001b[A\n","  8%|▊         | 19/224 [03:43\u003c37:38, 11.02s/it, val_loss=0.627]\u001b[A\n","  9%|▉         | 20/224 [03:43\u003c39:27, 11.60s/it, val_loss=0.627]\u001b[A\n","  9%|▉         | 20/224 [03:54\u003c39:27, 11.60s/it, val_loss=0.566]\u001b[A\n","  9%|▉         | 21/224 [03:54\u003c38:41, 11.43s/it, val_loss=0.566]\u001b[A\n","  9%|▉         | 21/224 [04:06\u003c38:41, 11.43s/it, val_loss=0.704]\u001b[A\n"," 10%|▉         | 22/224 [04:06\u003c38:58, 11.58s/it, val_loss=0.704]\u001b[A\n"," 10%|▉         | 22/224 [04:14\u003c38:58, 11.58s/it, val_loss=0.425]\u001b[A\n"," 10%|█         | 23/224 [04:14\u003c35:54, 10.72s/it, val_loss=0.425]\u001b[A\n"," 10%|█         | 23/224 [04:26\u003c35:54, 10.72s/it, val_loss=1.06] \u001b[A\n"," 11%|█         | 24/224 [04:26\u003c36:52, 11.06s/it, val_loss=1.06]\u001b[A\n"," 11%|█         | 24/224 [04:41\u003c36:52, 11.06s/it, val_loss=0.892]\u001b[A\n"," 11%|█         | 25/224 [04:41\u003c39:53, 12.03s/it, val_loss=0.892]\u001b[A\n"," 11%|█         | 25/224 [04:48\u003c39:53, 12.03s/it, val_loss=0.845]\u001b[A\n"," 12%|█▏        | 26/224 [04:48\u003c34:48, 10.55s/it, val_loss=0.845]\u001b[A\n"," 12%|█▏        | 26/224 [04:57\u003c34:48, 10.55s/it, val_loss=0.683]\u001b[A\n"," 12%|█▏        | 27/224 [04:57\u003c33:06, 10.08s/it, val_loss=0.683]\u001b[A\n"," 12%|█▏        | 27/224 [05:09\u003c33:06, 10.08s/it, val_loss=0.647]\u001b[A\n"," 12%|█▎        | 28/224 [05:09\u003c34:46, 10.64s/it, val_loss=0.647]\u001b[A\n"," 12%|█▎        | 28/224 [05:20\u003c34:46, 10.64s/it, val_loss=0.914]\u001b[A\n"," 13%|█▎        | 29/224 [05:20\u003c35:41, 10.98s/it, val_loss=0.914]\u001b[A\n"," 13%|█▎        | 29/224 [05:36\u003c35:41, 10.98s/it, val_loss=0.513]\u001b[A\n"," 13%|█▎        | 30/224 [05:36\u003c39:37, 12.25s/it, val_loss=0.513]\u001b[A\n"," 13%|█▎        | 30/224 [05:47\u003c39:37, 12.25s/it, val_loss=0.802]\u001b[A\n"," 14%|█▍        | 31/224 [05:47\u003c39:00, 12.13s/it, val_loss=0.802]\u001b[A\n"," 14%|█▍        | 31/224 [05:58\u003c39:00, 12.13s/it, val_loss=1.01] \u001b[A\n"," 14%|█▍        | 32/224 [05:58\u003c36:55, 11.54s/it, val_loss=1.01]\u001b[A\n"," 14%|█▍        | 32/224 [06:08\u003c36:55, 11.54s/it, val_loss=0.434]\u001b[A\n"," 15%|█▍        | 33/224 [06:08\u003c35:39, 11.20s/it, val_loss=0.434]\u001b[A\n"," 15%|█▍        | 33/224 [06:17\u003c35:39, 11.20s/it, val_loss=0.789]\u001b[A\n"," 15%|█▌        | 34/224 [06:17\u003c33:35, 10.61s/it, val_loss=0.789]\u001b[A\n"," 15%|█▌        | 34/224 [06:29\u003c33:35, 10.61s/it, val_loss=0.707]\u001b[A\n"," 16%|█▌        | 35/224 [06:29\u003c34:49, 11.05s/it, val_loss=0.707]\u001b[A\n"," 16%|█▌        | 35/224 [06:41\u003c34:49, 11.05s/it, val_loss=0.657]\u001b[A\n"," 16%|█▌        | 36/224 [06:41\u003c35:39, 11.38s/it, val_loss=0.657]\u001b[A\n"," 16%|█▌        | 36/224 [06:49\u003c35:39, 11.38s/it, val_loss=0.702]\u001b[A\n"," 17%|█▋        | 37/224 [06:49\u003c31:45, 10.19s/it, val_loss=0.702]\u001b[A\n"," 17%|█▋        | 37/224 [07:01\u003c31:45, 10.19s/it, val_loss=0.665]\u001b[A\n"," 17%|█▋        | 38/224 [07:01\u003c33:25, 10.78s/it, val_loss=0.665]\u001b[A\n"," 17%|█▋        | 38/224 [07:15\u003c33:25, 10.78s/it, val_loss=0.454]\u001b[A\n"," 17%|█▋        | 39/224 [07:15\u003c36:16, 11.76s/it, val_loss=0.454]\u001b[A\n"," 17%|█▋        | 39/224 [07:29\u003c36:16, 11.76s/it, val_loss=0.565]\u001b[A\n"," 18%|█▊        | 40/224 [07:29\u003c38:16, 12.48s/it, val_loss=0.565]\u001b[A\n"," 18%|█▊        | 40/224 [07:41\u003c38:16, 12.48s/it, val_loss=0.999]\u001b[A\n"," 18%|█▊        | 41/224 [07:41\u003c37:30, 12.30s/it, val_loss=0.999]\u001b[A\n"," 18%|█▊        | 41/224 [07:52\u003c37:30, 12.30s/it, val_loss=0.656]\u001b[A\n"," 19%|█▉        | 42/224 [07:52\u003c36:20, 11.98s/it, val_loss=0.656]\u001b[A\n"," 19%|█▉        | 42/224 [08:04\u003c36:20, 11.98s/it, val_loss=0.527]\u001b[A\n"," 19%|█▉        | 43/224 [08:04\u003c35:34, 11.79s/it, val_loss=0.527]\u001b[A\n"," 19%|█▉        | 43/224 [08:15\u003c35:34, 11.79s/it, val_loss=0.819]\u001b[A\n"," 20%|█▉        | 44/224 [08:15\u003c35:23, 11.80s/it, val_loss=0.819]\u001b[A\n"," 20%|█▉        | 44/224 [08:28\u003c35:23, 11.80s/it, val_loss=0.645]\u001b[A\n"," 20%|██        | 45/224 [08:28\u003c36:00, 12.07s/it, val_loss=0.645]\u001b[A\n"," 20%|██        | 45/224 [08:38\u003c36:00, 12.07s/it, val_loss=0.758]\u001b[A\n"," 21%|██        | 46/224 [08:38\u003c33:30, 11.29s/it, val_loss=0.758]\u001b[A\n"," 21%|██        | 46/224 [08:50\u003c33:30, 11.29s/it, val_loss=0.598]\u001b[A\n"," 21%|██        | 47/224 [08:50\u003c34:27, 11.68s/it, val_loss=0.598]\u001b[A\n"," 21%|██        | 47/224 [09:04\u003c34:27, 11.68s/it, val_loss=0.58] \u001b[A\n"," 21%|██▏       | 48/224 [09:04\u003c36:26, 12.42s/it, val_loss=0.58]\u001b[A\n"," 21%|██▏       | 48/224 [09:15\u003c36:26, 12.42s/it, val_loss=0.861]\u001b[A\n"," 22%|██▏       | 49/224 [09:15\u003c35:02, 12.01s/it, val_loss=0.861]\u001b[A\n"," 22%|██▏       | 49/224 [09:26\u003c35:02, 12.01s/it, val_loss=0.644]\u001b[A\n"," 22%|██▏       | 50/224 [09:26\u003c33:10, 11.44s/it, val_loss=0.644]\u001b[A\n"," 22%|██▏       | 50/224 [09:36\u003c33:10, 11.44s/it, val_loss=0.697]\u001b[A\n"," 23%|██▎       | 51/224 [09:36\u003c32:23, 11.24s/it, val_loss=0.697]\u001b[A\n"," 23%|██▎       | 51/224 [09:50\u003c32:23, 11.24s/it, val_loss=0.65] \u001b[A\n"," 23%|██▎       | 52/224 [09:50\u003c34:07, 11.90s/it, val_loss=0.65]\u001b[A\n"," 23%|██▎       | 52/224 [10:00\u003c34:07, 11.90s/it, val_loss=0.818]\u001b[A\n"," 24%|██▎       | 53/224 [10:00\u003c32:14, 11.32s/it, val_loss=0.818]\u001b[A\n"," 24%|██▎       | 53/224 [10:11\u003c32:14, 11.32s/it, val_loss=0.396]\u001b[A\n"," 24%|██▍       | 54/224 [10:11\u003c32:07, 11.34s/it, val_loss=0.396]\u001b[A\n"," 24%|██▍       | 54/224 [10:22\u003c32:07, 11.34s/it, val_loss=0.677]\u001b[A\n"," 25%|██▍       | 55/224 [10:22\u003c31:36, 11.22s/it, val_loss=0.677]\u001b[A\n"," 25%|██▍       | 55/224 [10:34\u003c31:36, 11.22s/it, val_loss=1.05] \u001b[A\n"," 25%|██▌       | 56/224 [10:34\u003c31:39, 11.31s/it, val_loss=1.05]\u001b[A\n"," 25%|██▌       | 56/224 [10:44\u003c31:39, 11.31s/it, val_loss=0.607]\u001b[A\n"," 25%|██▌       | 57/224 [10:44\u003c31:03, 11.16s/it, val_loss=0.607]\u001b[A\n"," 25%|██▌       | 57/224 [10:54\u003c31:03, 11.16s/it, val_loss=0.549]\u001b[A\n"," 26%|██▌       | 58/224 [10:54\u003c29:37, 10.71s/it, val_loss=0.549]\u001b[A\n"," 26%|██▌       | 58/224 [11:05\u003c29:37, 10.71s/it, val_loss=0.524]\u001b[A\n"," 26%|██▋       | 59/224 [11:05\u003c29:56, 10.89s/it, val_loss=0.524]\u001b[A\n"," 26%|██▋       | 59/224 [11:18\u003c29:56, 10.89s/it, val_loss=0.615]\u001b[A\n"," 27%|██▋       | 60/224 [11:18\u003c31:15, 11.43s/it, val_loss=0.615]\u001b[A\n"," 27%|██▋       | 60/224 [11:30\u003c31:15, 11.43s/it, val_loss=0.741]\u001b[A\n"," 27%|██▋       | 61/224 [11:30\u003c31:33, 11.62s/it, val_loss=0.741]\u001b[A\n"," 27%|██▋       | 61/224 [11:42\u003c31:33, 11.62s/it, val_loss=0.652]\u001b[A\n"," 28%|██▊       | 62/224 [11:42\u003c31:46, 11.77s/it, val_loss=0.652]\u001b[A\n"," 28%|██▊       | 62/224 [11:54\u003c31:46, 11.77s/it, val_loss=0.533]\u001b[A\n"," 28%|██▊       | 63/224 [11:54\u003c31:21, 11.69s/it, val_loss=0.533]\u001b[A\n"," 28%|██▊       | 63/224 [12:03\u003c31:21, 11.69s/it, val_loss=0.596]\u001b[A\n"," 29%|██▊       | 64/224 [12:03\u003c29:01, 10.89s/it, val_loss=0.596]\u001b[A\n"," 29%|██▊       | 64/224 [12:18\u003c29:01, 10.89s/it, val_loss=0.568]\u001b[A\n"," 29%|██▉       | 65/224 [12:18\u003c32:34, 12.29s/it, val_loss=0.568]\u001b[A\n"," 29%|██▉       | 65/224 [12:29\u003c32:34, 12.29s/it, val_loss=0.994]\u001b[A\n"," 29%|██▉       | 66/224 [12:29\u003c30:59, 11.77s/it, val_loss=0.994]\u001b[A\n"," 29%|██▉       | 66/224 [12:41\u003c30:59, 11.77s/it, val_loss=0.818]\u001b[A\n"," 30%|██▉       | 67/224 [12:41\u003c30:48, 11.77s/it, val_loss=0.818]\u001b[A\n"," 30%|██▉       | 67/224 [12:52\u003c30:48, 11.77s/it, val_loss=0.655]\u001b[A\n"," 30%|███       | 68/224 [12:52\u003c30:20, 11.67s/it, val_loss=0.655]\u001b[A\n"," 30%|███       | 68/224 [13:00\u003c30:20, 11.67s/it, val_loss=0.33] \u001b[A\n"," 31%|███       | 69/224 [13:00\u003c27:23, 10.61s/it, val_loss=0.33]\u001b[A\n"," 31%|███       | 69/224 [13:10\u003c27:23, 10.61s/it, val_loss=0.532]\u001b[A\n"," 31%|███▏      | 70/224 [13:10\u003c26:43, 10.41s/it, val_loss=0.532]\u001b[A\n"," 31%|███▏      | 70/224 [13:19\u003c26:43, 10.41s/it, val_loss=0.772]\u001b[A\n"," 32%|███▏      | 71/224 [13:19\u003c25:38, 10.06s/it, val_loss=0.772]\u001b[A\n"," 32%|███▏      | 71/224 [13:30\u003c25:38, 10.06s/it, val_loss=0.712]\u001b[A\n"," 32%|███▏      | 72/224 [13:30\u003c26:09, 10.32s/it, val_loss=0.712]\u001b[A\n"," 32%|███▏      | 72/224 [13:40\u003c26:09, 10.32s/it, val_loss=0.753]\u001b[A\n"," 33%|███▎      | 73/224 [13:40\u003c25:10, 10.00s/it, val_loss=0.753]\u001b[A\n"," 33%|███▎      | 73/224 [13:49\u003c25:10, 10.00s/it, val_loss=0.509]\u001b[A\n"," 33%|███▎      | 74/224 [13:49\u003c24:48,  9.92s/it, val_loss=0.509]\u001b[A\n"," 33%|███▎      | 74/224 [13:58\u003c24:48,  9.92s/it, val_loss=0.583]\u001b[A\n"," 33%|███▎      | 75/224 [13:58\u003c23:53,  9.62s/it, val_loss=0.583]\u001b[A\n"," 33%|███▎      | 75/224 [14:11\u003c23:53,  9.62s/it, val_loss=0.702]\u001b[A\n"," 34%|███▍      | 76/224 [14:11\u003c26:18, 10.67s/it, val_loss=0.702]\u001b[A\n"," 34%|███▍      | 76/224 [14:20\u003c26:18, 10.67s/it, val_loss=0.649]\u001b[A\n"," 34%|███▍      | 77/224 [14:20\u003c24:41, 10.08s/it, val_loss=0.649]\u001b[A\n"," 34%|███▍      | 77/224 [14:30\u003c24:41, 10.08s/it, val_loss=0.449]\u001b[A\n"," 35%|███▍      | 78/224 [14:30\u003c24:35, 10.11s/it, val_loss=0.449]\u001b[A\n"," 35%|███▍      | 78/224 [14:39\u003c24:35, 10.11s/it, val_loss=0.733]\u001b[A\n"," 35%|███▌      | 79/224 [14:39\u003c23:28,  9.72s/it, val_loss=0.733]\u001b[A\n"," 35%|███▌      | 79/224 [14:52\u003c23:28,  9.72s/it, val_loss=0.443]\u001b[A\n"," 36%|███▌      | 80/224 [14:52\u003c25:42, 10.71s/it, val_loss=0.443]\u001b[A\n"," 36%|███▌      | 80/224 [15:02\u003c25:42, 10.71s/it, val_loss=0.727]\u001b[A\n"," 36%|███▌      | 81/224 [15:02\u003c24:38, 10.34s/it, val_loss=0.727]\u001b[A\n"," 36%|███▌      | 81/224 [15:14\u003c24:38, 10.34s/it, val_loss=0.952]\u001b[A\n"," 37%|███▋      | 82/224 [15:14\u003c25:45, 10.88s/it, val_loss=0.952]\u001b[A\n"," 37%|███▋      | 82/224 [15:27\u003c25:45, 10.88s/it, val_loss=0.685]\u001b[A\n"," 37%|███▋      | 83/224 [15:27\u003c27:34, 11.73s/it, val_loss=0.685]\u001b[A\n"," 37%|███▋      | 83/224 [15:40\u003c27:34, 11.73s/it, val_loss=0.47] \u001b[A\n"," 38%|███▊      | 84/224 [15:40\u003c28:02, 12.02s/it, val_loss=0.47]\u001b[A\n"," 38%|███▊      | 84/224 [15:55\u003c28:02, 12.02s/it, val_loss=0.501]\u001b[A\n"," 38%|███▊      | 85/224 [15:55\u003c29:33, 12.76s/it, val_loss=0.501]\u001b[A\n"," 38%|███▊      | 85/224 [16:05\u003c29:33, 12.76s/it, val_loss=0.797]\u001b[A\n"," 38%|███▊      | 86/224 [16:05\u003c27:59, 12.17s/it, val_loss=0.797]\u001b[A\n"," 38%|███▊      | 86/224 [16:17\u003c27:59, 12.17s/it, val_loss=0.749]\u001b[A\n"," 39%|███▉      | 87/224 [16:17\u003c27:26, 12.02s/it, val_loss=0.749]\u001b[A\n"," 39%|███▉      | 87/224 [16:29\u003c27:26, 12.02s/it, val_loss=0.455]\u001b[A\n"," 39%|███▉      | 88/224 [16:29\u003c27:31, 12.14s/it, val_loss=0.455]\u001b[A\n"," 39%|███▉      | 88/224 [16:41\u003c27:31, 12.14s/it, val_loss=0.491]\u001b[A\n"," 40%|███▉      | 89/224 [16:41\u003c26:59, 12.00s/it, val_loss=0.491]\u001b[A\n"," 40%|███▉      | 89/224 [16:55\u003c26:59, 12.00s/it, val_loss=0.617]\u001b[A\n"," 40%|████      | 90/224 [16:55\u003c28:12, 12.63s/it, val_loss=0.617]\u001b[A\n"," 40%|████      | 90/224 [17:06\u003c28:12, 12.63s/it, val_loss=0.591]\u001b[A\n"," 41%|████      | 91/224 [17:06\u003c26:48, 12.10s/it, val_loss=0.591]\u001b[A\n"," 41%|████      | 91/224 [17:17\u003c26:48, 12.10s/it, val_loss=0.704]\u001b[A\n"," 41%|████      | 92/224 [17:17\u003c26:04, 11.85s/it, val_loss=0.704]\u001b[A\n"," 41%|████      | 92/224 [17:30\u003c26:04, 11.85s/it, val_loss=0.514]\u001b[A\n"," 42%|████▏     | 93/224 [17:30\u003c26:16, 12.04s/it, val_loss=0.514]\u001b[A\n"," 42%|████▏     | 93/224 [17:39\u003c26:16, 12.04s/it, val_loss=0.739]\u001b[A\n"," 42%|████▏     | 94/224 [17:39\u003c24:12, 11.17s/it, val_loss=0.739]\u001b[A\n"," 42%|████▏     | 94/224 [17:51\u003c24:12, 11.17s/it, val_loss=0.698]\u001b[A\n"," 42%|████▏     | 95/224 [17:51\u003c24:20, 11.32s/it, val_loss=0.698]\u001b[A\n"," 42%|████▏     | 95/224 [18:05\u003c24:20, 11.32s/it, val_loss=0.871]\u001b[A\n"," 43%|████▎     | 96/224 [18:05\u003c26:00, 12.19s/it, val_loss=0.871]\u001b[A\n"," 43%|████▎     | 96/224 [18:16\u003c26:00, 12.19s/it, val_loss=0.508]\u001b[A\n"," 43%|████▎     | 97/224 [18:16\u003c24:49, 11.72s/it, val_loss=0.508]\u001b[A\n"," 43%|████▎     | 97/224 [18:27\u003c24:49, 11.72s/it, val_loss=0.679]\u001b[A\n"," 44%|████▍     | 98/224 [18:27\u003c24:14, 11.54s/it, val_loss=0.679]\u001b[A\n"," 44%|████▍     | 98/224 [18:35\u003c24:14, 11.54s/it, val_loss=0.683]\u001b[A\n"," 44%|████▍     | 99/224 [18:35\u003c22:08, 10.63s/it, val_loss=0.683]\u001b[A\n"," 44%|████▍     | 99/224 [18:47\u003c22:08, 10.63s/it, val_loss=0.513]\u001b[A\n"," 45%|████▍     | 100/224 [18:47\u003c22:28, 10.88s/it, val_loss=0.513]\u001b[A\n"," 45%|████▍     | 100/224 [18:56\u003c22:28, 10.88s/it, val_loss=0.654]\u001b[A\n"," 45%|████▌     | 101/224 [18:56\u003c21:40, 10.58s/it, val_loss=0.654]\u001b[A\n"," 45%|████▌     | 101/224 [19:09\u003c21:40, 10.58s/it, val_loss=0.75] \u001b[A\n"," 46%|████▌     | 102/224 [19:09\u003c22:46, 11.20s/it, val_loss=0.75]\u001b[A\n"," 46%|████▌     | 102/224 [19:18\u003c22:46, 11.20s/it, val_loss=0.64]\u001b[A\n"," 46%|████▌     | 103/224 [19:18\u003c21:23, 10.60s/it, val_loss=0.64]\u001b[A\n"," 46%|████▌     | 103/224 [19:27\u003c21:23, 10.60s/it, val_loss=0.718]\u001b[A\n"," 46%|████▋     | 104/224 [19:27\u003c19:46,  9.88s/it, val_loss=0.718]\u001b[A\n"," 46%|████▋     | 104/224 [19:37\u003c19:46,  9.88s/it, val_loss=0.732]\u001b[A\n"," 47%|████▋     | 105/224 [19:37\u003c19:51, 10.01s/it, val_loss=0.732]\u001b[A\n"," 47%|████▋     | 105/224 [19:50\u003c19:51, 10.01s/it, val_loss=0.544]\u001b[A\n"," 47%|████▋     | 106/224 [19:50\u003c21:19, 10.84s/it, val_loss=0.544]\u001b[A\n"," 47%|████▋     | 106/224 [19:59\u003c21:19, 10.84s/it, val_loss=0.805]\u001b[A\n"," 48%|████▊     | 107/224 [19:59\u003c20:24, 10.47s/it, val_loss=0.805]\u001b[A\n"," 48%|████▊     | 107/224 [20:09\u003c20:24, 10.47s/it, val_loss=0.653]\u001b[A\n"," 48%|████▊     | 108/224 [20:09\u003c20:07, 10.41s/it, val_loss=0.653]\u001b[A\n"," 48%|████▊     | 108/224 [20:25\u003c20:07, 10.41s/it, val_loss=0.904]\u001b[A\n"," 49%|████▊     | 109/224 [20:25\u003c23:00, 12.00s/it, val_loss=0.904]\u001b[A\n"," 49%|████▊     | 109/224 [20:36\u003c23:00, 12.00s/it, val_loss=0.676]\u001b[A\n"," 49%|████▉     | 110/224 [20:36\u003c22:05, 11.63s/it, val_loss=0.676]\u001b[A\n"," 49%|████▉     | 110/224 [20:47\u003c22:05, 11.63s/it, val_loss=0.658]\u001b[A\n"," 50%|████▉     | 111/224 [20:47\u003c21:33, 11.45s/it, val_loss=0.658]\u001b[A\n"," 50%|████▉     | 111/224 [21:00\u003c21:33, 11.45s/it, val_loss=0.59] \u001b[A\n"," 50%|█████     | 112/224 [21:00\u003c22:08, 11.86s/it, val_loss=0.59]\u001b[A\n"," 50%|█████     | 112/224 [21:12\u003c22:08, 11.86s/it, val_loss=0.803]\u001b[A\n"," 50%|█████     | 113/224 [21:12\u003c22:07, 11.96s/it, val_loss=0.803]\u001b[A\n"," 50%|█████     | 113/224 [21:26\u003c22:07, 11.96s/it, val_loss=0.824]\u001b[A\n"," 51%|█████     | 114/224 [21:26\u003c23:07, 12.61s/it, val_loss=0.824]\u001b[A\n"," 51%|█████     | 114/224 [21:38\u003c23:07, 12.61s/it, val_loss=0.65] \u001b[A\n"," 51%|█████▏    | 115/224 [21:38\u003c22:25, 12.35s/it, val_loss=0.65]\u001b[A\n"," 51%|█████▏    | 115/224 [21:53\u003c22:25, 12.35s/it, val_loss=0.753]\u001b[A\n"," 52%|█████▏    | 116/224 [21:53\u003c23:32, 13.08s/it, val_loss=0.753]\u001b[A\n"," 52%|█████▏    | 116/224 [22:03\u003c23:32, 13.08s/it, val_loss=0.65] \u001b[A\n"," 52%|█████▏    | 117/224 [22:03\u003c21:35, 12.11s/it, val_loss=0.65]\u001b[A\n"," 52%|█████▏    | 117/224 [22:14\u003c21:35, 12.11s/it, val_loss=0.572]\u001b[A\n"," 53%|█████▎    | 118/224 [22:14\u003c21:14, 12.03s/it, val_loss=0.572]\u001b[A\n"," 53%|█████▎    | 118/224 [22:26\u003c21:14, 12.03s/it, val_loss=0.732]\u001b[A\n"," 53%|█████▎    | 119/224 [22:26\u003c20:51, 11.92s/it, val_loss=0.732]\u001b[A\n"," 53%|█████▎    | 119/224 [22:37\u003c20:51, 11.92s/it, val_loss=0.921]\u001b[A\n"," 54%|█████▎    | 120/224 [22:37\u003c20:07, 11.61s/it, val_loss=0.921]\u001b[A\n"," 54%|█████▎    | 120/224 [22:47\u003c20:07, 11.61s/it, val_loss=0.531]\u001b[A\n"," 54%|█████▍    | 121/224 [22:47\u003c19:00, 11.07s/it, val_loss=0.531]\u001b[A\n"," 54%|█████▍    | 121/224 [22:57\u003c19:00, 11.07s/it, val_loss=0.861]\u001b[A\n"," 54%|█████▍    | 122/224 [22:57\u003c18:11, 10.70s/it, val_loss=0.861]\u001b[A\n"," 54%|█████▍    | 122/224 [23:08\u003c18:11, 10.70s/it, val_loss=0.618]\u001b[A\n"," 55%|█████▍    | 123/224 [23:08\u003c18:24, 10.94s/it, val_loss=0.618]\u001b[A\n"," 55%|█████▍    | 123/224 [23:21\u003c18:24, 10.94s/it, val_loss=0.54] \u001b[A\n"," 55%|█████▌    | 124/224 [23:21\u003c19:14, 11.54s/it, val_loss=0.54]\u001b[A\n"," 55%|█████▌    | 124/224 [23:31\u003c19:14, 11.54s/it, val_loss=0.743]\u001b[A\n"," 56%|█████▌    | 125/224 [23:31\u003c18:07, 10.99s/it, val_loss=0.743]\u001b[A\n"," 56%|█████▌    | 125/224 [23:43\u003c18:07, 10.99s/it, val_loss=0.411]\u001b[A\n"," 56%|█████▋    | 126/224 [23:43\u003c18:38, 11.41s/it, val_loss=0.411]\u001b[A\n"," 56%|█████▋    | 126/224 [23:52\u003c18:38, 11.41s/it, val_loss=0.887]\u001b[A\n"," 57%|█████▋    | 127/224 [23:52\u003c17:02, 10.54s/it, val_loss=0.887]\u001b[A\n"," 57%|█████▋    | 127/224 [24:00\u003c17:02, 10.54s/it, val_loss=0.72] \u001b[A\n"," 57%|█████▋    | 128/224 [24:00\u003c16:04, 10.05s/it, val_loss=0.72]\u001b[A\n"," 57%|█████▋    | 128/224 [24:14\u003c16:04, 10.05s/it, val_loss=0.423]\u001b[A\n"," 58%|█████▊    | 129/224 [24:14\u003c17:25, 11.00s/it, val_loss=0.423]\u001b[A\n"," 58%|█████▊    | 129/224 [24:25\u003c17:25, 11.00s/it, val_loss=0.81] \u001b[A\n"," 58%|█████▊    | 130/224 [24:25\u003c17:12, 10.98s/it, val_loss=0.81]\u001b[A\n"," 58%|█████▊    | 130/224 [24:36\u003c17:12, 10.98s/it, val_loss=0.996]\u001b[A\n"," 58%|█████▊    | 131/224 [24:36\u003c17:19, 11.18s/it, val_loss=0.996]\u001b[A\n"," 58%|█████▊    | 131/224 [24:50\u003c17:19, 11.18s/it, val_loss=0.731]\u001b[A\n"," 59%|█████▉    | 132/224 [24:50\u003c18:20, 11.96s/it, val_loss=0.731]\u001b[A\n"," 59%|█████▉    | 132/224 [25:02\u003c18:20, 11.96s/it, val_loss=0.701]\u001b[A\n"," 59%|█████▉    | 133/224 [25:02\u003c18:11, 12.00s/it, val_loss=0.701]\u001b[A\n"," 59%|█████▉    | 133/224 [25:16\u003c18:11, 12.00s/it, val_loss=0.751]\u001b[A\n"," 60%|█████▉    | 134/224 [25:16\u003c18:54, 12.61s/it, val_loss=0.751]\u001b[A\n"," 60%|█████▉    | 134/224 [25:26\u003c18:54, 12.61s/it, val_loss=0.979]\u001b[A\n"," 60%|██████    | 135/224 [25:26\u003c17:39, 11.90s/it, val_loss=0.979]\u001b[A\n"," 60%|██████    | 135/224 [25:38\u003c17:39, 11.90s/it, val_loss=0.651]\u001b[A\n"," 61%|██████    | 136/224 [25:38\u003c17:29, 11.92s/it, val_loss=0.651]\u001b[A\n"," 61%|██████    | 136/224 [25:48\u003c17:29, 11.92s/it, val_loss=0.726]\u001b[A\n"," 61%|██████    | 137/224 [25:48\u003c16:27, 11.35s/it, val_loss=0.726]\u001b[A\n"," 61%|██████    | 137/224 [25:58\u003c16:27, 11.35s/it, val_loss=0.516]\u001b[A\n"," 62%|██████▏   | 138/224 [25:58\u003c15:23, 10.74s/it, val_loss=0.516]\u001b[A\n"," 62%|██████▏   | 138/224 [26:14\u003c15:23, 10.74s/it, val_loss=0.919]\u001b[A\n"," 62%|██████▏   | 139/224 [26:14\u003c17:22, 12.27s/it, val_loss=0.919]\u001b[A\n"," 62%|██████▏   | 139/224 [26:24\u003c17:22, 12.27s/it, val_loss=0.405]\u001b[A\n"," 62%|██████▎   | 140/224 [26:24\u003c16:26, 11.74s/it, val_loss=0.405]\u001b[A\n"," 62%|██████▎   | 140/224 [26:38\u003c16:26, 11.74s/it, val_loss=0.705]\u001b[A\n"," 63%|██████▎   | 141/224 [26:38\u003c16:55, 12.24s/it, val_loss=0.705]\u001b[A\n"," 63%|██████▎   | 141/224 [26:50\u003c16:55, 12.24s/it, val_loss=0.532]\u001b[A\n"," 63%|██████▎   | 142/224 [26:50\u003c16:41, 12.21s/it, val_loss=0.532]\u001b[A\n"," 63%|██████▎   | 142/224 [27:02\u003c16:41, 12.21s/it, val_loss=0.523]\u001b[A\n"," 64%|██████▍   | 143/224 [27:02\u003c16:43, 12.39s/it, val_loss=0.523]\u001b[A\n"," 64%|██████▍   | 143/224 [27:12\u003c16:43, 12.39s/it, val_loss=0.439]\u001b[A\n"," 64%|██████▍   | 144/224 [27:12\u003c15:34, 11.68s/it, val_loss=0.439]\u001b[A\n"," 64%|██████▍   | 144/224 [27:22\u003c15:34, 11.68s/it, val_loss=0.618]\u001b[A\n"," 65%|██████▍   | 145/224 [27:22\u003c14:42, 11.17s/it, val_loss=0.618]\u001b[A\n"," 65%|██████▍   | 145/224 [27:30\u003c14:42, 11.17s/it, val_loss=0.523]\u001b[A\n"," 65%|██████▌   | 146/224 [27:30\u003c13:14, 10.18s/it, val_loss=0.523]\u001b[A\n"," 65%|██████▌   | 146/224 [27:40\u003c13:14, 10.18s/it, val_loss=0.558]\u001b[A\n"," 66%|██████▌   | 147/224 [27:40\u003c12:42,  9.91s/it, val_loss=0.558]\u001b[A\n"," 66%|██████▌   | 147/224 [27:49\u003c12:42,  9.91s/it, val_loss=0.675]\u001b[A\n"," 66%|██████▌   | 148/224 [27:49\u003c12:25,  9.81s/it, val_loss=0.675]\u001b[A\n"," 66%|██████▌   | 148/224 [27:59\u003c12:25,  9.81s/it, val_loss=0.693]\u001b[A\n"," 67%|██████▋   | 149/224 [27:59\u003c12:11,  9.75s/it, val_loss=0.693]\u001b[A\n"," 67%|██████▋   | 149/224 [28:10\u003c12:11,  9.75s/it, val_loss=0.615]\u001b[A\n"," 67%|██████▋   | 150/224 [28:10\u003c12:44, 10.33s/it, val_loss=0.615]\u001b[A\n"," 67%|██████▋   | 150/224 [28:24\u003c12:44, 10.33s/it, val_loss=0.645]\u001b[A\n"," 67%|██████▋   | 151/224 [28:24\u003c13:38, 11.21s/it, val_loss=0.645]\u001b[A\n"," 67%|██████▋   | 151/224 [28:33\u003c13:38, 11.21s/it, val_loss=1.01] \u001b[A\n"," 68%|██████▊   | 152/224 [28:33\u003c12:41, 10.57s/it, val_loss=1.01]\u001b[A\n"," 68%|██████▊   | 152/224 [28:45\u003c12:41, 10.57s/it, val_loss=0.752]\u001b[A\n"," 68%|██████▊   | 153/224 [28:45\u003c13:04, 11.04s/it, val_loss=0.752]\u001b[A\n"," 68%|██████▊   | 153/224 [28:56\u003c13:04, 11.04s/it, val_loss=0.736]\u001b[A\n"," 69%|██████▉   | 154/224 [28:56\u003c12:57, 11.10s/it, val_loss=0.736]\u001b[A\n"," 69%|██████▉   | 154/224 [29:09\u003c12:57, 11.10s/it, val_loss=0.657]\u001b[A\n"," 69%|██████▉   | 155/224 [29:09\u003c13:16, 11.54s/it, val_loss=0.657]\u001b[A\n"," 69%|██████▉   | 155/224 [29:18\u003c13:16, 11.54s/it, val_loss=0.493]\u001b[A\n"," 70%|██████▉   | 156/224 [29:18\u003c12:14, 10.79s/it, val_loss=0.493]\u001b[A\n"," 70%|██████▉   | 156/224 [29:30\u003c12:14, 10.79s/it, val_loss=0.693]\u001b[A\n"," 70%|███████   | 157/224 [29:30\u003c12:28, 11.18s/it, val_loss=0.693]\u001b[A\n"," 70%|███████   | 157/224 [29:41\u003c12:28, 11.18s/it, val_loss=0.877]\u001b[A\n"," 71%|███████   | 158/224 [29:41\u003c12:11, 11.09s/it, val_loss=0.877]\u001b[A\n"," 71%|███████   | 158/224 [29:51\u003c12:11, 11.09s/it, val_loss=0.462]\u001b[A\n"," 71%|███████   | 159/224 [29:51\u003c11:46, 10.86s/it, val_loss=0.462]\u001b[A\n"," 71%|███████   | 159/224 [30:03\u003c11:46, 10.86s/it, val_loss=0.707]\u001b[A\n"," 71%|███████▏  | 160/224 [30:03\u003c11:53, 11.16s/it, val_loss=0.707]\u001b[A\n"," 71%|███████▏  | 160/224 [30:12\u003c11:53, 11.16s/it, val_loss=0.758]\u001b[A\n"," 72%|███████▏  | 161/224 [30:12\u003c11:11, 10.66s/it, val_loss=0.758]\u001b[A\n"," 72%|███████▏  | 161/224 [30:25\u003c11:11, 10.66s/it, val_loss=0.772]\u001b[A\n"," 72%|███████▏  | 162/224 [30:25\u003c11:34, 11.21s/it, val_loss=0.772]\u001b[A\n"," 72%|███████▏  | 162/224 [30:37\u003c11:34, 11.21s/it, val_loss=0.578]\u001b[A\n"," 73%|███████▎  | 163/224 [30:37\u003c11:31, 11.34s/it, val_loss=0.578]\u001b[A\n"," 73%|███████▎  | 163/224 [30:46\u003c11:31, 11.34s/it, val_loss=0.513]\u001b[A\n"," 73%|███████▎  | 164/224 [30:46\u003c10:37, 10.63s/it, val_loss=0.513]\u001b[A\n"," 73%|███████▎  | 164/224 [30:56\u003c10:37, 10.63s/it, val_loss=0.933]\u001b[A\n"," 74%|███████▎  | 165/224 [30:56\u003c10:15, 10.44s/it, val_loss=0.933]\u001b[A\n"," 74%|███████▎  | 165/224 [31:09\u003c10:15, 10.44s/it, val_loss=0.945]\u001b[A\n"," 74%|███████▍  | 166/224 [31:09\u003c10:53, 11.26s/it, val_loss=0.945]\u001b[A\n"," 74%|███████▍  | 166/224 [31:24\u003c10:53, 11.26s/it, val_loss=0.528]\u001b[A\n"," 75%|███████▍  | 167/224 [31:24\u003c11:51, 12.48s/it, val_loss=0.528]\u001b[A\n"," 75%|███████▍  | 167/224 [31:35\u003c11:51, 12.48s/it, val_loss=0.746]\u001b[A\n"," 75%|███████▌  | 168/224 [31:35\u003c11:13, 12.03s/it, val_loss=0.746]\u001b[A\n"," 75%|███████▌  | 168/224 [31:48\u003c11:13, 12.03s/it, val_loss=0.538]\u001b[A\n"," 75%|███████▌  | 169/224 [31:48\u003c11:14, 12.26s/it, val_loss=0.538]\u001b[A\n"," 75%|███████▌  | 169/224 [31:57\u003c11:14, 12.26s/it, val_loss=0.748]\u001b[A\n"," 76%|███████▌  | 170/224 [31:57\u003c10:09, 11.28s/it, val_loss=0.748]\u001b[A\n"," 76%|███████▌  | 170/224 [32:09\u003c10:09, 11.28s/it, val_loss=0.862]\u001b[A\n"," 76%|███████▋  | 171/224 [32:09\u003c10:15, 11.61s/it, val_loss=0.862]\u001b[A\n"," 76%|███████▋  | 171/224 [32:24\u003c10:15, 11.61s/it, val_loss=0.597]\u001b[A\n"," 77%|███████▋  | 172/224 [32:24\u003c10:45, 12.42s/it, val_loss=0.597]\u001b[A\n"," 77%|███████▋  | 172/224 [32:35\u003c10:45, 12.42s/it, val_loss=0.918]\u001b[A\n"," 77%|███████▋  | 173/224 [32:35\u003c10:12, 12.00s/it, val_loss=0.918]\u001b[A\n"," 77%|███████▋  | 173/224 [32:44\u003c10:12, 12.00s/it, val_loss=0.648]\u001b[A\n"," 78%|███████▊  | 174/224 [32:44\u003c09:27, 11.35s/it, val_loss=0.648]\u001b[A\n"," 78%|███████▊  | 174/224 [32:57\u003c09:27, 11.35s/it, val_loss=0.608]\u001b[A\n"," 78%|███████▊  | 175/224 [32:57\u003c09:27, 11.59s/it, val_loss=0.608]\u001b[A\n"," 78%|███████▊  | 175/224 [33:05\u003c09:27, 11.59s/it, val_loss=0.649]\u001b[A\n"," 79%|███████▊  | 176/224 [33:05\u003c08:25, 10.53s/it, val_loss=0.649]\u001b[A\n"," 79%|███████▊  | 176/224 [33:13\u003c08:25, 10.53s/it, val_loss=0.512]\u001b[A\n"," 79%|███████▉  | 177/224 [33:13\u003c07:39,  9.77s/it, val_loss=0.512]\u001b[A\n"," 79%|███████▉  | 177/224 [33:23\u003c07:39,  9.77s/it, val_loss=0.58] \u001b[A\n"," 79%|███████▉  | 178/224 [33:23\u003c07:39,  9.99s/it, val_loss=0.58]\u001b[A\n"," 79%|███████▉  | 178/224 [33:40\u003c07:39,  9.99s/it, val_loss=0.646]\u001b[A\n"," 80%|███████▉  | 179/224 [33:40\u003c08:58, 11.97s/it, val_loss=0.646]\u001b[A\n"," 80%|███████▉  | 179/224 [33:52\u003c08:58, 11.97s/it, val_loss=0.771]\u001b[A\n"," 80%|████████  | 180/224 [33:52\u003c08:53, 12.13s/it, val_loss=0.771]\u001b[A\n"," 80%|████████  | 180/224 [34:06\u003c08:53, 12.13s/it, val_loss=0.521]\u001b[A\n"," 81%|████████  | 181/224 [34:06\u003c09:08, 12.76s/it, val_loss=0.521]\u001b[A\n"," 81%|████████  | 181/224 [34:16\u003c09:08, 12.76s/it, val_loss=0.353]\u001b[A\n"," 81%|████████▏ | 182/224 [34:16\u003c08:18, 11.86s/it, val_loss=0.353]\u001b[A\n"," 81%|████████▏ | 182/224 [34:29\u003c08:18, 11.86s/it, val_loss=0.915]\u001b[A\n"," 82%|████████▏ | 183/224 [34:29\u003c08:12, 12.01s/it, val_loss=0.915]\u001b[A\n"," 82%|████████▏ | 183/224 [34:40\u003c08:12, 12.01s/it, val_loss=0.424]\u001b[A\n"," 82%|████████▏ | 184/224 [34:40\u003c07:52, 11.82s/it, val_loss=0.424]\u001b[A\n"," 82%|████████▏ | 184/224 [34:54\u003c07:52, 11.82s/it, val_loss=1.36] \u001b[A\n"," 83%|████████▎ | 185/224 [34:54\u003c08:05, 12.44s/it, val_loss=1.36]\u001b[A\n"," 83%|████████▎ | 185/224 [35:05\u003c08:05, 12.44s/it, val_loss=0.768]\u001b[A\n"," 83%|████████▎ | 186/224 [35:05\u003c07:37, 12.03s/it, val_loss=0.768]\u001b[A\n"," 83%|████████▎ | 186/224 [35:16\u003c07:37, 12.03s/it, val_loss=0.644]\u001b[A\n"," 83%|████████▎ | 187/224 [35:16\u003c07:11, 11.66s/it, val_loss=0.644]\u001b[A\n"," 83%|████████▎ | 187/224 [35:27\u003c07:11, 11.66s/it, val_loss=1.04] \u001b[A\n"," 84%|████████▍ | 188/224 [35:27\u003c06:52, 11.46s/it, val_loss=1.04]\u001b[A\n"," 84%|████████▍ | 188/224 [35:41\u003c06:52, 11.46s/it, val_loss=0.767]\u001b[A\n"," 84%|████████▍ | 189/224 [35:41\u003c07:08, 12.24s/it, val_loss=0.767]\u001b[A\n"," 84%|████████▍ | 189/224 [35:49\u003c07:08, 12.24s/it, val_loss=0.691]\u001b[A\n"," 85%|████████▍ | 190/224 [35:49\u003c06:19, 11.16s/it, val_loss=0.691]\u001b[A\n"," 85%|████████▍ | 190/224 [36:03\u003c06:19, 11.16s/it, val_loss=0.941]\u001b[A\n"," 85%|████████▌ | 191/224 [36:03\u003c06:31, 11.87s/it, val_loss=0.941]\u001b[A\n"," 85%|████████▌ | 191/224 [36:14\u003c06:31, 11.87s/it, val_loss=0.583]\u001b[A\n"," 86%|████████▌ | 192/224 [36:14\u003c06:11, 11.62s/it, val_loss=0.583]\u001b[A\n"," 86%|████████▌ | 192/224 [36:25\u003c06:11, 11.62s/it, val_loss=0.879]\u001b[A\n"," 86%|████████▌ | 193/224 [36:25\u003c05:58, 11.57s/it, val_loss=0.879]\u001b[A\n"," 86%|████████▌ | 193/224 [36:36\u003c05:58, 11.57s/it, val_loss=0.588]\u001b[A\n"," 87%|████████▋ | 194/224 [36:36\u003c05:39, 11.33s/it, val_loss=0.588]\u001b[A\n"," 87%|████████▋ | 194/224 [36:49\u003c05:39, 11.33s/it, val_loss=0.66] \u001b[A\n"," 87%|████████▋ | 195/224 [36:49\u003c05:45, 11.91s/it, val_loss=0.66]\u001b[A\n"," 87%|████████▋ | 195/224 [37:00\u003c05:45, 11.91s/it, val_loss=0.614]\u001b[A\n"," 88%|████████▊ | 196/224 [37:00\u003c05:20, 11.46s/it, val_loss=0.614]\u001b[A\n"," 88%|████████▊ | 196/224 [37:09\u003c05:20, 11.46s/it, val_loss=0.785]\u001b[A\n"," 88%|████████▊ | 197/224 [37:09\u003c04:53, 10.86s/it, val_loss=0.785]\u001b[A\n"," 88%|████████▊ | 197/224 [37:20\u003c04:53, 10.86s/it, val_loss=0.703]\u001b[A\n"," 88%|████████▊ | 198/224 [37:20\u003c04:43, 10.89s/it, val_loss=0.703]\u001b[A\n"," 88%|████████▊ | 198/224 [37:31\u003c04:43, 10.89s/it, val_loss=0.556]\u001b[A\n"," 89%|████████▉ | 199/224 [37:31\u003c04:29, 10.77s/it, val_loss=0.556]\u001b[A\n"," 89%|████████▉ | 199/224 [37:44\u003c04:29, 10.77s/it, val_loss=0.619]\u001b[A\n"," 89%|████████▉ | 200/224 [37:44\u003c04:33, 11.39s/it, val_loss=0.619]\u001b[A\n"," 89%|████████▉ | 200/224 [37:56\u003c04:33, 11.39s/it, val_loss=0.664]\u001b[A\n"," 90%|████████▉ | 201/224 [37:56\u003c04:32, 11.85s/it, val_loss=0.664]\u001b[A\n"," 90%|████████▉ | 201/224 [38:06\u003c04:32, 11.85s/it, val_loss=0.834]\u001b[A\n"," 90%|█████████ | 202/224 [38:06\u003c04:04, 11.12s/it, val_loss=0.834]\u001b[A\n"," 90%|█████████ | 202/224 [38:16\u003c04:04, 11.12s/it, val_loss=0.445]\u001b[A\n"," 91%|█████████ | 203/224 [38:16\u003c03:49, 10.93s/it, val_loss=0.445]\u001b[A\n"," 91%|█████████ | 203/224 [38:29\u003c03:49, 10.93s/it, val_loss=0.596]\u001b[A\n"," 91%|█████████ | 204/224 [38:29\u003c03:49, 11.45s/it, val_loss=0.596]\u001b[A\n"," 91%|█████████ | 204/224 [38:39\u003c03:49, 11.45s/it, val_loss=0.652]\u001b[A\n"," 92%|█████████▏| 205/224 [38:39\u003c03:30, 11.07s/it, val_loss=0.652]\u001b[A\n"," 92%|█████████▏| 205/224 [38:51\u003c03:30, 11.07s/it, val_loss=0.793]\u001b[A\n"," 92%|█████████▏| 206/224 [38:51\u003c03:20, 11.14s/it, val_loss=0.793]\u001b[A\n"," 92%|█████████▏| 206/224 [38:59\u003c03:20, 11.14s/it, val_loss=0.615]\u001b[A\n"," 92%|█████████▏| 207/224 [38:59\u003c02:58, 10.47s/it, val_loss=0.615]\u001b[A\n"," 92%|█████████▏| 207/224 [39:13\u003c02:58, 10.47s/it, val_loss=0.498]\u001b[A\n"," 93%|█████████▎| 208/224 [39:13\u003c03:00, 11.29s/it, val_loss=0.498]\u001b[A\n"," 93%|█████████▎| 208/224 [39:23\u003c03:00, 11.29s/it, val_loss=0.708]\u001b[A\n"," 93%|█████████▎| 209/224 [39:23\u003c02:47, 11.15s/it, val_loss=0.708]\u001b[A\n"," 93%|█████████▎| 209/224 [39:38\u003c02:47, 11.15s/it, val_loss=0.734]\u001b[A\n"," 94%|█████████▍| 210/224 [39:38\u003c02:49, 12.12s/it, val_loss=0.734]\u001b[A\n"," 94%|█████████▍| 210/224 [39:47\u003c02:49, 12.12s/it, val_loss=0.544]\u001b[A\n"," 94%|█████████▍| 211/224 [39:47\u003c02:26, 11.25s/it, val_loss=0.544]\u001b[A\n"," 94%|█████████▍| 211/224 [39:56\u003c02:26, 11.25s/it, val_loss=0.627]\u001b[A\n"," 95%|█████████▍| 212/224 [39:56\u003c02:04, 10.41s/it, val_loss=0.627]\u001b[A\n"," 95%|█████████▍| 212/224 [40:06\u003c02:04, 10.41s/it, val_loss=0.717]\u001b[A\n"," 95%|█████████▌| 213/224 [40:06\u003c01:55, 10.52s/it, val_loss=0.717]\u001b[A\n"," 95%|█████████▌| 213/224 [40:20\u003c01:55, 10.52s/it, val_loss=0.561]\u001b[A\n"," 96%|█████████▌| 214/224 [40:20\u003c01:55, 11.52s/it, val_loss=0.561]\u001b[A\n"," 96%|█████████▌| 214/224 [40:30\u003c01:55, 11.52s/it, val_loss=0.953]\u001b[A\n"," 96%|█████████▌| 215/224 [40:30\u003c01:39, 11.01s/it, val_loss=0.953]\u001b[A\n"," 96%|█████████▌| 215/224 [40:41\u003c01:39, 11.01s/it, val_loss=0.79] \u001b[A\n"," 96%|█████████▋| 216/224 [40:41\u003c01:27, 10.92s/it, val_loss=0.79]\u001b[A\n"," 96%|█████████▋| 216/224 [40:51\u003c01:27, 10.92s/it, val_loss=0.928]\u001b[A\n"," 97%|█████████▋| 217/224 [40:51\u003c01:15, 10.76s/it, val_loss=0.928]\u001b[A\n"," 97%|█████████▋| 217/224 [41:00\u003c01:15, 10.76s/it, val_loss=0.822]\u001b[A\n"," 97%|█████████▋| 218/224 [41:00\u003c01:00, 10.06s/it, val_loss=0.822]\u001b[A\n"," 97%|█████████▋| 218/224 [41:10\u003c01:00, 10.06s/it, val_loss=0.52] \u001b[A\n"," 98%|█████████▊| 219/224 [41:10\u003c00:51, 10.30s/it, val_loss=0.52]\u001b[A\n"," 98%|█████████▊| 219/224 [41:20\u003c00:51, 10.30s/it, val_loss=0.455]\u001b[A\n"," 98%|█████████▊| 220/224 [41:20\u003c00:39,  9.95s/it, val_loss=0.455]\u001b[A\n"," 98%|█████████▊| 220/224 [41:28\u003c00:39,  9.95s/it, val_loss=0.727]\u001b[A\n"," 99%|█████████▊| 221/224 [41:28\u003c00:28,  9.61s/it, val_loss=0.727]\u001b[A\n"," 99%|█████████▊| 221/224 [41:39\u003c00:28,  9.61s/it, val_loss=0.694]\u001b[A\n"," 99%|█████████▉| 222/224 [41:39\u003c00:19,  9.87s/it, val_loss=0.694]\u001b[A\n"," 99%|█████████▉| 222/224 [41:46\u003c00:19,  9.87s/it, val_loss=0.966]\u001b[A\n","100%|█████████▉| 223/224 [41:46\u003c00:09,  9.12s/it, val_loss=0.966]\u001b[A\n","100%|█████████▉| 223/224 [41:54\u003c00:09,  9.12s/it, val_loss=0.422]\u001b[A\n","100%|██████████| 224/224 [41:54\u003c00:00, 11.23s/it, val_loss=0.422]\n"]},{"data":{"text/plain":["{'val_loss': 0.6823181553861024}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["save model weight\n"]},{"name":"stderr","output_type":"stream","text":[" 97%|█████████▋| 997/1026 [4:12:33\u003c07:17, 15.07s/it, loss=0.473, lr=4.91e-6]"]},{"name":"stdout","output_type":"stream","text":["===========steps： 997 ==========\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/224 [00:00\u003c?, ?it/s]\u001b[A\n","  0%|          | 0/224 [00:09\u003c?, ?it/s, val_loss=0.531]\u001b[A\n","  0%|          | 1/224 [00:09\u003c35:33,  9.57s/it, val_loss=0.531]\u001b[A\n","  0%|          | 1/224 [00:18\u003c35:33,  9.57s/it, val_loss=0.681]\u001b[A\n","  1%|          | 2/224 [00:18\u003c35:03,  9.48s/it, val_loss=0.681]\u001b[A\n","  1%|          | 2/224 [00:31\u003c35:03,  9.48s/it, val_loss=0.723]\u001b[A\n","  1%|▏         | 3/224 [00:31\u003c39:29, 10.72s/it, val_loss=0.723]\u001b[A\n","  1%|▏         | 3/224 [00:41\u003c39:29, 10.72s/it, val_loss=0.651]\u001b[A\n","  2%|▏         | 4/224 [00:41\u003c39:01, 10.65s/it, val_loss=0.651]\u001b[A\n","  2%|▏         | 4/224 [00:54\u003c39:01, 10.65s/it, val_loss=0.699]\u001b[A\n","  2%|▏         | 5/224 [00:54\u003c41:11, 11.28s/it, val_loss=0.699]\u001b[A\n","  2%|▏         | 5/224 [01:07\u003c41:11, 11.28s/it, val_loss=0.675]\u001b[A\n","  3%|▎         | 6/224 [01:07\u003c44:02, 12.12s/it, val_loss=0.675]\u001b[A\n","  3%|▎         | 6/224 [01:17\u003c44:02, 12.12s/it, val_loss=0.785]\u001b[A\n","  3%|▎         | 7/224 [01:17\u003c40:19, 11.15s/it, val_loss=0.785]\u001b[A\n","  3%|▎         | 7/224 [01:29\u003c40:19, 11.15s/it, val_loss=0.579]\u001b[A\n","  4%|▎         | 8/224 [01:29\u003c42:01, 11.67s/it, val_loss=0.579]\u001b[A\n","  4%|▎         | 8/224 [01:35\u003c42:01, 11.67s/it, val_loss=0.394]\u001b[A\n","  4%|▍         | 9/224 [01:35\u003c35:23,  9.87s/it, val_loss=0.394]\u001b[A\n","  4%|▍         | 9/224 [01:48\u003c35:23,  9.87s/it, val_loss=0.754]\u001b[A\n","  4%|▍         | 10/224 [01:48\u003c38:52, 10.90s/it, val_loss=0.754]\u001b[A\n","  4%|▍         | 10/224 [01:58\u003c38:52, 10.90s/it, val_loss=0.738]\u001b[A\n","  5%|▍         | 11/224 [01:58\u003c37:10, 10.47s/it, val_loss=0.738]\u001b[A\n","  5%|▍         | 11/224 [02:09\u003c37:10, 10.47s/it, val_loss=0.511]\u001b[A\n","  5%|▌         | 12/224 [02:09\u003c38:07, 10.79s/it, val_loss=0.511]\u001b[A\n","  5%|▌         | 12/224 [02:18\u003c38:07, 10.79s/it, val_loss=0.548]\u001b[A\n","  6%|▌         | 13/224 [02:18\u003c35:55, 10.22s/it, val_loss=0.548]\u001b[A\n","  6%|▌         | 13/224 [02:29\u003c35:55, 10.22s/it, val_loss=0.556]\u001b[A\n","  6%|▋         | 14/224 [02:29\u003c36:17, 10.37s/it, val_loss=0.556]\u001b[A\n","  6%|▋         | 14/224 [02:37\u003c36:17, 10.37s/it, val_loss=0.593]\u001b[A\n","  7%|▋         | 15/224 [02:37\u003c33:14,  9.54s/it, val_loss=0.593]\u001b[A\n","  7%|▋         | 15/224 [02:48\u003c33:14,  9.54s/it, val_loss=0.726]\u001b[A\n","  7%|▋         | 16/224 [02:48\u003c35:00, 10.10s/it, val_loss=0.726]\u001b[A\n","  7%|▋         | 16/224 [03:02\u003c35:00, 10.10s/it, val_loss=0.617]\u001b[A\n","  8%|▊         | 17/224 [03:02\u003c38:35, 11.19s/it, val_loss=0.617]\u001b[A\n","  8%|▊         | 17/224 [03:09\u003c38:35, 11.19s/it, val_loss=0.745]\u001b[A\n","  8%|▊         | 18/224 [03:09\u003c34:47, 10.13s/it, val_loss=0.745]\u001b[A\n","  8%|▊         | 18/224 [03:21\u003c34:47, 10.13s/it, val_loss=0.866]\u001b[A\n","  8%|▊         | 19/224 [03:21\u003c36:01, 10.54s/it, val_loss=0.866]\u001b[A\n","  8%|▊         | 19/224 [03:34\u003c36:01, 10.54s/it, val_loss=0.69] \u001b[A\n","  9%|▉         | 20/224 [03:34\u003c38:05, 11.20s/it, val_loss=0.69]\u001b[A\n","  9%|▉         | 20/224 [03:44\u003c38:05, 11.20s/it, val_loss=0.53]\u001b[A\n","  9%|▉         | 21/224 [03:44\u003c37:20, 11.04s/it, val_loss=0.53]\u001b[A\n","  9%|▉         | 21/224 [03:56\u003c37:20, 11.04s/it, val_loss=0.651]\u001b[A\n"," 10%|▉         | 22/224 [03:56\u003c37:34, 11.16s/it, val_loss=0.651]\u001b[A\n"," 10%|▉         | 22/224 [04:04\u003c37:34, 11.16s/it, val_loss=0.418]\u001b[A\n"," 10%|█         | 23/224 [04:04\u003c34:41, 10.36s/it, val_loss=0.418]\u001b[A\n"," 10%|█         | 23/224 [04:16\u003c34:41, 10.36s/it, val_loss=0.871]\u001b[A\n"," 11%|█         | 24/224 [04:16\u003c36:10, 10.85s/it, val_loss=0.871]\u001b[A\n"," 11%|█         | 24/224 [04:31\u003c36:10, 10.85s/it, val_loss=0.722]\u001b[A\n"," 11%|█         | 25/224 [04:31\u003c39:21, 11.87s/it, val_loss=0.722]\u001b[A\n"," 11%|█         | 25/224 [04:37\u003c39:21, 11.87s/it, val_loss=0.685]\u001b[A\n"," 12%|█▏        | 26/224 [04:37\u003c34:15, 10.38s/it, val_loss=0.685]\u001b[A\n"," 12%|█▏        | 26/224 [04:46\u003c34:15, 10.38s/it, val_loss=0.585]\u001b[A\n"," 12%|█▏        | 27/224 [04:46\u003c32:33,  9.92s/it, val_loss=0.585]\u001b[A\n"," 12%|█▏        | 27/224 [04:58\u003c32:33,  9.92s/it, val_loss=0.647]\u001b[A\n"," 12%|█▎        | 28/224 [04:58\u003c33:47, 10.34s/it, val_loss=0.647]\u001b[A\n"," 12%|█▎        | 28/224 [05:09\u003c33:47, 10.34s/it, val_loss=0.888]\u001b[A\n"," 13%|█▎        | 29/224 [05:09\u003c34:24, 10.59s/it, val_loss=0.888]\u001b[A\n"," 13%|█▎        | 29/224 [05:24\u003c34:24, 10.59s/it, val_loss=0.574]\u001b[A\n"," 13%|█▎        | 30/224 [05:24\u003c38:16, 11.84s/it, val_loss=0.574]\u001b[A\n"," 13%|█▎        | 30/224 [05:35\u003c38:16, 11.84s/it, val_loss=0.806]\u001b[A\n"," 14%|█▍        | 31/224 [05:35\u003c37:50, 11.77s/it, val_loss=0.806]\u001b[A\n"," 14%|█▍        | 31/224 [05:45\u003c37:50, 11.77s/it, val_loss=0.885]\u001b[A\n"," 14%|█▍        | 32/224 [05:45\u003c36:16, 11.34s/it, val_loss=0.885]\u001b[A\n"," 14%|█▍        | 32/224 [05:56\u003c36:16, 11.34s/it, val_loss=0.503]\u001b[A\n"," 15%|█▍        | 33/224 [05:56\u003c35:04, 11.02s/it, val_loss=0.503]\u001b[A\n"," 15%|█▍        | 33/224 [06:05\u003c35:04, 11.02s/it, val_loss=0.686]\u001b[A\n"," 15%|█▌        | 34/224 [06:05\u003c32:57, 10.41s/it, val_loss=0.686]\u001b[A\n"," 15%|█▌        | 34/224 [06:16\u003c32:57, 10.41s/it, val_loss=0.701]\u001b[A\n"," 16%|█▌        | 35/224 [06:16\u003c34:01, 10.80s/it, val_loss=0.701]\u001b[A\n"," 16%|█▌        | 35/224 [06:28\u003c34:01, 10.80s/it, val_loss=0.688]\u001b[A\n"," 16%|█▌        | 36/224 [06:28\u003c34:50, 11.12s/it, val_loss=0.688]\u001b[A\n"," 16%|█▌        | 36/224 [06:36\u003c34:50, 11.12s/it, val_loss=0.683]\u001b[A\n"," 17%|█▋        | 37/224 [06:36\u003c31:09, 10.00s/it, val_loss=0.683]\u001b[A\n"," 17%|█▋        | 37/224 [06:48\u003c31:09, 10.00s/it, val_loss=0.66] \u001b[A\n"," 17%|█▋        | 38/224 [06:48\u003c32:46, 10.57s/it, val_loss=0.66]\u001b[A\n"," 17%|█▋        | 38/224 [07:01\u003c32:46, 10.57s/it, val_loss=0.502]\u001b[A\n"," 17%|█▋        | 39/224 [07:01\u003c35:30, 11.52s/it, val_loss=0.502]\u001b[A\n"," 17%|█▋        | 39/224 [07:15\u003c35:30, 11.52s/it, val_loss=0.545]\u001b[A\n"," 18%|█▊        | 40/224 [07:15\u003c37:41, 12.29s/it, val_loss=0.545]\u001b[A\n"," 18%|█▊        | 40/224 [07:27\u003c37:41, 12.29s/it, val_loss=1]    \u001b[A\n"," 18%|█▊        | 41/224 [07:27\u003c36:50, 12.08s/it, val_loss=1]\u001b[A\n"," 18%|█▊        | 41/224 [07:38\u003c36:50, 12.08s/it, val_loss=0.633]\u001b[A\n"," 19%|█▉        | 42/224 [07:38\u003c35:41, 11.77s/it, val_loss=0.633]\u001b[A\n"," 19%|█▉        | 42/224 [07:49\u003c35:41, 11.77s/it, val_loss=0.556]\u001b[A\n"," 19%|█▉        | 43/224 [07:49\u003c34:56, 11.58s/it, val_loss=0.556]\u001b[A\n"," 19%|█▉        | 43/224 [08:01\u003c34:56, 11.58s/it, val_loss=0.954]\u001b[A\n"," 20%|█▉        | 44/224 [08:01\u003c35:00, 11.67s/it, val_loss=0.954]\u001b[A\n"," 20%|█▉        | 44/224 [08:14\u003c35:00, 11.67s/it, val_loss=0.626]\u001b[A\n"," 20%|██        | 45/224 [08:14\u003c35:38, 11.95s/it, val_loss=0.626]\u001b[A\n"," 20%|██        | 45/224 [08:23\u003c35:38, 11.95s/it, val_loss=0.713]\u001b[A\n"," 21%|██        | 46/224 [08:23\u003c33:20, 11.24s/it, val_loss=0.713]\u001b[A\n"," 21%|██        | 46/224 [08:36\u003c33:20, 11.24s/it, val_loss=0.688]\u001b[A\n"," 21%|██        | 47/224 [08:36\u003c34:46, 11.79s/it, val_loss=0.688]\u001b[A\n"," 21%|██        | 47/224 [08:51\u003c34:46, 11.79s/it, val_loss=0.531]\u001b[A\n"," 21%|██▏       | 48/224 [08:51\u003c36:51, 12.57s/it, val_loss=0.531]\u001b[A\n"," 21%|██▏       | 48/224 [09:02\u003c36:51, 12.57s/it, val_loss=0.787]\u001b[A\n"," 22%|██▏       | 49/224 [09:02\u003c35:27, 12.16s/it, val_loss=0.787]\u001b[A\n"," 22%|██▏       | 49/224 [09:12\u003c35:27, 12.16s/it, val_loss=0.614]\u001b[A\n"," 22%|██▏       | 50/224 [09:12\u003c33:19, 11.49s/it, val_loss=0.614]\u001b[A\n"," 22%|██▏       | 50/224 [09:23\u003c33:19, 11.49s/it, val_loss=0.574]\u001b[A\n"," 23%|██▎       | 51/224 [09:23\u003c32:33, 11.29s/it, val_loss=0.574]\u001b[A\n"," 23%|██▎       | 51/224 [09:36\u003c32:33, 11.29s/it, val_loss=0.73] \u001b[A\n"," 23%|██▎       | 52/224 [09:36\u003c34:19, 11.98s/it, val_loss=0.73]\u001b[A\n"," 23%|██▎       | 52/224 [09:46\u003c34:19, 11.98s/it, val_loss=0.939]\u001b[A\n"," 24%|██▎       | 53/224 [09:46\u003c32:34, 11.43s/it, val_loss=0.939]\u001b[A\n"," 24%|██▎       | 53/224 [09:58\u003c32:34, 11.43s/it, val_loss=0.462]\u001b[A\n"," 24%|██▍       | 54/224 [09:58\u003c32:20, 11.41s/it, val_loss=0.462]\u001b[A\n"," 24%|██▍       | 54/224 [10:09\u003c32:20, 11.41s/it, val_loss=0.747]\u001b[A\n"," 25%|██▍       | 55/224 [10:09\u003c31:37, 11.23s/it, val_loss=0.747]\u001b[A\n"," 25%|██▍       | 55/224 [10:20\u003c31:37, 11.23s/it, val_loss=0.835]\u001b[A\n"," 25%|██▌       | 56/224 [10:20\u003c31:46, 11.35s/it, val_loss=0.835]\u001b[A\n"," 25%|██▌       | 56/224 [10:31\u003c31:46, 11.35s/it, val_loss=0.704]\u001b[A\n"," 25%|██▌       | 57/224 [10:31\u003c31:07, 11.18s/it, val_loss=0.704]\u001b[A\n"," 25%|██▌       | 57/224 [10:41\u003c31:07, 11.18s/it, val_loss=0.487]\u001b[A\n"," 26%|██▌       | 58/224 [10:41\u003c29:45, 10.76s/it, val_loss=0.487]\u001b[A\n"," 26%|██▌       | 58/224 [10:52\u003c29:45, 10.76s/it, val_loss=0.49] \u001b[A\n"," 26%|██▋       | 59/224 [10:52\u003c30:00, 10.91s/it, val_loss=0.49]\u001b[A\n"," 26%|██▋       | 59/224 [11:04\u003c30:00, 10.91s/it, val_loss=0.699]\u001b[A\n"," 27%|██▋       | 60/224 [11:04\u003c30:43, 11.24s/it, val_loss=0.699]\u001b[A\n"," 27%|██▋       | 60/224 [11:16\u003c30:43, 11.24s/it, val_loss=0.698]\u001b[A\n"," 27%|██▋       | 61/224 [11:16\u003c30:50, 11.35s/it, val_loss=0.698]\u001b[A\n"," 27%|██▋       | 61/224 [11:27\u003c30:50, 11.35s/it, val_loss=0.694]\u001b[A\n"," 28%|██▊       | 62/224 [11:27\u003c30:48, 11.41s/it, val_loss=0.694]\u001b[A\n"," 28%|██▊       | 62/224 [11:38\u003c30:48, 11.41s/it, val_loss=0.526]\u001b[A\n"," 28%|██▊       | 63/224 [11:38\u003c30:20, 11.31s/it, val_loss=0.526]\u001b[A\n"," 28%|██▊       | 63/224 [11:47\u003c30:20, 11.31s/it, val_loss=0.515]\u001b[A\n"," 29%|██▊       | 64/224 [11:47\u003c28:11, 10.57s/it, val_loss=0.515]\u001b[A\n"," 29%|██▊       | 64/224 [12:02\u003c28:11, 10.57s/it, val_loss=0.521]\u001b[A\n"," 29%|██▉       | 65/224 [12:02\u003c31:35, 11.92s/it, val_loss=0.521]\u001b[A\n"," 29%|██▉       | 65/224 [12:12\u003c31:35, 11.92s/it, val_loss=1.43] \u001b[A\n"," 29%|██▉       | 66/224 [12:12\u003c29:59, 11.39s/it, val_loss=1.43]\u001b[A\n"," 29%|██▉       | 66/224 [12:24\u003c29:59, 11.39s/it, val_loss=0.805]\u001b[A\n"," 30%|██▉       | 67/224 [12:24\u003c30:09, 11.53s/it, val_loss=0.805]\u001b[A\n"," 30%|██▉       | 67/224 [12:36\u003c30:09, 11.53s/it, val_loss=0.59] \u001b[A\n"," 30%|███       | 68/224 [12:36\u003c29:50, 11.47s/it, val_loss=0.59]\u001b[A\n"," 30%|███       | 68/224 [12:44\u003c29:50, 11.47s/it, val_loss=0.45]\u001b[A\n"," 31%|███       | 69/224 [12:44\u003c27:05, 10.49s/it, val_loss=0.45]\u001b[A\n"," 31%|███       | 69/224 [12:54\u003c27:05, 10.49s/it, val_loss=0.503]\u001b[A\n"," 31%|███▏      | 70/224 [12:54\u003c27:00, 10.53s/it, val_loss=0.503]\u001b[A\n"," 31%|███▏      | 70/224 [13:04\u003c27:00, 10.53s/it, val_loss=0.805]\u001b[A\n"," 32%|███▏      | 71/224 [13:04\u003c26:14, 10.29s/it, val_loss=0.805]\u001b[A\n"," 32%|███▏      | 71/224 [13:15\u003c26:14, 10.29s/it, val_loss=0.641]\u001b[A\n"," 32%|███▏      | 72/224 [13:15\u003c26:43, 10.55s/it, val_loss=0.641]\u001b[A\n"," 32%|███▏      | 72/224 [13:25\u003c26:43, 10.55s/it, val_loss=0.701]\u001b[A\n"," 33%|███▎      | 73/224 [13:25\u003c25:55, 10.30s/it, val_loss=0.701]\u001b[A\n"," 33%|███▎      | 73/224 [13:35\u003c25:55, 10.30s/it, val_loss=0.612]\u001b[A\n"," 33%|███▎      | 74/224 [13:35\u003c25:27, 10.18s/it, val_loss=0.612]\u001b[A\n"," 33%|███▎      | 74/224 [13:44\u003c25:27, 10.18s/it, val_loss=0.655]\u001b[A\n"," 33%|███▎      | 75/224 [13:44\u003c24:49, 10.00s/it, val_loss=0.655]\u001b[A\n"," 33%|███▎      | 75/224 [13:58\u003c24:49, 10.00s/it, val_loss=0.892]\u001b[A\n"," 34%|███▍      | 76/224 [13:58\u003c27:26, 11.12s/it, val_loss=0.892]\u001b[A\n"," 34%|███▍      | 76/224 [14:07\u003c27:26, 11.12s/it, val_loss=0.739]\u001b[A\n"," 34%|███▍      | 77/224 [14:07\u003c25:50, 10.55s/it, val_loss=0.739]\u001b[A\n"," 34%|███▍      | 77/224 [14:18\u003c25:50, 10.55s/it, val_loss=0.464]\u001b[A\n"," 35%|███▍      | 78/224 [14:18\u003c25:49, 10.62s/it, val_loss=0.464]\u001b[A\n"," 35%|███▍      | 78/224 [14:27\u003c25:49, 10.62s/it, val_loss=0.738]\u001b[A\n"," 35%|███▌      | 79/224 [14:27\u003c24:27, 10.12s/it, val_loss=0.738]\u001b[A\n"," 35%|███▌      | 79/224 [14:40\u003c24:27, 10.12s/it, val_loss=0.424]\u001b[A\n"," 36%|███▌      | 80/224 [14:40\u003c26:21, 10.98s/it, val_loss=0.424]\u001b[A\n"," 36%|███▌      | 80/224 [14:49\u003c26:21, 10.98s/it, val_loss=0.836]\u001b[A\n"," 36%|███▌      | 81/224 [14:49\u003c24:59, 10.49s/it, val_loss=0.836]\u001b[A\n"," 36%|███▌      | 81/224 [15:02\u003c24:59, 10.49s/it, val_loss=0.718]\u001b[A\n"," 37%|███▋      | 82/224 [15:02\u003c25:58, 10.97s/it, val_loss=0.718]\u001b[A\n"," 37%|███▋      | 82/224 [15:15\u003c25:58, 10.97s/it, val_loss=0.62] \u001b[A\n"," 37%|███▋      | 83/224 [15:15\u003c27:27, 11.69s/it, val_loss=0.62]\u001b[A\n"," 37%|███▋      | 83/224 [15:27\u003c27:27, 11.69s/it, val_loss=0.418]\u001b[A\n"," 38%|███▊      | 84/224 [15:27\u003c27:27, 11.77s/it, val_loss=0.418]\u001b[A\n"," 38%|███▊      | 84/224 [15:41\u003c27:27, 11.77s/it, val_loss=0.483]\u001b[A\n"," 38%|███▊      | 85/224 [15:41\u003c28:36, 12.35s/it, val_loss=0.483]\u001b[A\n"," 38%|███▊      | 85/224 [15:51\u003c28:36, 12.35s/it, val_loss=0.82] \u001b[A\n"," 38%|███▊      | 86/224 [15:51\u003c26:57, 11.72s/it, val_loss=0.82]\u001b[A\n"," 38%|███▊      | 86/224 [16:02\u003c26:57, 11.72s/it, val_loss=0.643]\u001b[A\n"," 39%|███▉      | 87/224 [16:02\u003c26:17, 11.52s/it, val_loss=0.643]\u001b[A\n"," 39%|███▉      | 87/224 [16:14\u003c26:17, 11.52s/it, val_loss=0.441]\u001b[A\n"," 39%|███▉      | 88/224 [16:14\u003c26:36, 11.74s/it, val_loss=0.441]\u001b[A\n"," 39%|███▉      | 88/224 [16:26\u003c26:36, 11.74s/it, val_loss=0.519]\u001b[A\n"," 40%|███▉      | 89/224 [16:26\u003c26:12, 11.65s/it, val_loss=0.519]\u001b[A\n"," 40%|███▉      | 89/224 [16:39\u003c26:12, 11.65s/it, val_loss=0.619]\u001b[A\n"," 40%|████      | 90/224 [16:39\u003c27:21, 12.25s/it, val_loss=0.619]\u001b[A\n"," 40%|████      | 90/224 [16:50\u003c27:21, 12.25s/it, val_loss=0.526]\u001b[A\n"," 41%|████      | 91/224 [16:50\u003c26:10, 11.81s/it, val_loss=0.526]\u001b[A\n"," 41%|████      | 91/224 [17:01\u003c26:10, 11.81s/it, val_loss=0.615]\u001b[A\n"," 41%|████      | 92/224 [17:01\u003c25:28, 11.58s/it, val_loss=0.615]\u001b[A\n"," 41%|████      | 92/224 [17:14\u003c25:28, 11.58s/it, val_loss=0.478]\u001b[A\n"," 42%|████▏     | 93/224 [17:14\u003c26:00, 11.91s/it, val_loss=0.478]\u001b[A\n"," 42%|████▏     | 93/224 [17:23\u003c26:00, 11.91s/it, val_loss=0.667]\u001b[A\n"," 42%|████▏     | 94/224 [17:23\u003c24:15, 11.19s/it, val_loss=0.667]\u001b[A\n"," 42%|████▏     | 94/224 [17:36\u003c24:15, 11.19s/it, val_loss=0.922]\u001b[A\n"," 42%|████▏     | 95/224 [17:36\u003c24:47, 11.53s/it, val_loss=0.922]\u001b[A\n"," 42%|████▏     | 95/224 [17:50\u003c24:47, 11.53s/it, val_loss=0.66] \u001b[A\n"," 43%|████▎     | 96/224 [17:50\u003c26:40, 12.50s/it, val_loss=0.66]\u001b[A\n"," 43%|████▎     | 96/224 [18:01\u003c26:40, 12.50s/it, val_loss=0.445]\u001b[A\n"," 43%|████▎     | 97/224 [18:01\u003c25:21, 11.98s/it, val_loss=0.445]\u001b[A\n"," 43%|████▎     | 97/224 [18:12\u003c25:21, 11.98s/it, val_loss=0.668]\u001b[A\n"," 44%|████▍     | 98/224 [18:12\u003c24:27, 11.65s/it, val_loss=0.668]\u001b[A\n"," 44%|████▍     | 98/224 [18:20\u003c24:27, 11.65s/it, val_loss=0.63] \u001b[A\n"," 44%|████▍     | 99/224 [18:20\u003c22:07, 10.62s/it, val_loss=0.63]\u001b[A\n"," 44%|████▍     | 99/224 [18:31\u003c22:07, 10.62s/it, val_loss=0.495]\u001b[A\n"," 45%|████▍     | 100/224 [18:31\u003c22:16, 10.78s/it, val_loss=0.495]\u001b[A\n"," 45%|████▍     | 100/224 [18:41\u003c22:16, 10.78s/it, val_loss=0.602]\u001b[A\n"," 45%|████▌     | 101/224 [18:41\u003c21:32, 10.51s/it, val_loss=0.602]\u001b[A\n"," 45%|████▌     | 101/224 [18:54\u003c21:32, 10.51s/it, val_loss=0.613]\u001b[A\n"," 46%|████▌     | 102/224 [18:54\u003c22:33, 11.09s/it, val_loss=0.613]\u001b[A\n"," 46%|████▌     | 102/224 [19:03\u003c22:33, 11.09s/it, val_loss=0.622]\u001b[A\n"," 46%|████▌     | 103/224 [19:03\u003c21:10, 10.50s/it, val_loss=0.622]\u001b[A\n"," 46%|████▌     | 103/224 [19:11\u003c21:10, 10.50s/it, val_loss=0.656]\u001b[A\n"," 46%|████▋     | 104/224 [19:11\u003c19:24,  9.70s/it, val_loss=0.656]\u001b[A\n"," 46%|████▋     | 104/224 [19:21\u003c19:24,  9.70s/it, val_loss=0.694]\u001b[A\n"," 47%|████▋     | 105/224 [19:21\u003c19:21,  9.76s/it, val_loss=0.694]\u001b[A\n"," 47%|████▋     | 105/224 [19:33\u003c19:21,  9.76s/it, val_loss=0.683]\u001b[A\n"," 47%|████▋     | 106/224 [19:33\u003c20:44, 10.55s/it, val_loss=0.683]\u001b[A\n"," 47%|████▋     | 106/224 [19:43\u003c20:44, 10.55s/it, val_loss=0.578]\u001b[A\n"," 48%|████▊     | 107/224 [19:43\u003c20:07, 10.32s/it, val_loss=0.578]\u001b[A\n"," 48%|████▊     | 107/224 [19:53\u003c20:07, 10.32s/it, val_loss=0.606]\u001b[A\n"," 48%|████▊     | 108/224 [19:53\u003c19:55, 10.31s/it, val_loss=0.606]\u001b[A\n"," 48%|████▊     | 108/224 [20:09\u003c19:55, 10.31s/it, val_loss=0.858]\u001b[A\n"," 49%|████▊     | 109/224 [20:09\u003c23:00, 12.01s/it, val_loss=0.858]\u001b[A\n"," 49%|████▊     | 109/224 [20:19\u003c23:00, 12.01s/it, val_loss=0.693]\u001b[A\n"," 49%|████▉     | 110/224 [20:19\u003c21:57, 11.56s/it, val_loss=0.693]\u001b[A\n"," 49%|████▉     | 110/224 [20:30\u003c21:57, 11.56s/it, val_loss=0.635]\u001b[A\n"," 50%|████▉     | 111/224 [20:30\u003c21:10, 11.24s/it, val_loss=0.635]\u001b[A\n"," 50%|████▉     | 111/224 [20:42\u003c21:10, 11.24s/it, val_loss=0.546]\u001b[A\n"," 50%|█████     | 112/224 [20:42\u003c21:26, 11.48s/it, val_loss=0.546]\u001b[A\n"," 50%|█████     | 112/224 [20:54\u003c21:26, 11.48s/it, val_loss=0.78] \u001b[A\n"," 50%|█████     | 113/224 [20:54\u003c21:17, 11.51s/it, val_loss=0.78]\u001b[A\n"," 50%|█████     | 113/224 [21:07\u003c21:17, 11.51s/it, val_loss=0.683]\u001b[A\n"," 51%|█████     | 114/224 [21:07\u003c22:21, 12.19s/it, val_loss=0.683]\u001b[A\n"," 51%|█████     | 114/224 [21:19\u003c22:21, 12.19s/it, val_loss=0.681]\u001b[A\n"," 51%|█████▏    | 115/224 [21:19\u003c21:42, 11.95s/it, val_loss=0.681]\u001b[A\n"," 51%|█████▏    | 115/224 [21:33\u003c21:42, 11.95s/it, val_loss=0.578]\u001b[A\n"," 52%|█████▏    | 116/224 [21:33\u003c22:41, 12.61s/it, val_loss=0.578]\u001b[A\n"," 52%|█████▏    | 116/224 [21:42\u003c22:41, 12.61s/it, val_loss=0.603]\u001b[A\n"," 52%|█████▏    | 117/224 [21:42\u003c20:49, 11.68s/it, val_loss=0.603]\u001b[A\n"," 52%|█████▏    | 117/224 [21:54\u003c20:49, 11.68s/it, val_loss=0.54] \u001b[A\n"," 53%|█████▎    | 118/224 [21:54\u003c20:45, 11.75s/it, val_loss=0.54]\u001b[A\n"," 53%|█████▎    | 118/224 [22:05\u003c20:45, 11.75s/it, val_loss=0.601]\u001b[A\n"," 53%|█████▎    | 119/224 [22:05\u003c20:12, 11.55s/it, val_loss=0.601]\u001b[A\n"," 53%|█████▎    | 119/224 [22:16\u003c20:12, 11.55s/it, val_loss=0.755]\u001b[A\n"," 54%|█████▎    | 120/224 [22:16\u003c19:29, 11.24s/it, val_loss=0.755]\u001b[A\n"," 54%|█████▎    | 120/224 [22:26\u003c19:29, 11.24s/it, val_loss=0.487]\u001b[A\n"," 54%|█████▍    | 121/224 [22:26\u003c18:31, 10.79s/it, val_loss=0.487]\u001b[A\n"," 54%|█████▍    | 121/224 [22:36\u003c18:31, 10.79s/it, val_loss=0.764]\u001b[A\n"," 54%|█████▍    | 122/224 [22:36\u003c17:52, 10.51s/it, val_loss=0.764]\u001b[A\n"," 54%|█████▍    | 122/224 [22:47\u003c17:52, 10.51s/it, val_loss=0.598]\u001b[A\n"," 55%|█████▍    | 123/224 [22:47\u003c18:09, 10.79s/it, val_loss=0.598]\u001b[A\n"," 55%|█████▍    | 123/224 [23:00\u003c18:09, 10.79s/it, val_loss=0.488]\u001b[A\n"," 55%|█████▌    | 124/224 [23:00\u003c19:02, 11.43s/it, val_loss=0.488]\u001b[A\n"," 55%|█████▌    | 124/224 [23:10\u003c19:02, 11.43s/it, val_loss=0.64] \u001b[A\n"," 56%|█████▌    | 125/224 [23:10\u003c17:57, 10.88s/it, val_loss=0.64]\u001b[A\n"," 56%|█████▌    | 125/224 [23:22\u003c17:57, 10.88s/it, val_loss=0.491]\u001b[A\n"," 56%|█████▋    | 126/224 [23:22\u003c18:39, 11.42s/it, val_loss=0.491]\u001b[A\n"," 56%|█████▋    | 126/224 [23:31\u003c18:39, 11.42s/it, val_loss=0.865]\u001b[A\n"," 57%|█████▋    | 127/224 [23:31\u003c17:09, 10.61s/it, val_loss=0.865]\u001b[A\n"," 57%|█████▋    | 127/224 [23:40\u003c17:09, 10.61s/it, val_loss=0.709]\u001b[A\n"," 57%|█████▋    | 128/224 [23:40\u003c16:09, 10.10s/it, val_loss=0.709]\u001b[A\n"," 57%|█████▋    | 128/224 [23:53\u003c16:09, 10.10s/it, val_loss=0.427]\u001b[A\n"," 58%|█████▊    | 129/224 [23:53\u003c17:27, 11.03s/it, val_loss=0.427]\u001b[A\n"," 58%|█████▊    | 129/224 [24:04\u003c17:27, 11.03s/it, val_loss=0.769]\u001b[A\n"," 58%|█████▊    | 130/224 [24:04\u003c17:21, 11.08s/it, val_loss=0.769]\u001b[A\n"," 58%|█████▊    | 130/224 [24:16\u003c17:21, 11.08s/it, val_loss=0.775]\u001b[A\n"," 58%|█████▊    | 131/224 [24:16\u003c17:20, 11.19s/it, val_loss=0.775]\u001b[A\n"," 58%|█████▊    | 131/224 [24:29\u003c17:20, 11.19s/it, val_loss=0.603]\u001b[A\n"," 59%|█████▉    | 132/224 [24:29\u003c18:22, 11.98s/it, val_loss=0.603]\u001b[A\n"," 59%|█████▉    | 132/224 [24:42\u003c18:22, 11.98s/it, val_loss=0.634]\u001b[A\n"," 59%|█████▉    | 133/224 [24:42\u003c18:17, 12.06s/it, val_loss=0.634]\u001b[A\n"," 59%|█████▉    | 133/224 [24:56\u003c18:17, 12.06s/it, val_loss=0.616]\u001b[A\n"," 60%|█████▉    | 134/224 [24:56\u003c19:00, 12.67s/it, val_loss=0.616]\u001b[A\n"," 60%|█████▉    | 134/224 [25:06\u003c19:00, 12.67s/it, val_loss=0.961]\u001b[A\n"," 60%|██████    | 135/224 [25:06\u003c17:49, 12.02s/it, val_loss=0.961]\u001b[A\n"," 60%|██████    | 135/224 [25:18\u003c17:49, 12.02s/it, val_loss=0.628]\u001b[A\n"," 61%|██████    | 136/224 [25:18\u003c17:39, 12.03s/it, val_loss=0.628]\u001b[A\n"," 61%|██████    | 136/224 [25:29\u003c17:39, 12.03s/it, val_loss=0.624]\u001b[A\n"," 61%|██████    | 137/224 [25:29\u003c16:38, 11.47s/it, val_loss=0.624]\u001b[A\n"," 61%|██████    | 137/224 [25:38\u003c16:38, 11.47s/it, val_loss=0.544]\u001b[A\n"," 62%|██████▏   | 138/224 [25:38\u003c15:38, 10.91s/it, val_loss=0.544]\u001b[A\n"," 62%|██████▏   | 138/224 [25:55\u003c15:38, 10.91s/it, val_loss=0.685]\u001b[A\n"," 62%|██████▏   | 139/224 [25:55\u003c17:47, 12.56s/it, val_loss=0.685]\u001b[A\n"," 62%|██████▏   | 139/224 [26:06\u003c17:47, 12.56s/it, val_loss=0.409]\u001b[A\n"," 62%|██████▎   | 140/224 [26:06\u003c16:57, 12.11s/it, val_loss=0.409]\u001b[A\n"," 62%|██████▎   | 140/224 [26:19\u003c16:57, 12.11s/it, val_loss=0.629]\u001b[A\n"," 63%|██████▎   | 141/224 [26:19\u003c17:27, 12.62s/it, val_loss=0.629]\u001b[A\n"," 63%|██████▎   | 141/224 [26:32\u003c17:27, 12.62s/it, val_loss=0.548]\u001b[A\n"," 63%|██████▎   | 142/224 [26:32\u003c17:08, 12.54s/it, val_loss=0.548]\u001b[A\n"," 63%|██████▎   | 142/224 [26:45\u003c17:08, 12.54s/it, val_loss=0.611]\u001b[A\n"," 64%|██████▍   | 143/224 [26:45\u003c17:14, 12.78s/it, val_loss=0.611]\u001b[A\n"," 64%|██████▍   | 143/224 [26:56\u003c17:14, 12.78s/it, val_loss=0.43] \u001b[A\n"," 64%|██████▍   | 144/224 [26:56\u003c16:06, 12.09s/it, val_loss=0.43]\u001b[A\n"," 64%|██████▍   | 144/224 [27:06\u003c16:06, 12.09s/it, val_loss=0.613]\u001b[A\n"," 65%|██████▍   | 145/224 [27:06\u003c15:19, 11.64s/it, val_loss=0.613]\u001b[A\n"," 65%|██████▍   | 145/224 [27:14\u003c15:19, 11.64s/it, val_loss=0.526]\u001b[A\n"," 65%|██████▌   | 146/224 [27:14\u003c13:49, 10.63s/it, val_loss=0.526]\u001b[A\n"," 65%|██████▌   | 146/224 [27:24\u003c13:49, 10.63s/it, val_loss=0.531]\u001b[A\n"," 66%|██████▌   | 147/224 [27:24\u003c13:06, 10.21s/it, val_loss=0.531]\u001b[A\n"," 66%|██████▌   | 147/224 [27:33\u003c13:06, 10.21s/it, val_loss=0.706]\u001b[A\n"," 66%|██████▌   | 148/224 [27:33\u003c12:42, 10.03s/it, val_loss=0.706]\u001b[A\n"," 66%|██████▌   | 148/224 [27:43\u003c12:42, 10.03s/it, val_loss=0.716]\u001b[A\n"," 67%|██████▋   | 149/224 [27:43\u003c12:25,  9.94s/it, val_loss=0.716]\u001b[A\n"," 67%|██████▋   | 149/224 [27:55\u003c12:25,  9.94s/it, val_loss=0.645]\u001b[A\n"," 67%|██████▋   | 150/224 [27:55\u003c12:58, 10.52s/it, val_loss=0.645]\u001b[A\n"," 67%|██████▋   | 150/224 [28:08\u003c12:58, 10.52s/it, val_loss=0.614]\u001b[A\n"," 67%|██████▋   | 151/224 [28:08\u003c13:52, 11.40s/it, val_loss=0.614]\u001b[A\n"," 67%|██████▋   | 151/224 [28:18\u003c13:52, 11.40s/it, val_loss=0.947]\u001b[A\n"," 68%|██████▊   | 152/224 [28:18\u003c12:54, 10.75s/it, val_loss=0.947]\u001b[A\n"," 68%|██████▊   | 152/224 [28:30\u003c12:54, 10.75s/it, val_loss=0.736]\u001b[A\n"," 68%|██████▊   | 153/224 [28:30\u003c13:22, 11.30s/it, val_loss=0.736]\u001b[A\n"," 68%|██████▊   | 153/224 [28:42\u003c13:22, 11.30s/it, val_loss=0.638]\u001b[A\n"," 69%|██████▉   | 154/224 [28:42\u003c13:24, 11.49s/it, val_loss=0.638]\u001b[A\n"," 69%|██████▉   | 154/224 [28:56\u003c13:24, 11.49s/it, val_loss=0.636]\u001b[A\n"," 69%|██████▉   | 155/224 [28:56\u003c13:55, 12.11s/it, val_loss=0.636]\u001b[A\n"," 69%|██████▉   | 155/224 [29:05\u003c13:55, 12.11s/it, val_loss=0.479]\u001b[A\n"," 70%|██████▉   | 156/224 [29:05\u003c12:54, 11.39s/it, val_loss=0.479]\u001b[A\n"," 70%|██████▉   | 156/224 [29:18\u003c12:54, 11.39s/it, val_loss=0.581]\u001b[A\n"," 70%|███████   | 157/224 [29:18\u003c13:08, 11.77s/it, val_loss=0.581]\u001b[A\n"," 70%|███████   | 157/224 [29:29\u003c13:08, 11.77s/it, val_loss=0.823]\u001b[A\n"," 71%|███████   | 158/224 [29:29\u003c12:47, 11.62s/it, val_loss=0.823]\u001b[A\n"," 71%|███████   | 158/224 [29:40\u003c12:47, 11.62s/it, val_loss=0.406]\u001b[A\n"," 71%|███████   | 159/224 [29:40\u003c12:13, 11.29s/it, val_loss=0.406]\u001b[A\n"," 71%|███████   | 159/224 [29:52\u003c12:13, 11.29s/it, val_loss=0.712]\u001b[A\n"," 71%|███████▏  | 160/224 [29:52\u003c12:24, 11.64s/it, val_loss=0.712]\u001b[A\n"," 71%|███████▏  | 160/224 [30:02\u003c12:24, 11.64s/it, val_loss=0.675]\u001b[A\n"," 72%|███████▏  | 161/224 [30:02\u003c11:36, 11.06s/it, val_loss=0.675]\u001b[A\n"," 72%|███████▏  | 161/224 [30:15\u003c11:36, 11.06s/it, val_loss=0.643]\u001b[A\n"," 72%|███████▏  | 162/224 [30:15\u003c11:53, 11.51s/it, val_loss=0.643]\u001b[A\n"," 72%|███████▏  | 162/224 [30:27\u003c11:53, 11.51s/it, val_loss=0.63] \u001b[A\n"," 73%|███████▎  | 163/224 [30:27\u003c11:50, 11.65s/it, val_loss=0.63]\u001b[A\n"," 73%|███████▎  | 163/224 [30:36\u003c11:50, 11.65s/it, val_loss=0.494]\u001b[A\n"," 73%|███████▎  | 164/224 [30:36\u003c10:56, 10.95s/it, val_loss=0.494]\u001b[A\n"," 73%|███████▎  | 164/224 [30:46\u003c10:56, 10.95s/it, val_loss=0.893]\u001b[A\n"," 74%|███████▎  | 165/224 [30:46\u003c10:33, 10.74s/it, val_loss=0.893]\u001b[A\n"," 74%|███████▎  | 165/224 [31:00\u003c10:33, 10.74s/it, val_loss=0.852]\u001b[A\n"," 74%|███████▍  | 166/224 [31:00\u003c11:10, 11.57s/it, val_loss=0.852]\u001b[A\n"," 74%|███████▍  | 166/224 [31:15\u003c11:10, 11.57s/it, val_loss=0.494]\u001b[A\n"," 75%|███████▍  | 167/224 [31:15\u003c11:58, 12.61s/it, val_loss=0.494]\u001b[A\n"," 75%|███████▍  | 167/224 [31:25\u003c11:58, 12.61s/it, val_loss=0.671]\u001b[A\n"," 75%|███████▌  | 168/224 [31:25\u003c11:10, 11.97s/it, val_loss=0.671]\u001b[A\n"," 75%|███████▌  | 168/224 [31:37\u003c11:10, 11.97s/it, val_loss=0.512]\u001b[A\n"," 75%|███████▌  | 169/224 [31:37\u003c11:05, 12.09s/it, val_loss=0.512]\u001b[A\n"," 75%|███████▌  | 169/224 [31:46\u003c11:05, 12.09s/it, val_loss=0.694]\u001b[A\n"," 76%|███████▌  | 170/224 [31:46\u003c10:00, 11.11s/it, val_loss=0.694]\u001b[A\n"," 76%|███████▌  | 170/224 [31:58\u003c10:00, 11.11s/it, val_loss=0.681]\u001b[A\n"," 76%|███████▋  | 171/224 [31:58\u003c10:05, 11.43s/it, val_loss=0.681]\u001b[A\n"," 76%|███████▋  | 171/224 [32:12\u003c10:05, 11.43s/it, val_loss=0.514]\u001b[A\n"," 77%|███████▋  | 172/224 [32:12\u003c10:26, 12.05s/it, val_loss=0.514]\u001b[A\n"," 77%|███████▋  | 172/224 [32:23\u003c10:26, 12.05s/it, val_loss=0.827]\u001b[A\n"," 77%|███████▋  | 173/224 [32:23\u003c09:56, 11.69s/it, val_loss=0.827]\u001b[A\n"," 77%|███████▋  | 173/224 [32:32\u003c09:56, 11.69s/it, val_loss=0.653]\u001b[A\n"," 78%|███████▊  | 174/224 [32:32\u003c09:09, 11.00s/it, val_loss=0.653]\u001b[A\n"," 78%|███████▊  | 174/224 [32:44\u003c09:09, 11.00s/it, val_loss=0.604]\u001b[A\n"," 78%|███████▊  | 175/224 [32:44\u003c09:11, 11.26s/it, val_loss=0.604]\u001b[A\n"," 78%|███████▊  | 175/224 [32:52\u003c09:11, 11.26s/it, val_loss=0.608]\u001b[A\n"," 79%|███████▊  | 176/224 [32:52\u003c08:14, 10.30s/it, val_loss=0.608]\u001b[A\n"," 79%|███████▊  | 176/224 [33:00\u003c08:14, 10.30s/it, val_loss=0.535]\u001b[A\n"," 79%|███████▉  | 177/224 [33:00\u003c07:29,  9.57s/it, val_loss=0.535]\u001b[A\n"," 79%|███████▉  | 177/224 [33:10\u003c07:29,  9.57s/it, val_loss=0.564]\u001b[A\n"," 79%|███████▉  | 178/224 [33:11\u003c07:33,  9.85s/it, val_loss=0.564]\u001b[A\n"," 79%|███████▉  | 178/224 [33:27\u003c07:33,  9.85s/it, val_loss=0.585]\u001b[A\n"," 80%|███████▉  | 179/224 [33:27\u003c08:55, 11.90s/it, val_loss=0.585]\u001b[A\n"," 80%|███████▉  | 179/224 [33:39\u003c08:55, 11.90s/it, val_loss=0.646]\u001b[A\n"," 80%|████████  | 180/224 [33:39\u003c08:47, 11.98s/it, val_loss=0.646]\u001b[A\n"," 80%|████████  | 180/224 [33:53\u003c08:47, 11.98s/it, val_loss=0.57] \u001b[A\n"," 81%|████████  | 181/224 [33:53\u003c09:02, 12.61s/it, val_loss=0.57]\u001b[A\n"," 81%|████████  | 181/224 [34:03\u003c09:02, 12.61s/it, val_loss=0.349]\u001b[A\n"," 81%|████████▏ | 182/224 [34:03\u003c08:13, 11.76s/it, val_loss=0.349]\u001b[A\n"," 81%|████████▏ | 182/224 [34:16\u003c08:13, 11.76s/it, val_loss=0.691]\u001b[A\n"," 82%|████████▏ | 183/224 [34:16\u003c08:09, 11.93s/it, val_loss=0.691]\u001b[A\n"," 82%|████████▏ | 183/224 [34:27\u003c08:09, 11.93s/it, val_loss=0.458]\u001b[A\n"," 82%|████████▏ | 184/224 [34:27\u003c07:53, 11.83s/it, val_loss=0.458]\u001b[A\n"," 82%|████████▏ | 184/224 [34:41\u003c07:53, 11.83s/it, val_loss=0.951]\u001b[A\n"," 83%|████████▎ | 185/224 [34:41\u003c08:03, 12.38s/it, val_loss=0.951]\u001b[A\n"," 83%|████████▎ | 185/224 [34:52\u003c08:03, 12.38s/it, val_loss=0.707]\u001b[A\n"," 83%|████████▎ | 186/224 [34:52\u003c07:35, 11.98s/it, val_loss=0.707]\u001b[A\n"," 83%|████████▎ | 186/224 [35:02\u003c07:35, 11.98s/it, val_loss=0.601]\u001b[A\n"," 83%|████████▎ | 187/224 [35:02\u003c07:03, 11.45s/it, val_loss=0.601]\u001b[A\n"," 83%|████████▎ | 187/224 [35:13\u003c07:03, 11.45s/it, val_loss=0.757]\u001b[A\n"," 84%|████████▍ | 188/224 [35:13\u003c06:44, 11.22s/it, val_loss=0.757]\u001b[A\n"," 84%|████████▍ | 188/224 [35:27\u003c06:44, 11.22s/it, val_loss=0.693]\u001b[A\n"," 84%|████████▍ | 189/224 [35:27\u003c07:00, 12.02s/it, val_loss=0.693]\u001b[A\n"," 84%|████████▍ | 189/224 [35:35\u003c07:00, 12.02s/it, val_loss=0.743]\u001b[A\n"," 85%|████████▍ | 190/224 [35:35\u003c06:13, 10.98s/it, val_loss=0.743]\u001b[A\n"," 85%|████████▍ | 190/224 [35:49\u003c06:13, 10.98s/it, val_loss=0.778]\u001b[A\n"," 85%|████████▌ | 191/224 [35:49\u003c06:26, 11.71s/it, val_loss=0.778]\u001b[A\n"," 85%|████████▌ | 191/224 [36:00\u003c06:26, 11.71s/it, val_loss=0.708]\u001b[A\n"," 86%|████████▌ | 192/224 [36:00\u003c06:08, 11.52s/it, val_loss=0.708]\u001b[A\n"," 86%|████████▌ | 192/224 [36:11\u003c06:08, 11.52s/it, val_loss=0.757]\u001b[A\n"," 86%|████████▌ | 193/224 [36:11\u003c05:55, 11.47s/it, val_loss=0.757]\u001b[A\n"," 86%|████████▌ | 193/224 [36:22\u003c05:55, 11.47s/it, val_loss=0.551]\u001b[A\n"," 87%|████████▋ | 194/224 [36:22\u003c05:35, 11.18s/it, val_loss=0.551]\u001b[A\n"," 87%|████████▋ | 194/224 [36:35\u003c05:35, 11.18s/it, val_loss=0.625]\u001b[A\n"," 87%|████████▋ | 195/224 [36:35\u003c05:40, 11.73s/it, val_loss=0.625]\u001b[A\n"," 87%|████████▋ | 195/224 [36:45\u003c05:40, 11.73s/it, val_loss=0.486]\u001b[A\n"," 88%|████████▊ | 196/224 [36:45\u003c05:14, 11.24s/it, val_loss=0.486]\u001b[A\n"," 88%|████████▊ | 196/224 [36:54\u003c05:14, 11.24s/it, val_loss=0.63] \u001b[A\n"," 88%|████████▊ | 197/224 [36:54\u003c04:48, 10.70s/it, val_loss=0.63]\u001b[A\n"," 88%|████████▊ | 197/224 [37:05\u003c04:48, 10.70s/it, val_loss=0.627]\u001b[A\n"," 88%|████████▊ | 198/224 [37:05\u003c04:41, 10.81s/it, val_loss=0.627]\u001b[A\n"," 88%|████████▊ | 198/224 [37:16\u003c04:41, 10.81s/it, val_loss=0.578]\u001b[A\n"," 89%|████████▉ | 199/224 [37:16\u003c04:27, 10.70s/it, val_loss=0.578]\u001b[A\n"," 89%|████████▉ | 199/224 [37:28\u003c04:27, 10.70s/it, val_loss=0.642]\u001b[A\n"," 89%|████████▉ | 200/224 [37:28\u003c04:29, 11.24s/it, val_loss=0.642]\u001b[A\n"," 89%|████████▉ | 200/224 [37:41\u003c04:29, 11.24s/it, val_loss=0.835]\u001b[A\n"," 90%|████████▉ | 201/224 [37:41\u003c04:29, 11.71s/it, val_loss=0.835]\u001b[A\n"," 90%|████████▉ | 201/224 [37:50\u003c04:29, 11.71s/it, val_loss=0.781]\u001b[A\n"," 90%|█████████ | 202/224 [37:50\u003c04:02, 11.01s/it, val_loss=0.781]\u001b[A\n"," 90%|█████████ | 202/224 [38:01\u003c04:02, 11.01s/it, val_loss=0.445]\u001b[A\n"," 91%|█████████ | 203/224 [38:01\u003c03:48, 10.87s/it, val_loss=0.445]\u001b[A\n"," 91%|█████████ | 203/224 [38:14\u003c03:48, 10.87s/it, val_loss=0.664]\u001b[A\n"," 91%|█████████ | 204/224 [38:14\u003c03:48, 11.44s/it, val_loss=0.664]\u001b[A\n"," 91%|█████████ | 204/224 [38:24\u003c03:48, 11.44s/it, val_loss=0.577]\u001b[A\n"," 92%|█████████▏| 205/224 [38:24\u003c03:33, 11.23s/it, val_loss=0.577]\u001b[A\n"," 92%|█████████▏| 205/224 [38:36\u003c03:33, 11.23s/it, val_loss=0.832]\u001b[A\n"," 92%|█████████▏| 206/224 [38:36\u003c03:22, 11.24s/it, val_loss=0.832]\u001b[A\n"," 92%|█████████▏| 206/224 [38:44\u003c03:22, 11.24s/it, val_loss=0.612]\u001b[A\n"," 92%|█████████▏| 207/224 [38:44\u003c02:58, 10.50s/it, val_loss=0.612]\u001b[A\n"," 92%|█████████▏| 207/224 [38:58\u003c02:58, 10.50s/it, val_loss=0.48] \u001b[A\n"," 93%|█████████▎| 208/224 [38:58\u003c03:02, 11.43s/it, val_loss=0.48]\u001b[A\n"," 93%|█████████▎| 208/224 [39:09\u003c03:02, 11.43s/it, val_loss=0.674]\u001b[A\n"," 93%|█████████▎| 209/224 [39:09\u003c02:48, 11.24s/it, val_loss=0.674]\u001b[A\n"," 93%|█████████▎| 209/224 [39:23\u003c02:48, 11.24s/it, val_loss=0.707]\u001b[A\n"," 94%|█████████▍| 210/224 [39:23\u003c02:48, 12.05s/it, val_loss=0.707]\u001b[A\n"," 94%|█████████▍| 210/224 [39:32\u003c02:48, 12.05s/it, val_loss=0.569]\u001b[A\n"," 94%|█████████▍| 211/224 [39:32\u003c02:24, 11.15s/it, val_loss=0.569]\u001b[A\n"," 94%|█████████▍| 211/224 [39:40\u003c02:24, 11.15s/it, val_loss=0.596]\u001b[A\n"," 95%|█████████▍| 212/224 [39:40\u003c02:03, 10.31s/it, val_loss=0.596]\u001b[A\n"," 95%|█████████▍| 212/224 [39:51\u003c02:03, 10.31s/it, val_loss=0.692]\u001b[A\n"," 95%|█████████▌| 213/224 [39:51\u003c01:54, 10.40s/it, val_loss=0.692]\u001b[A\n"," 95%|█████████▌| 213/224 [40:05\u003c01:54, 10.40s/it, val_loss=0.528]\u001b[A\n"," 96%|█████████▌| 214/224 [40:05\u003c01:54, 11.44s/it, val_loss=0.528]\u001b[A\n"," 96%|█████████▌| 214/224 [40:15\u003c01:54, 11.44s/it, val_loss=0.829]\u001b[A\n"," 96%|█████████▌| 215/224 [40:15\u003c01:39, 11.04s/it, val_loss=0.829]\u001b[A\n"," 96%|█████████▌| 215/224 [40:26\u003c01:39, 11.04s/it, val_loss=0.606]\u001b[A\n"," 96%|█████████▋| 216/224 [40:26\u003c01:28, 11.02s/it, val_loss=0.606]\u001b[A\n"," 96%|█████████▋| 216/224 [40:36\u003c01:28, 11.02s/it, val_loss=0.824]\u001b[A\n"," 97%|█████████▋| 217/224 [40:36\u003c01:16, 10.95s/it, val_loss=0.824]\u001b[A\n"," 97%|█████████▋| 217/224 [40:45\u003c01:16, 10.95s/it, val_loss=0.811]\u001b[A\n"," 97%|█████████▋| 218/224 [40:45\u003c01:01, 10.29s/it, val_loss=0.811]\u001b[A\n"," 97%|█████████▋| 218/224 [40:57\u003c01:01, 10.29s/it, val_loss=0.405]\u001b[A\n"," 98%|█████████▊| 219/224 [40:57\u003c00:53, 10.67s/it, val_loss=0.405]\u001b[A\n"," 98%|█████████▊| 219/224 [41:06\u003c00:53, 10.67s/it, val_loss=0.449]\u001b[A\n"," 98%|█████████▊| 220/224 [41:06\u003c00:40, 10.19s/it, val_loss=0.449]\u001b[A\n"," 98%|█████████▊| 220/224 [41:15\u003c00:40, 10.19s/it, val_loss=0.685]\u001b[A\n"," 99%|█████████▊| 221/224 [41:15\u003c00:29,  9.79s/it, val_loss=0.685]\u001b[A\n"," 99%|█████████▊| 221/224 [41:25\u003c00:29,  9.79s/it, val_loss=0.622]\u001b[A\n"," 99%|█████████▉| 222/224 [41:25\u003c00:20, 10.02s/it, val_loss=0.622]\u001b[A\n"," 99%|█████████▉| 222/224 [41:33\u003c00:20, 10.02s/it, val_loss=0.882]\u001b[A\n","100%|█████████▉| 223/224 [41:33\u003c00:09,  9.29s/it, val_loss=0.882]\u001b[A\n","100%|█████████▉| 223/224 [41:41\u003c00:09,  9.29s/it, val_loss=0.4]  \u001b[A\n","100%|██████████| 224/224 [41:41\u003c00:00, 11.17s/it, val_loss=0.4]\n"]},{"data":{"text/plain":["{'val_loss': 0.6475950852292104}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["save model weight\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1026/1026 [5:00:17\u003c00:00, 17.56s/it, loss=0.473, lr=4.91e-6]\n"]},{"data":{"text/plain":["{'train_loss': 0.7433371880368007}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" ==========end epoch ==========\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 224/224 [41:33\u003c00:00, 11.13s/it, val_loss=0.371]\n"]},{"data":{"text/plain":["{'val_loss': 0.6329080687605255}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:1 ============== #\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|████▊     | 498/1026 [1:49:41\u003c1:52:50, 12.82s/it, loss=0.677, lr=4.03e-6]"]},{"name":"stdout","output_type":"stream","text":["===========steps： 498 ==========\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/224 [00:00\u003c?, ?it/s]\u001b[A\n","  0%|          | 0/224 [00:10\u003c?, ?it/s, val_loss=0.52]\u001b[A\n","  0%|          | 1/224 [00:10\u003c39:01, 10.50s/it, val_loss=0.52]\u001b[A\n","  0%|          | 1/224 [00:20\u003c39:01, 10.50s/it, val_loss=0.655]\u001b[A\n","  1%|          | 2/224 [00:20\u003c38:29, 10.40s/it, val_loss=0.655]\u001b[A\n","  1%|          | 2/224 [00:34\u003c38:29, 10.40s/it, val_loss=0.587]\u001b[A\n","  1%|▏         | 3/224 [00:34\u003c43:48, 11.89s/it, val_loss=0.587]\u001b[A\n","  1%|▏         | 3/224 [00:46\u003c43:48, 11.89s/it, val_loss=0.574]\u001b[A\n","  2%|▏         | 4/224 [00:46\u003c43:16, 11.80s/it, val_loss=0.574]\u001b[A\n","  2%|▏         | 4/224 [01:00\u003c43:16, 11.80s/it, val_loss=0.447]\u001b[A\n","  2%|▏         | 5/224 [01:00\u003c46:08, 12.64s/it, val_loss=0.447]\u001b[A\n","  2%|▏         | 5/224 [01:16\u003c46:08, 12.64s/it, val_loss=0.751]\u001b[A\n","  3%|▎         | 6/224 [01:16\u003c49:45, 13.69s/it, val_loss=0.751]\u001b[A\n","  3%|▎         | 6/224 [01:26\u003c49:45, 13.69s/it, val_loss=0.865]\u001b[A\n","  3%|▎         | 7/224 [01:26\u003c45:26, 12.57s/it, val_loss=0.865]\u001b[A\n","  3%|▎         | 7/224 [01:40\u003c45:26, 12.57s/it, val_loss=0.573]\u001b[A\n","  4%|▎         | 8/224 [01:40\u003c47:13, 13.12s/it, val_loss=0.573]\u001b[A\n","  4%|▎         | 8/224 [01:47\u003c47:13, 13.12s/it, val_loss=0.304]\u001b[A\n","  4%|▍         | 9/224 [01:47\u003c39:44, 11.09s/it, val_loss=0.304]\u001b[A\n","  4%|▍         | 9/224 [02:01\u003c39:44, 11.09s/it, val_loss=0.865]\u001b[A\n","  4%|▍         | 10/224 [02:01\u003c43:33, 12.21s/it, val_loss=0.865]\u001b[A\n","  4%|▍         | 10/224 [02:12\u003c43:33, 12.21s/it, val_loss=0.703]\u001b[A\n","  5%|▍         | 11/224 [02:12\u003c41:35, 11.72s/it, val_loss=0.703]\u001b[A\n","  5%|▍         | 11/224 [02:25\u003c41:35, 11.72s/it, val_loss=0.537]\u001b[A\n","  5%|▌         | 12/224 [02:25\u003c42:44, 12.10s/it, val_loss=0.537]\u001b[A\n","  5%|▌         | 12/224 [02:35\u003c42:44, 12.10s/it, val_loss=0.516]\u001b[A\n","  6%|▌         | 13/224 [02:35\u003c40:16, 11.45s/it, val_loss=0.516]\u001b[A\n","  6%|▌         | 13/224 [02:47\u003c40:16, 11.45s/it, val_loss=0.534]\u001b[A\n","  6%|▋         | 14/224 [02:47\u003c41:12, 11.77s/it, val_loss=0.534]\u001b[A\n","  6%|▋         | 14/224 [02:56\u003c41:12, 11.77s/it, val_loss=0.565]\u001b[A\n","  7%|▋         | 15/224 [02:56\u003c37:49, 10.86s/it, val_loss=0.565]\u001b[A\n","  7%|▋         | 15/224 [03:09\u003c37:49, 10.86s/it, val_loss=0.67] \u001b[A\n","  7%|▋         | 16/224 [03:09\u003c39:59, 11.54s/it, val_loss=0.67]\u001b[A\n","  7%|▋         | 16/224 [03:25\u003c39:59, 11.54s/it, val_loss=0.626]\u001b[A\n","  8%|▊         | 17/224 [03:25\u003c44:11, 12.81s/it, val_loss=0.626]\u001b[A\n","  8%|▊         | 17/224 [03:34\u003c44:11, 12.81s/it, val_loss=0.669]\u001b[A\n","  8%|▊         | 18/224 [03:34\u003c39:57, 11.64s/it, val_loss=0.669]\u001b[A\n","  8%|▊         | 18/224 [03:47\u003c39:57, 11.64s/it, val_loss=0.888]\u001b[A\n","  8%|▊         | 19/224 [03:47\u003c41:21, 12.10s/it, val_loss=0.888]\u001b[A\n","  8%|▊         | 19/224 [04:01\u003c41:21, 12.10s/it, val_loss=0.592]\u001b[A\n","  9%|▉         | 20/224 [04:01\u003c43:15, 12.72s/it, val_loss=0.592]\u001b[A\n","  9%|▉         | 20/224 [04:13\u003c43:15, 12.72s/it, val_loss=0.519]\u001b[A\n","  9%|▉         | 21/224 [04:13\u003c42:24, 12.53s/it, val_loss=0.519]\u001b[A\n","  9%|▉         | 21/224 [04:27\u003c42:24, 12.53s/it, val_loss=0.6]  \u001b[A\n"," 10%|▉         | 22/224 [04:27\u003c42:49, 12.72s/it, val_loss=0.6]\u001b[A\n"," 10%|▉         | 22/224 [04:36\u003c42:49, 12.72s/it, val_loss=0.382]\u001b[A\n"," 10%|█         | 23/224 [04:36\u003c39:44, 11.86s/it, val_loss=0.382]\u001b[A\n"," 10%|█         | 23/224 [04:51\u003c39:44, 11.86s/it, val_loss=1.1]  \u001b[A\n"," 11%|█         | 24/224 [04:51\u003c41:43, 12.52s/it, val_loss=1.1]\u001b[A\n"," 11%|█         | 24/224 [05:07\u003c41:43, 12.52s/it, val_loss=0.916]\u001b[A\n"," 11%|█         | 25/224 [05:07\u003c45:41, 13.78s/it, val_loss=0.916]\u001b[A\n"," 11%|█         | 25/224 [05:15\u003c45:41, 13.78s/it, val_loss=0.668]\u001b[A\n"," 12%|█▏        | 26/224 [05:15\u003c39:56, 12.10s/it, val_loss=0.668]\u001b[A\n"," 12%|█▏        | 26/224 [05:26\u003c39:56, 12.10s/it, val_loss=0.658]\u001b[A\n"," 12%|█▏        | 27/224 [05:26\u003c38:07, 11.61s/it, val_loss=0.658]\u001b[A\n"," 12%|█▏        | 27/224 [05:39\u003c38:07, 11.61s/it, val_loss=0.544]\u001b[A\n"," 12%|█▎        | 28/224 [05:39\u003c39:28, 12.09s/it, val_loss=0.544]\u001b[A\n"," 12%|█▎        | 28/224 [05:52\u003c39:28, 12.09s/it, val_loss=0.94] \u001b[A\n"," 13%|█▎        | 29/224 [05:52\u003c39:59, 12.31s/it, val_loss=0.94]\u001b[A\n"," 13%|█▎        | 29/224 [06:09\u003c39:59, 12.31s/it, val_loss=0.464]\u001b[A\n"," 13%|█▎        | 30/224 [06:09\u003c44:05, 13.63s/it, val_loss=0.464]\u001b[A\n"," 13%|█▎        | 30/224 [06:22\u003c44:05, 13.63s/it, val_loss=0.752]\u001b[A\n"," 14%|█▍        | 31/224 [06:22\u003c43:09, 13.42s/it, val_loss=0.752]\u001b[A\n"," 14%|█▍        | 31/224 [06:33\u003c43:09, 13.42s/it, val_loss=0.998]\u001b[A\n"," 14%|█▍        | 32/224 [06:33\u003c40:43, 12.73s/it, val_loss=0.998]\u001b[A\n"," 14%|█▍        | 32/224 [06:44\u003c40:43, 12.73s/it, val_loss=0.427]\u001b[A\n"," 15%|█▍        | 33/224 [06:44\u003c39:14, 12.33s/it, val_loss=0.427]\u001b[A\n"," 15%|█▍        | 33/224 [06:54\u003c39:14, 12.33s/it, val_loss=0.714]\u001b[A\n"," 15%|█▌        | 34/224 [06:54\u003c37:01, 11.69s/it, val_loss=0.714]\u001b[A\n"," 15%|█▌        | 34/224 [07:07\u003c37:01, 11.69s/it, val_loss=0.624]\u001b[A\n"," 16%|█▌        | 35/224 [07:07\u003c38:00, 12.07s/it, val_loss=0.624]\u001b[A\n"," 16%|█▌        | 35/224 [07:20\u003c38:00, 12.07s/it, val_loss=0.667]\u001b[A\n"," 16%|█▌        | 36/224 [07:21\u003c38:57, 12.43s/it, val_loss=0.667]\u001b[A\n"," 16%|█▌        | 36/224 [07:29\u003c38:57, 12.43s/it, val_loss=0.594]\u001b[A\n"," 17%|█▋        | 37/224 [07:29\u003c34:47, 11.16s/it, val_loss=0.594]\u001b[A\n"," 17%|█▋        | 37/224 [07:42\u003c34:47, 11.16s/it, val_loss=0.759]\u001b[A\n"," 17%|█▋        | 38/224 [07:42\u003c36:40, 11.83s/it, val_loss=0.759]\u001b[A\n"," 17%|█▋        | 38/224 [07:57\u003c36:40, 11.83s/it, val_loss=0.414]\u001b[A\n"," 17%|█▋        | 39/224 [07:57\u003c39:46, 12.90s/it, val_loss=0.414]\u001b[A\n"," 17%|█▋        | 39/224 [08:13\u003c39:46, 12.90s/it, val_loss=0.487]\u001b[A\n"," 18%|█▊        | 40/224 [08:13\u003c42:01, 13.70s/it, val_loss=0.487]\u001b[A\n"," 18%|█▊        | 40/224 [08:26\u003c42:01, 13.70s/it, val_loss=1.03] \u001b[A\n"," 18%|█▊        | 41/224 [08:26\u003c41:10, 13.50s/it, val_loss=1.03]\u001b[A\n"," 18%|█▊        | 41/224 [08:39\u003c41:10, 13.50s/it, val_loss=0.574]\u001b[A\n"," 19%|█▉        | 42/224 [08:39\u003c40:02, 13.20s/it, val_loss=0.574]\u001b[A\n"," 19%|█▉        | 42/224 [08:51\u003c40:02, 13.20s/it, val_loss=0.512]\u001b[A\n"," 19%|█▉        | 43/224 [08:51\u003c39:17, 13.03s/it, val_loss=0.512]\u001b[A\n"," 19%|█▉        | 43/224 [09:05\u003c39:17, 13.03s/it, val_loss=0.795]\u001b[A\n"," 20%|█▉        | 44/224 [09:05\u003c39:27, 13.15s/it, val_loss=0.795]\u001b[A\n"," 20%|█▉        | 44/224 [09:19\u003c39:27, 13.15s/it, val_loss=0.599]\u001b[A\n"," 20%|██        | 45/224 [09:19\u003c40:20, 13.52s/it, val_loss=0.599]\u001b[A\n"," 20%|██        | 45/224 [09:30\u003c40:20, 13.52s/it, val_loss=0.696]\u001b[A\n"," 21%|██        | 46/224 [09:30\u003c37:38, 12.69s/it, val_loss=0.696]\u001b[A\n"," 21%|██        | 46/224 [09:44\u003c37:38, 12.69s/it, val_loss=0.531]\u001b[A\n"," 21%|██        | 47/224 [09:44\u003c38:47, 13.15s/it, val_loss=0.531]\u001b[A\n"," 21%|██        | 47/224 [10:00\u003c38:47, 13.15s/it, val_loss=0.488]\u001b[A\n"," 21%|██▏       | 48/224 [10:00\u003c40:57, 13.96s/it, val_loss=0.488]\u001b[A\n"," 21%|██▏       | 48/224 [10:12\u003c40:57, 13.96s/it, val_loss=0.812]\u001b[A\n"," 22%|██▏       | 49/224 [10:12\u003c39:14, 13.45s/it, val_loss=0.812]\u001b[A\n"," 22%|██▏       | 49/224 [10:23\u003c39:14, 13.45s/it, val_loss=0.622]\u001b[A\n"," 22%|██▏       | 50/224 [10:23\u003c36:51, 12.71s/it, val_loss=0.622]\u001b[A\n"," 22%|██▏       | 50/224 [10:35\u003c36:51, 12.71s/it, val_loss=0.605]\u001b[A\n"," 23%|██▎       | 51/224 [10:35\u003c36:04, 12.51s/it, val_loss=0.605]\u001b[A\n"," 23%|██▎       | 51/224 [10:50\u003c36:04, 12.51s/it, val_loss=0.725]\u001b[A\n"," 23%|██▎       | 52/224 [10:50\u003c38:00, 13.26s/it, val_loss=0.725]\u001b[A\n"," 23%|██▎       | 52/224 [11:01\u003c38:00, 13.26s/it, val_loss=0.96] \u001b[A\n"," 24%|██▎       | 53/224 [11:01\u003c35:58, 12.62s/it, val_loss=0.96]\u001b[A\n"," 24%|██▎       | 53/224 [11:14\u003c35:58, 12.62s/it, val_loss=0.341]\u001b[A\n"," 24%|██▍       | 54/224 [11:14\u003c35:43, 12.61s/it, val_loss=0.341]\u001b[A\n"," 24%|██▍       | 54/224 [11:26\u003c35:43, 12.61s/it, val_loss=0.614]\u001b[A\n"," 25%|██▍       | 55/224 [11:26\u003c34:55, 12.40s/it, val_loss=0.614]\u001b[A\n"," 25%|██▍       | 55/224 [11:39\u003c34:55, 12.40s/it, val_loss=0.909]\u001b[A\n"," 25%|██▌       | 56/224 [11:39\u003c35:02, 12.51s/it, val_loss=0.909]\u001b[A\n"," 25%|██▌       | 56/224 [11:50\u003c35:02, 12.51s/it, val_loss=0.602]\u001b[A\n"," 25%|██▌       | 57/224 [11:50\u003c34:16, 12.31s/it, val_loss=0.602]\u001b[A\n"," 25%|██▌       | 57/224 [12:01\u003c34:16, 12.31s/it, val_loss=0.464]\u001b[A\n"," 26%|██▌       | 58/224 [12:01\u003c32:48, 11.86s/it, val_loss=0.464]\u001b[A\n"," 26%|██▌       | 58/224 [12:14\u003c32:48, 11.86s/it, val_loss=0.524]\u001b[A\n"," 26%|██▋       | 59/224 [12:14\u003c33:10, 12.06s/it, val_loss=0.524]\u001b[A\n"," 26%|██▋       | 59/224 [12:27\u003c33:10, 12.06s/it, val_loss=0.549]\u001b[A\n"," 27%|██▋       | 60/224 [12:27\u003c33:59, 12.44s/it, val_loss=0.549]\u001b[A\n"," 27%|██▋       | 60/224 [12:40\u003c33:59, 12.44s/it, val_loss=0.607]\u001b[A\n"," 27%|██▋       | 61/224 [12:40\u003c34:06, 12.55s/it, val_loss=0.607]\u001b[A\n"," 27%|██▋       | 61/224 [12:53\u003c34:06, 12.55s/it, val_loss=0.59] \u001b[A\n"," 28%|██▊       | 62/224 [12:53\u003c34:04, 12.62s/it, val_loss=0.59]\u001b[A\n"," 28%|██▊       | 62/224 [13:05\u003c34:04, 12.62s/it, val_loss=0.486]\u001b[A\n"," 28%|██▊       | 63/224 [13:05\u003c33:36, 12.52s/it, val_loss=0.486]\u001b[A\n"," 28%|██▊       | 63/224 [13:15\u003c33:36, 12.52s/it, val_loss=0.482]\u001b[A\n"," 29%|██▊       | 64/224 [13:15\u003c31:15, 11.72s/it, val_loss=0.482]\u001b[A\n"," 29%|██▊       | 64/224 [13:32\u003c31:15, 11.72s/it, val_loss=0.534]\u001b[A\n"," 29%|██▉       | 65/224 [13:32\u003c35:10, 13.28s/it, val_loss=0.534]\u001b[A\n"," 29%|██▉       | 65/224 [13:43\u003c35:10, 13.28s/it, val_loss=1.01] \u001b[A\n"," 29%|██▉       | 66/224 [13:43\u003c33:19, 12.66s/it, val_loss=1.01]\u001b[A\n"," 29%|██▉       | 66/224 [13:56\u003c33:19, 12.66s/it, val_loss=0.637]\u001b[A\n"," 30%|██▉       | 67/224 [13:56\u003c33:04, 12.64s/it, val_loss=0.637]\u001b[A\n"," 30%|██▉       | 67/224 [14:08\u003c33:04, 12.64s/it, val_loss=0.509]\u001b[A\n"," 30%|███       | 68/224 [14:08\u003c32:26, 12.47s/it, val_loss=0.509]\u001b[A\n"," 30%|███       | 68/224 [14:16\u003c32:26, 12.47s/it, val_loss=0.347]\u001b[A\n"," 31%|███       | 69/224 [14:16\u003c29:20, 11.36s/it, val_loss=0.347]\u001b[A\n"," 31%|███       | 69/224 [14:27\u003c29:20, 11.36s/it, val_loss=0.439]\u001b[A\n"," 31%|███▏      | 70/224 [14:27\u003c28:54, 11.26s/it, val_loss=0.439]\u001b[A\n"," 31%|███▏      | 70/224 [14:37\u003c28:54, 11.26s/it, val_loss=0.759]\u001b[A\n"," 32%|███▏      | 71/224 [14:38\u003c27:49, 10.91s/it, val_loss=0.759]\u001b[A\n"," 32%|███▏      | 71/224 [14:49\u003c27:49, 10.91s/it, val_loss=0.616]\u001b[A\n"," 32%|███▏      | 72/224 [14:49\u003c28:22, 11.20s/it, val_loss=0.616]\u001b[A\n"," 32%|███▏      | 72/224 [15:00\u003c28:22, 11.20s/it, val_loss=0.678]\u001b[A\n"," 33%|███▎      | 73/224 [15:00\u003c27:27, 10.91s/it, val_loss=0.678]\u001b[A\n"," 33%|███▎      | 73/224 [15:10\u003c27:27, 10.91s/it, val_loss=0.556]\u001b[A\n"," 33%|███▎      | 74/224 [15:10\u003c27:05, 10.83s/it, val_loss=0.556]\u001b[A\n"," 33%|███▎      | 74/224 [15:20\u003c27:05, 10.83s/it, val_loss=0.525]\u001b[A\n"," 33%|███▎      | 75/224 [15:20\u003c25:59, 10.46s/it, val_loss=0.525]\u001b[A\n"," 33%|███▎      | 75/224 [15:35\u003c25:59, 10.46s/it, val_loss=0.631]\u001b[A\n"," 34%|███▍      | 76/224 [15:35\u003c28:53, 11.72s/it, val_loss=0.631]\u001b[A\n"," 34%|███▍      | 76/224 [15:44\u003c28:53, 11.72s/it, val_loss=0.58] \u001b[A\n"," 34%|███▍      | 77/224 [15:44\u003c27:17, 11.14s/it, val_loss=0.58]\u001b[A\n"," 34%|███▍      | 77/224 [15:56\u003c27:17, 11.14s/it, val_loss=0.407]\u001b[A\n"," 35%|███▍      | 78/224 [15:56\u003c27:20, 11.24s/it, val_loss=0.407]\u001b[A\n"," 35%|███▍      | 78/224 [16:05\u003c27:20, 11.24s/it, val_loss=0.719]\u001b[A\n"," 35%|███▌      | 79/224 [16:05\u003c26:00, 10.76s/it, val_loss=0.719]\u001b[A\n"," 35%|███▌      | 79/224 [16:19\u003c26:00, 10.76s/it, val_loss=0.389]\u001b[A\n"," 36%|███▌      | 80/224 [16:19\u003c28:07, 11.72s/it, val_loss=0.389]\u001b[A\n"," 36%|███▌      | 80/224 [16:30\u003c28:07, 11.72s/it, val_loss=0.697]\u001b[A\n"," 36%|███▌      | 81/224 [16:30\u003c26:49, 11.26s/it, val_loss=0.697]\u001b[A\n"," 36%|███▌      | 81/224 [16:42\u003c26:49, 11.26s/it, val_loss=0.874]\u001b[A\n"," 37%|███▋      | 82/224 [16:42\u003c27:48, 11.75s/it, val_loss=0.874]\u001b[A\n"," 37%|███▋      | 82/224 [16:57\u003c27:48, 11.75s/it, val_loss=0.616]\u001b[A\n"," 37%|███▋      | 83/224 [16:57\u003c29:29, 12.55s/it, val_loss=0.616]\u001b[A\n"," 37%|███▋      | 83/224 [17:10\u003c29:29, 12.55s/it, val_loss=0.357]\u001b[A\n"," 38%|███▊      | 84/224 [17:10\u003c29:43, 12.74s/it, val_loss=0.357]\u001b[A\n"," 38%|███▊      | 84/224 [17:25\u003c29:43, 12.74s/it, val_loss=0.442]\u001b[A\n"," 38%|███▊      | 85/224 [17:25\u003c31:22, 13.54s/it, val_loss=0.442]\u001b[A\n"," 38%|███▊      | 85/224 [17:37\u003c31:22, 13.54s/it, val_loss=0.77] \u001b[A\n"," 38%|███▊      | 86/224 [17:37\u003c29:40, 12.90s/it, val_loss=0.77]\u001b[A\n"," 38%|███▊      | 86/224 [17:49\u003c29:40, 12.90s/it, val_loss=0.734]\u001b[A\n"," 39%|███▉      | 87/224 [17:49\u003c29:04, 12.73s/it, val_loss=0.734]\u001b[A\n"," 39%|███▉      | 87/224 [18:03\u003c29:04, 12.73s/it, val_loss=0.421]\u001b[A\n"," 39%|███▉      | 88/224 [18:03\u003c29:23, 12.97s/it, val_loss=0.421]\u001b[A\n"," 39%|███▉      | 88/224 [18:15\u003c29:23, 12.97s/it, val_loss=0.421]\u001b[A\n"," 40%|███▉      | 89/224 [18:15\u003c28:45, 12.78s/it, val_loss=0.421]\u001b[A\n"," 40%|███▉      | 89/224 [18:30\u003c28:45, 12.78s/it, val_loss=0.547]\u001b[A\n"," 40%|████      | 90/224 [18:30\u003c29:59, 13.43s/it, val_loss=0.547]\u001b[A\n"," 40%|████      | 90/224 [18:42\u003c29:59, 13.43s/it, val_loss=0.593]\u001b[A\n"," 41%|████      | 91/224 [18:42\u003c28:36, 12.91s/it, val_loss=0.593]\u001b[A\n"," 41%|████      | 91/224 [18:54\u003c28:36, 12.91s/it, val_loss=0.606]\u001b[A\n"," 41%|████      | 92/224 [18:54\u003c27:46, 12.62s/it, val_loss=0.606]\u001b[A\n"," 41%|████      | 92/224 [19:07\u003c27:46, 12.62s/it, val_loss=0.481]\u001b[A\n"," 42%|████▏     | 93/224 [19:07\u003c28:03, 12.85s/it, val_loss=0.481]\u001b[A\n"," 42%|████▏     | 93/224 [19:17\u003c28:03, 12.85s/it, val_loss=0.636]\u001b[A\n"," 42%|████▏     | 94/224 [19:17\u003c26:02, 12.02s/it, val_loss=0.636]\u001b[A\n"," 42%|████▏     | 94/224 [19:30\u003c26:02, 12.02s/it, val_loss=0.604]\u001b[A\n"," 42%|████▏     | 95/224 [19:30\u003c26:25, 12.29s/it, val_loss=0.604]\u001b[A\n"," 42%|████▏     | 95/224 [19:46\u003c26:25, 12.29s/it, val_loss=0.772]\u001b[A\n"," 43%|████▎     | 96/224 [19:46\u003c28:26, 13.33s/it, val_loss=0.772]\u001b[A\n"," 43%|████▎     | 96/224 [19:57\u003c28:26, 13.33s/it, val_loss=0.39] \u001b[A\n"," 43%|████▎     | 97/224 [19:57\u003c27:05, 12.80s/it, val_loss=0.39]\u001b[A\n"," 43%|████▎     | 97/224 [20:09\u003c27:05, 12.80s/it, val_loss=0.552]\u001b[A\n"," 44%|████▍     | 98/224 [20:09\u003c26:12, 12.48s/it, val_loss=0.552]\u001b[A\n"," 44%|████▍     | 98/224 [20:18\u003c26:12, 12.48s/it, val_loss=0.604]\u001b[A\n"," 44%|████▍     | 99/224 [20:18\u003c23:42, 11.38s/it, val_loss=0.604]\u001b[A\n"," 44%|████▍     | 99/224 [20:30\u003c23:42, 11.38s/it, val_loss=0.454]\u001b[A\n"," 45%|████▍     | 100/224 [20:30\u003c24:01, 11.63s/it, val_loss=0.454]\u001b[A\n"," 45%|████▍     | 100/224 [20:41\u003c24:01, 11.63s/it, val_loss=0.592]\u001b[A\n"," 45%|████▌     | 101/224 [20:41\u003c23:09, 11.30s/it, val_loss=0.592]\u001b[A\n"," 45%|████▌     | 101/224 [20:54\u003c23:09, 11.30s/it, val_loss=0.7]  \u001b[A\n"," 46%|████▌     | 102/224 [20:54\u003c24:16, 11.94s/it, val_loss=0.7]\u001b[A\n"," 46%|████▌     | 102/224 [21:04\u003c24:16, 11.94s/it, val_loss=0.565]\u001b[A\n"," 46%|████▌     | 103/224 [21:04\u003c22:52, 11.34s/it, val_loss=0.565]\u001b[A\n"," 46%|████▌     | 103/224 [21:13\u003c22:52, 11.34s/it, val_loss=0.689]\u001b[A\n"," 46%|████▋     | 104/224 [21:13\u003c21:12, 10.61s/it, val_loss=0.689]\u001b[A\n"," 46%|████▋     | 104/224 [21:24\u003c21:12, 10.61s/it, val_loss=0.659]\u001b[A\n"," 47%|████▋     | 105/224 [21:24\u003c21:21, 10.77s/it, val_loss=0.659]\u001b[A\n"," 47%|████▋     | 105/224 [21:38\u003c21:21, 10.77s/it, val_loss=0.529]\u001b[A\n"," 47%|████▋     | 106/224 [21:38\u003c23:08, 11.77s/it, val_loss=0.529]\u001b[A\n"," 47%|████▋     | 106/224 [21:49\u003c23:08, 11.77s/it, val_loss=0.722]\u001b[A\n"," 48%|████▊     | 107/224 [21:49\u003c22:12, 11.39s/it, val_loss=0.722]\u001b[A\n"," 48%|████▊     | 107/224 [22:00\u003c22:12, 11.39s/it, val_loss=0.598]\u001b[A\n"," 48%|████▊     | 108/224 [22:00\u003c21:50, 11.30s/it, val_loss=0.598]\u001b[A\n"," 48%|████▊     | 108/224 [22:17\u003c21:50, 11.30s/it, val_loss=0.789]\u001b[A\n"," 49%|████▊     | 109/224 [22:17\u003c24:52, 12.98s/it, val_loss=0.789]\u001b[A\n"," 49%|████▊     | 109/224 [22:28\u003c24:52, 12.98s/it, val_loss=0.68] \u001b[A\n"," 49%|████▉     | 110/224 [22:28\u003c23:53, 12.57s/it, val_loss=0.68]\u001b[A\n"," 49%|████▉     | 110/224 [22:40\u003c23:53, 12.57s/it, val_loss=0.631]\u001b[A\n"," 50%|████▉     | 111/224 [22:40\u003c23:10, 12.31s/it, val_loss=0.631]\u001b[A\n"," 50%|████▉     | 111/224 [22:54\u003c23:10, 12.31s/it, val_loss=0.521]\u001b[A\n"," 50%|█████     | 112/224 [22:54\u003c23:43, 12.71s/it, val_loss=0.521]\u001b[A\n"," 50%|█████     | 112/224 [23:07\u003c23:43, 12.71s/it, val_loss=0.669]\u001b[A\n"," 50%|█████     | 113/224 [23:07\u003c23:50, 12.89s/it, val_loss=0.669]\u001b[A\n"," 50%|█████     | 113/224 [23:22\u003c23:50, 12.89s/it, val_loss=0.795]\u001b[A\n"," 51%|█████     | 114/224 [23:22\u003c25:02, 13.66s/it, val_loss=0.795]\u001b[A\n"," 51%|█████     | 114/224 [23:35\u003c25:02, 13.66s/it, val_loss=0.628]\u001b[A\n"," 51%|█████▏    | 115/224 [23:35\u003c24:20, 13.40s/it, val_loss=0.628]\u001b[A\n"," 51%|█████▏    | 115/224 [23:51\u003c24:20, 13.40s/it, val_loss=0.69] \u001b[A\n"," 52%|█████▏    | 116/224 [23:51\u003c25:34, 14.21s/it, val_loss=0.69]\u001b[A\n"," 52%|█████▏    | 116/224 [24:02\u003c25:34, 14.21s/it, val_loss=0.552]\u001b[A\n"," 52%|█████▏    | 117/224 [24:02\u003c23:27, 13.16s/it, val_loss=0.552]\u001b[A\n"," 52%|█████▏    | 117/224 [24:15\u003c23:27, 13.16s/it, val_loss=0.528]\u001b[A\n"," 53%|█████▎    | 118/224 [24:15\u003c23:06, 13.08s/it, val_loss=0.528]\u001b[A\n"," 53%|█████▎    | 118/224 [24:28\u003c23:06, 13.08s/it, val_loss=0.599]\u001b[A\n"," 53%|█████▎    | 119/224 [24:28\u003c22:39, 12.95s/it, val_loss=0.599]\u001b[A\n"," 53%|█████▎    | 119/224 [24:39\u003c22:39, 12.95s/it, val_loss=0.893]\u001b[A\n"," 54%|█████▎    | 120/224 [24:39\u003c21:51, 12.61s/it, val_loss=0.893]\u001b[A\n"," 54%|█████▎    | 120/224 [24:50\u003c21:51, 12.61s/it, val_loss=0.485]\u001b[A\n"," 54%|█████▍    | 121/224 [24:50\u003c20:37, 12.01s/it, val_loss=0.485]\u001b[A\n"," 54%|█████▍    | 121/224 [25:01\u003c20:37, 12.01s/it, val_loss=0.842]\u001b[A\n"," 54%|█████▍    | 122/224 [25:01\u003c19:40, 11.58s/it, val_loss=0.842]\u001b[A\n"," 54%|█████▍    | 122/224 [25:13\u003c19:40, 11.58s/it, val_loss=0.539]\u001b[A\n"," 55%|█████▍    | 123/224 [25:13\u003c19:50, 11.79s/it, val_loss=0.539]\u001b[A\n"," 55%|█████▍    | 123/224 [25:27\u003c19:50, 11.79s/it, val_loss=0.467]\u001b[A\n"," 55%|█████▌    | 124/224 [25:27\u003c20:44, 12.44s/it, val_loss=0.467]\u001b[A\n"," 55%|█████▌    | 124/224 [25:37\u003c20:44, 12.44s/it, val_loss=0.668]\u001b[A\n"," 56%|█████▌    | 125/224 [25:37\u003c19:27, 11.79s/it, val_loss=0.668]\u001b[A\n"," 56%|█████▌    | 125/224 [25:50\u003c19:27, 11.79s/it, val_loss=0.407]\u001b[A\n"," 56%|█████▋    | 126/224 [25:50\u003c20:02, 12.27s/it, val_loss=0.407]\u001b[A\n"," 56%|█████▋    | 126/224 [26:00\u003c20:02, 12.27s/it, val_loss=0.868]\u001b[A\n"," 57%|█████▋    | 127/224 [26:00\u003c18:19, 11.33s/it, val_loss=0.868]\u001b[A\n"," 57%|█████▋    | 127/224 [26:09\u003c18:19, 11.33s/it, val_loss=0.655]\u001b[A\n"," 57%|█████▋    | 128/224 [26:09\u003c17:16, 10.80s/it, val_loss=0.655]\u001b[A\n"," 57%|█████▋    | 128/224 [26:23\u003c17:16, 10.80s/it, val_loss=0.391]\u001b[A\n"," 58%|█████▊    | 129/224 [26:23\u003c18:41, 11.81s/it, val_loss=0.391]\u001b[A\n"," 58%|█████▊    | 129/224 [26:35\u003c18:41, 11.81s/it, val_loss=0.803]\u001b[A\n"," 58%|█████▊    | 130/224 [26:35\u003c18:28, 11.79s/it, val_loss=0.803]\u001b[A\n"," 58%|█████▊    | 130/224 [26:48\u003c18:28, 11.79s/it, val_loss=0.801]\u001b[A\n"," 58%|█████▊    | 131/224 [26:48\u003c18:35, 12.00s/it, val_loss=0.801]\u001b[A\n"," 58%|█████▊    | 131/224 [27:02\u003c18:35, 12.00s/it, val_loss=0.75] \u001b[A\n"," 59%|█████▉    | 132/224 [27:02\u003c19:39, 12.82s/it, val_loss=0.75]\u001b[A\n"," 59%|█████▉    | 132/224 [27:15\u003c19:39, 12.82s/it, val_loss=0.713]\u001b[A\n"," 59%|█████▉    | 133/224 [27:15\u003c19:34, 12.91s/it, val_loss=0.713]\u001b[A\n"," 59%|█████▉    | 133/224 [27:30\u003c19:34, 12.91s/it, val_loss=0.7]  \u001b[A\n"," 60%|█████▉    | 134/224 [27:30\u003c20:15, 13.50s/it, val_loss=0.7]\u001b[A\n"," 60%|█████▉    | 134/224 [27:41\u003c20:15, 13.50s/it, val_loss=0.888]\u001b[A\n"," 60%|██████    | 135/224 [27:41\u003c18:52, 12.72s/it, val_loss=0.888]\u001b[A\n"," 60%|██████    | 135/224 [27:54\u003c18:52, 12.72s/it, val_loss=0.593]\u001b[A\n"," 61%|██████    | 136/224 [27:54\u003c18:38, 12.71s/it, val_loss=0.593]\u001b[A\n"," 61%|██████    | 136/224 [28:04\u003c18:38, 12.71s/it, val_loss=0.674]\u001b[A\n"," 61%|██████    | 137/224 [28:04\u003c17:31, 12.09s/it, val_loss=0.674]\u001b[A\n"," 61%|██████    | 137/224 [28:15\u003c17:31, 12.09s/it, val_loss=0.501]\u001b[A\n"," 62%|██████▏   | 138/224 [28:15\u003c16:27, 11.48s/it, val_loss=0.501]\u001b[A\n"," 62%|██████▏   | 138/224 [28:32\u003c16:27, 11.48s/it, val_loss=0.712]\u001b[A\n"," 62%|██████▏   | 139/224 [28:32\u003c18:44, 13.23s/it, val_loss=0.712]\u001b[A\n"," 62%|██████▏   | 139/224 [28:43\u003c18:44, 13.23s/it, val_loss=0.341]\u001b[A\n"," 62%|██████▎   | 140/224 [28:43\u003c17:49, 12.73s/it, val_loss=0.341]\u001b[A\n"," 62%|██████▎   | 140/224 [28:58\u003c17:49, 12.73s/it, val_loss=0.661]\u001b[A\n"," 63%|██████▎   | 141/224 [28:58\u003c18:25, 13.32s/it, val_loss=0.661]\u001b[A\n"," 63%|██████▎   | 141/224 [29:11\u003c18:25, 13.32s/it, val_loss=0.588]\u001b[A\n"," 63%|██████▎   | 142/224 [29:11\u003c18:07, 13.27s/it, val_loss=0.588]\u001b[A\n"," 63%|██████▎   | 142/224 [29:26\u003c18:07, 13.27s/it, val_loss=0.435]\u001b[A\n"," 64%|██████▍   | 143/224 [29:26\u003c18:20, 13.59s/it, val_loss=0.435]\u001b[A\n"," 64%|██████▍   | 143/224 [29:37\u003c18:20, 13.59s/it, val_loss=0.35] \u001b[A\n"," 64%|██████▍   | 144/224 [29:37\u003c17:06, 12.83s/it, val_loss=0.35]\u001b[A\n"," 64%|██████▍   | 144/224 [29:48\u003c17:06, 12.83s/it, val_loss=0.664]\u001b[A\n"," 65%|██████▍   | 145/224 [29:48\u003c16:11, 12.30s/it, val_loss=0.664]\u001b[A\n"," 65%|██████▍   | 145/224 [29:56\u003c16:11, 12.30s/it, val_loss=0.502]\u001b[A\n"," 65%|██████▌   | 146/224 [29:56\u003c14:36, 11.23s/it, val_loss=0.502]\u001b[A\n"," 65%|██████▌   | 146/224 [30:06\u003c14:36, 11.23s/it, val_loss=0.527]\u001b[A\n"," 66%|██████▌   | 147/224 [30:06\u003c13:52, 10.81s/it, val_loss=0.527]\u001b[A\n"," 66%|██████▌   | 147/224 [30:17\u003c13:52, 10.81s/it, val_loss=0.623]\u001b[A\n"," 66%|██████▌   | 148/224 [30:17\u003c13:28, 10.64s/it, val_loss=0.623]\u001b[A\n"," 66%|██████▌   | 148/224 [30:27\u003c13:28, 10.64s/it, val_loss=0.571]\u001b[A\n"," 67%|██████▋   | 149/224 [30:27\u003c13:09, 10.53s/it, val_loss=0.571]\u001b[A\n"," 67%|██████▋   | 149/224 [30:39\u003c13:09, 10.53s/it, val_loss=0.555]\u001b[A\n"," 67%|██████▋   | 150/224 [30:39\u003c13:43, 11.13s/it, val_loss=0.555]\u001b[A\n"," 67%|██████▋   | 150/224 [30:53\u003c13:43, 11.13s/it, val_loss=0.63] \u001b[A\n"," 67%|██████▋   | 151/224 [30:53\u003c14:38, 12.03s/it, val_loss=0.63]\u001b[A\n"," 67%|██████▋   | 151/224 [31:03\u003c14:38, 12.03s/it, val_loss=0.837]\u001b[A\n"," 68%|██████▊   | 152/224 [31:03\u003c13:34, 11.31s/it, val_loss=0.837]\u001b[A\n"," 68%|██████▊   | 152/224 [31:16\u003c13:34, 11.31s/it, val_loss=0.695]\u001b[A\n"," 68%|██████▊   | 153/224 [31:16\u003c14:00, 11.83s/it, val_loss=0.695]\u001b[A\n"," 68%|██████▊   | 153/224 [31:29\u003c14:00, 11.83s/it, val_loss=0.69] \u001b[A\n"," 69%|██████▉   | 154/224 [31:29\u003c14:02, 12.04s/it, val_loss=0.69]\u001b[A\n"," 69%|██████▉   | 154/224 [31:43\u003c14:02, 12.04s/it, val_loss=0.662]\u001b[A\n"," 69%|██████▉   | 155/224 [31:43\u003c14:32, 12.65s/it, val_loss=0.662]\u001b[A\n"," 69%|██████▉   | 155/224 [31:53\u003c14:32, 12.65s/it, val_loss=0.465]\u001b[A\n"," 70%|██████▉   | 156/224 [31:53\u003c13:29, 11.91s/it, val_loss=0.465]\u001b[A\n"," 70%|██████▉   | 156/224 [32:06\u003c13:29, 11.91s/it, val_loss=0.65] \u001b[A\n"," 70%|███████   | 157/224 [32:06\u003c13:50, 12.40s/it, val_loss=0.65]\u001b[A\n"," 70%|███████   | 157/224 [32:19\u003c13:50, 12.40s/it, val_loss=0.802]\u001b[A\n"," 71%|███████   | 158/224 [32:19\u003c13:33, 12.33s/it, val_loss=0.802]\u001b[A\n"," 71%|███████   | 158/224 [32:30\u003c13:33, 12.33s/it, val_loss=0.42] \u001b[A\n"," 71%|███████   | 159/224 [32:30\u003c13:03, 12.05s/it, val_loss=0.42]\u001b[A\n"," 71%|███████   | 159/224 [32:43\u003c13:03, 12.05s/it, val_loss=0.666]\u001b[A\n"," 71%|███████▏  | 160/224 [32:43\u003c13:16, 12.45s/it, val_loss=0.666]\u001b[A\n"," 71%|███████▏  | 160/224 [32:54\u003c13:16, 12.45s/it, val_loss=0.639]\u001b[A\n"," 72%|███████▏  | 161/224 [32:54\u003c12:28, 11.88s/it, val_loss=0.639]\u001b[A\n"," 72%|███████▏  | 161/224 [33:08\u003c12:28, 11.88s/it, val_loss=0.634]\u001b[A\n"," 72%|███████▏  | 162/224 [33:08\u003c12:53, 12.48s/it, val_loss=0.634]\u001b[A\n"," 72%|███████▏  | 162/224 [33:21\u003c12:53, 12.48s/it, val_loss=0.584]\u001b[A\n"," 73%|███████▎  | 163/224 [33:21\u003c12:47, 12.58s/it, val_loss=0.584]\u001b[A\n"," 73%|███████▎  | 163/224 [33:31\u003c12:47, 12.58s/it, val_loss=0.504]\u001b[A\n"," 73%|███████▎  | 164/224 [33:31\u003c11:47, 11.79s/it, val_loss=0.504]\u001b[A\n"," 73%|███████▎  | 164/224 [33:42\u003c11:47, 11.79s/it, val_loss=0.909]\u001b[A\n"," 74%|███████▎  | 165/224 [33:42\u003c11:22, 11.57s/it, val_loss=0.909]\u001b[A\n"," 74%|███████▎  | 165/224 [33:56\u003c11:22, 11.57s/it, val_loss=0.86] \u001b[A\n"," 74%|███████▍  | 166/224 [33:56\u003c11:59, 12.40s/it, val_loss=0.86]\u001b[A\n"," 74%|███████▍  | 166/224 [34:13\u003c11:59, 12.40s/it, val_loss=0.365]\u001b[A\n"," 75%|███████▍  | 167/224 [34:13\u003c12:57, 13.65s/it, val_loss=0.365]\u001b[A\n"," 75%|███████▍  | 167/224 [34:24\u003c12:57, 13.65s/it, val_loss=0.635]\u001b[A\n"," 75%|███████▌  | 168/224 [34:24\u003c12:06, 12.97s/it, val_loss=0.635]\u001b[A\n"," 75%|███████▌  | 168/224 [34:37\u003c12:06, 12.97s/it, val_loss=0.499]\u001b[A\n"," 75%|███████▌  | 169/224 [34:37\u003c11:57, 13.05s/it, val_loss=0.499]\u001b[A\n"," 75%|███████▌  | 169/224 [34:47\u003c11:57, 13.05s/it, val_loss=0.626]\u001b[A\n"," 76%|███████▌  | 170/224 [34:47\u003c10:48, 12.02s/it, val_loss=0.626]\u001b[A\n"," 76%|███████▌  | 170/224 [35:00\u003c10:48, 12.02s/it, val_loss=0.704]\u001b[A\n"," 76%|███████▋  | 171/224 [35:00\u003c10:52, 12.32s/it, val_loss=0.704]\u001b[A\n"," 76%|███████▋  | 171/224 [35:15\u003c10:52, 12.32s/it, val_loss=0.535]\u001b[A\n"," 77%|███████▋  | 172/224 [35:15\u003c11:19, 13.07s/it, val_loss=0.535]\u001b[A\n"," 77%|███████▋  | 172/224 [35:26\u003c11:19, 13.07s/it, val_loss=0.824]\u001b[A\n"," 77%|███████▋  | 173/224 [35:26\u003c10:41, 12.58s/it, val_loss=0.824]\u001b[A\n"," 77%|███████▋  | 173/224 [35:36\u003c10:41, 12.58s/it, val_loss=0.65] \u001b[A\n"," 78%|███████▊  | 174/224 [35:36\u003c09:52, 11.86s/it, val_loss=0.65]\u001b[A\n"," 78%|███████▊  | 174/224 [35:49\u003c09:52, 11.86s/it, val_loss=0.627]\u001b[A\n"," 78%|███████▊  | 175/224 [35:49\u003c09:56, 12.16s/it, val_loss=0.627]\u001b[A\n"," 78%|███████▊  | 175/224 [35:58\u003c09:56, 12.16s/it, val_loss=0.544]\u001b[A\n"," 79%|███████▊  | 176/224 [35:58\u003c08:52, 11.10s/it, val_loss=0.544]\u001b[A\n"," 79%|███████▊  | 176/224 [36:06\u003c08:52, 11.10s/it, val_loss=0.523]\u001b[A\n"," 79%|███████▉  | 177/224 [36:06\u003c08:07, 10.37s/it, val_loss=0.523]\u001b[A\n"," 79%|███████▉  | 177/224 [36:18\u003c08:07, 10.37s/it, val_loss=0.539]\u001b[A\n"," 79%|███████▉  | 178/224 [36:18\u003c08:09, 10.65s/it, val_loss=0.539]\u001b[A\n"," 79%|███████▉  | 178/224 [36:36\u003c08:09, 10.65s/it, val_loss=0.596]\u001b[A\n"," 80%|███████▉  | 179/224 [36:36\u003c09:40, 12.90s/it, val_loss=0.596]\u001b[A\n"," 80%|███████▉  | 179/224 [36:49\u003c09:40, 12.90s/it, val_loss=0.743]\u001b[A\n"," 80%|████████  | 180/224 [36:49\u003c09:33, 13.03s/it, val_loss=0.743]\u001b[A\n"," 80%|████████  | 180/224 [37:04\u003c09:33, 13.03s/it, val_loss=0.526]\u001b[A\n"," 81%|████████  | 181/224 [37:04\u003c09:44, 13.59s/it, val_loss=0.526]\u001b[A\n"," 81%|████████  | 181/224 [37:15\u003c09:44, 13.59s/it, val_loss=0.354]\u001b[A\n"," 81%|████████▏ | 182/224 [37:15\u003c08:51, 12.65s/it, val_loss=0.354]\u001b[A\n"," 81%|████████▏ | 182/224 [37:28\u003c08:51, 12.65s/it, val_loss=0.787]\u001b[A\n"," 82%|████████▏ | 183/224 [37:28\u003c08:46, 12.84s/it, val_loss=0.787]\u001b[A\n"," 82%|████████▏ | 183/224 [37:40\u003c08:46, 12.84s/it, val_loss=0.426]\u001b[A\n"," 82%|████████▏ | 184/224 [37:40\u003c08:27, 12.68s/it, val_loss=0.426]\u001b[A\n"," 82%|████████▏ | 184/224 [37:55\u003c08:27, 12.68s/it, val_loss=1.13] \u001b[A\n"," 83%|████████▎ | 185/224 [37:55\u003c08:41, 13.36s/it, val_loss=1.13]\u001b[A\n"," 83%|████████▎ | 185/224 [38:07\u003c08:41, 13.36s/it, val_loss=0.707]\u001b[A\n"," 83%|████████▎ | 186/224 [38:07\u003c08:10, 12.91s/it, val_loss=0.707]\u001b[A\n"," 83%|████████▎ | 186/224 [38:18\u003c08:10, 12.91s/it, val_loss=0.636]\u001b[A\n"," 83%|████████▎ | 187/224 [38:18\u003c07:35, 12.32s/it, val_loss=0.636]\u001b[A\n"," 83%|████████▎ | 187/224 [38:29\u003c07:35, 12.32s/it, val_loss=0.723]\u001b[A\n"," 84%|████████▍ | 188/224 [38:29\u003c07:15, 12.09s/it, val_loss=0.723]\u001b[A\n"," 84%|████████▍ | 188/224 [38:44\u003c07:15, 12.09s/it, val_loss=0.684]\u001b[A\n"," 84%|████████▍ | 189/224 [38:44\u003c07:30, 12.87s/it, val_loss=0.684]\u001b[A\n"," 84%|████████▍ | 189/224 [38:53\u003c07:30, 12.87s/it, val_loss=0.724]\u001b[A\n"," 85%|████████▍ | 190/224 [38:53\u003c06:38, 11.73s/it, val_loss=0.724]\u001b[A\n"," 85%|████████▍ | 190/224 [39:08\u003c06:38, 11.73s/it, val_loss=0.782]\u001b[A\n"," 85%|████████▌ | 191/224 [39:08\u003c06:55, 12.58s/it, val_loss=0.782]\u001b[A\n"," 85%|████████▌ | 191/224 [39:20\u003c06:55, 12.58s/it, val_loss=0.622]\u001b[A\n"," 86%|████████▌ | 192/224 [39:20\u003c06:34, 12.34s/it, val_loss=0.622]\u001b[A\n"," 86%|████████▌ | 192/224 [39:32\u003c06:34, 12.34s/it, val_loss=0.853]\u001b[A\n"," 86%|████████▌ | 193/224 [39:32\u003c06:22, 12.34s/it, val_loss=0.853]\u001b[A\n"," 86%|████████▌ | 193/224 [39:43\u003c06:22, 12.34s/it, val_loss=0.544]\u001b[A\n"," 87%|████████▋ | 194/224 [39:43\u003c06:02, 12.07s/it, val_loss=0.544]\u001b[A\n"," 87%|████████▋ | 194/224 [39:57\u003c06:02, 12.07s/it, val_loss=0.541]\u001b[A\n"," 87%|████████▋ | 195/224 [39:57\u003c06:08, 12.70s/it, val_loss=0.541]\u001b[A\n"," 87%|████████▋ | 195/224 [40:09\u003c06:08, 12.70s/it, val_loss=0.537]\u001b[A\n"," 88%|████████▊ | 196/224 [40:09\u003c05:41, 12.20s/it, val_loss=0.537]\u001b[A\n"," 88%|████████▊ | 196/224 [40:19\u003c05:41, 12.20s/it, val_loss=0.63] \u001b[A\n"," 88%|████████▊ | 197/224 [40:19\u003c05:12, 11.56s/it, val_loss=0.63]\u001b[A\n"," 88%|████████▊ | 197/224 [40:30\u003c05:12, 11.56s/it, val_loss=0.667]\u001b[A\n"," 88%|████████▊ | 198/224 [40:30\u003c05:01, 11.59s/it, val_loss=0.667]\u001b[A\n"," 88%|████████▊ | 198/224 [40:41\u003c05:01, 11.59s/it, val_loss=0.5]  \u001b[A\n"," 89%|████████▉ | 199/224 [40:41\u003c04:46, 11.46s/it, val_loss=0.5]\u001b[A\n"," 89%|████████▉ | 199/224 [40:55\u003c04:46, 11.46s/it, val_loss=0.562]\u001b[A\n"," 89%|████████▉ | 200/224 [40:55\u003c04:51, 12.13s/it, val_loss=0.562]\u001b[A\n"," 89%|████████▉ | 200/224 [41:09\u003c04:51, 12.13s/it, val_loss=0.685]\u001b[A\n"," 90%|████████▉ | 201/224 [41:09\u003c04:49, 12.61s/it, val_loss=0.685]\u001b[A\n"," 90%|████████▉ | 201/224 [41:19\u003c04:49, 12.61s/it, val_loss=0.64] \u001b[A\n"," 90%|█████████ | 202/224 [41:19\u003c04:20, 11.86s/it, val_loss=0.64]\u001b[A\n"," 90%|█████████ | 202/224 [41:30\u003c04:20, 11.86s/it, val_loss=0.387]\u001b[A\n"," 91%|█████████ | 203/224 [41:30\u003c04:04, 11.66s/it, val_loss=0.387]\u001b[A\n"," 91%|█████████ | 203/224 [41:44\u003c04:04, 11.66s/it, val_loss=0.556]\u001b[A\n"," 91%|█████████ | 204/224 [41:44\u003c04:03, 12.19s/it, val_loss=0.556]\u001b[A\n"," 91%|█████████ | 204/224 [41:54\u003c04:03, 12.19s/it, val_loss=0.608]\u001b[A\n"," 92%|█████████▏| 205/224 [41:54\u003c03:43, 11.76s/it, val_loss=0.608]\u001b[A\n"," 92%|█████████▏| 205/224 [42:06\u003c03:43, 11.76s/it, val_loss=0.719]\u001b[A\n"," 92%|█████████▏| 206/224 [42:06\u003c03:32, 11.81s/it, val_loss=0.719]\u001b[A\n"," 92%|█████████▏| 206/224 [42:16\u003c03:32, 11.81s/it, val_loss=0.593]\u001b[A\n"," 92%|█████████▏| 207/224 [42:16\u003c03:07, 11.05s/it, val_loss=0.593]\u001b[A\n"," 92%|█████████▏| 207/224 [42:29\u003c03:07, 11.05s/it, val_loss=0.452]\u001b[A\n"," 93%|█████████▎| 208/224 [42:29\u003c03:09, 11.84s/it, val_loss=0.452]\u001b[A\n"," 93%|█████████▎| 208/224 [42:41\u003c03:09, 11.84s/it, val_loss=0.601]\u001b[A\n"," 93%|█████████▎| 209/224 [42:41\u003c02:55, 11.70s/it, val_loss=0.601]\u001b[A\n"," 93%|█████████▎| 209/224 [42:55\u003c02:55, 11.70s/it, val_loss=0.69] \u001b[A\n"," 94%|█████████▍| 210/224 [42:55\u003c02:56, 12.60s/it, val_loss=0.69]\u001b[A\n"," 94%|█████████▍| 210/224 [43:05\u003c02:56, 12.60s/it, val_loss=0.552]\u001b[A\n"," 94%|█████████▍| 211/224 [43:05\u003c02:32, 11.72s/it, val_loss=0.552]\u001b[A\n"," 94%|█████████▍| 211/224 [43:14\u003c02:32, 11.72s/it, val_loss=0.575]\u001b[A\n"," 95%|█████████▍| 212/224 [43:14\u003c02:09, 10.81s/it, val_loss=0.575]\u001b[A\n"," 95%|█████████▍| 212/224 [43:25\u003c02:09, 10.81s/it, val_loss=0.658]\u001b[A\n"," 95%|█████████▌| 213/224 [43:25\u003c01:59, 10.89s/it, val_loss=0.658]\u001b[A\n"," 95%|█████████▌| 213/224 [43:39\u003c01:59, 10.89s/it, val_loss=0.562]\u001b[A\n"," 96%|█████████▌| 214/224 [43:39\u003c01:59, 11.96s/it, val_loss=0.562]\u001b[A\n"," 96%|█████████▌| 214/224 [43:50\u003c01:59, 11.96s/it, val_loss=0.808]\u001b[A\n"," 96%|█████████▌| 215/224 [43:50\u003c01:43, 11.55s/it, val_loss=0.808]\u001b[A\n"," 96%|█████████▌| 215/224 [44:01\u003c01:43, 11.55s/it, val_loss=0.644]\u001b[A\n"," 96%|█████████▋| 216/224 [44:01\u003c01:32, 11.52s/it, val_loss=0.644]\u001b[A\n"," 96%|█████████▋| 216/224 [44:12\u003c01:32, 11.52s/it, val_loss=0.848]\u001b[A\n"," 97%|█████████▋| 217/224 [44:13\u003c01:20, 11.44s/it, val_loss=0.848]\u001b[A\n"," 97%|█████████▋| 217/224 [44:22\u003c01:20, 11.44s/it, val_loss=0.769]\u001b[A\n"," 97%|█████████▋| 218/224 [44:22\u003c01:04, 10.74s/it, val_loss=0.769]\u001b[A\n"," 97%|█████████▋| 218/224 [44:33\u003c01:04, 10.74s/it, val_loss=0.389]\u001b[A\n"," 98%|█████████▊| 219/224 [44:33\u003c00:54, 10.96s/it, val_loss=0.389]\u001b[A\n"," 98%|█████████▊| 219/224 [44:43\u003c00:54, 10.96s/it, val_loss=0.424]\u001b[A\n"," 98%|█████████▊| 220/224 [44:43\u003c00:42, 10.52s/it, val_loss=0.424]\u001b[A\n"," 98%|█████████▊| 220/224 [44:52\u003c00:42, 10.52s/it, val_loss=0.616]\u001b[A\n"," 99%|█████████▊| 221/224 [44:52\u003c00:30, 10.14s/it, val_loss=0.616]\u001b[A\n"," 99%|█████████▊| 221/224 [45:03\u003c00:30, 10.14s/it, val_loss=0.635]\u001b[A\n"," 99%|█████████▉| 222/224 [45:03\u003c00:20, 10.44s/it, val_loss=0.635]\u001b[A\n"," 99%|█████████▉| 222/224 [45:11\u003c00:20, 10.44s/it, val_loss=1.05] \u001b[A\n","100%|█████████▉| 223/224 [45:11\u003c00:09,  9.65s/it, val_loss=1.05]\u001b[A\n","100%|█████████▉| 223/224 [45:19\u003c00:09,  9.65s/it, val_loss=0.324]\u001b[A\n","100%|██████████| 224/224 [45:19\u003c00:00, 12.14s/it, val_loss=0.324]\n"]},{"data":{"text/plain":["{'val_loss': 0.6254954468601452}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["save model weight\n"]},{"name":"stderr","output_type":"stream","text":[" 97%|█████████▋| 997/1026 [4:28:48\u003c06:13, 12.88s/it, loss=0.677, lr=4.03e-6]"]},{"name":"stdout","output_type":"stream","text":["===========steps： 997 ==========\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/224 [00:00\u003c?, ?it/s]\u001b[A\n","  0%|          | 0/224 [00:10\u003c?, ?it/s, val_loss=0.584]\u001b[A\n","  0%|          | 1/224 [00:10\u003c39:14, 10.56s/it, val_loss=0.584]\u001b[A\n","  0%|          | 1/224 [00:20\u003c39:14, 10.56s/it, val_loss=0.712]\u001b[A\n","  1%|          | 2/224 [00:20\u003c38:32, 10.42s/it, val_loss=0.712]\u001b[A\n","  1%|          | 2/224 [00:34\u003c38:32, 10.42s/it, val_loss=0.503]\u001b[A\n","  1%|▏         | 3/224 [00:34\u003c43:33, 11.83s/it, val_loss=0.503]\u001b[A\n","  1%|▏         | 3/224 [00:45\u003c43:33, 11.83s/it, val_loss=0.577]\u001b[A\n","  2%|▏         | 4/224 [00:45\u003c43:03, 11.74s/it, val_loss=0.577]\u001b[A\n","  2%|▏         | 4/224 [00:59\u003c43:03, 11.74s/it, val_loss=0.503]\u001b[A\n","  2%|▏         | 5/224 [00:59\u003c45:36, 12.50s/it, val_loss=0.503]\u001b[A\n","  2%|▏         | 5/224 [01:15\u003c45:36, 12.50s/it, val_loss=0.698]\u001b[A\n","  3%|▎         | 6/224 [01:15\u003c48:45, 13.42s/it, val_loss=0.698]\u001b[A\n","  3%|▎         | 6/224 [01:25\u003c48:45, 13.42s/it, val_loss=0.937]\u001b[A\n","  3%|▎         | 7/224 [01:25\u003c44:42, 12.36s/it, val_loss=0.937]\u001b[A\n","  3%|▎         | 7/224 [01:39\u003c44:42, 12.36s/it, val_loss=0.554]\u001b[A\n","  4%|▎         | 8/224 [01:39\u003c46:50, 13.01s/it, val_loss=0.554]\u001b[A\n","  4%|▎         | 8/224 [01:46\u003c46:50, 13.01s/it, val_loss=0.246]\u001b[A\n","  4%|▍         | 9/224 [01:46\u003c39:33, 11.04s/it, val_loss=0.246]\u001b[A\n","  4%|▍         | 9/224 [02:01\u003c39:33, 11.04s/it, val_loss=0.859]\u001b[A\n","  4%|▍         | 10/224 [02:01\u003c43:37, 12.23s/it, val_loss=0.859]\u001b[A\n","  4%|▍         | 10/224 [02:12\u003c43:37, 12.23s/it, val_loss=0.642]\u001b[A\n","  5%|▍         | 11/224 [02:12\u003c41:52, 11.80s/it, val_loss=0.642]\u001b[A\n","  5%|▍         | 11/224 [02:24\u003c41:52, 11.80s/it, val_loss=0.566]\u001b[A\n","  5%|▌         | 12/224 [02:24\u003c42:44, 12.10s/it, val_loss=0.566]\u001b[A\n","  5%|▌         | 12/224 [02:34\u003c42:44, 12.10s/it, val_loss=0.534]\u001b[A\n","  6%|▌         | 13/224 [02:34\u003c39:48, 11.32s/it, val_loss=0.534]\u001b[A\n","  6%|▌         | 13/224 [02:46\u003c39:48, 11.32s/it, val_loss=0.542]\u001b[A\n","  6%|▋         | 14/224 [02:46\u003c40:21, 11.53s/it, val_loss=0.542]\u001b[A\n","  6%|▋         | 14/224 [02:54\u003c40:21, 11.53s/it, val_loss=0.729]\u001b[A\n","  7%|▋         | 15/224 [02:54\u003c37:06, 10.65s/it, val_loss=0.729]\u001b[A\n","  7%|▋         | 15/224 [03:07\u003c37:06, 10.65s/it, val_loss=0.692]\u001b[A\n","  7%|▋         | 16/224 [03:07\u003c39:00, 11.25s/it, val_loss=0.692]\u001b[A\n","  7%|▋         | 16/224 [03:22\u003c39:00, 11.25s/it, val_loss=0.593]\u001b[A\n","  8%|▊         | 17/224 [03:22\u003c42:46, 12.40s/it, val_loss=0.593]\u001b[A\n","  8%|▊         | 17/224 [03:31\u003c42:46, 12.40s/it, val_loss=0.577]\u001b[A\n","  8%|▊         | 18/224 [03:31\u003c38:42, 11.28s/it, val_loss=0.577]\u001b[A\n","  8%|▊         | 18/224 [03:44\u003c38:42, 11.28s/it, val_loss=0.796]\u001b[A\n","  8%|▊         | 19/224 [03:44\u003c40:01, 11.71s/it, val_loss=0.796]\u001b[A\n","  8%|▊         | 19/224 [03:57\u003c40:01, 11.71s/it, val_loss=0.617]\u001b[A\n","  9%|▉         | 20/224 [03:57\u003c41:46, 12.29s/it, val_loss=0.617]\u001b[A\n","  9%|▉         | 20/224 [04:09\u003c41:46, 12.29s/it, val_loss=0.543]\u001b[A\n","  9%|▉         | 21/224 [04:09\u003c40:53, 12.09s/it, val_loss=0.543]\u001b[A\n","  9%|▉         | 21/224 [04:22\u003c40:53, 12.09s/it, val_loss=0.669]\u001b[A\n"," 10%|▉         | 22/224 [04:22\u003c41:18, 12.27s/it, val_loss=0.669]\u001b[A\n"," 10%|▉         | 22/224 [04:31\u003c41:18, 12.27s/it, val_loss=0.382]\u001b[A\n"," 10%|█         | 23/224 [04:31\u003c38:15, 11.42s/it, val_loss=0.382]\u001b[A\n"," 10%|█         | 23/224 [04:44\u003c38:15, 11.42s/it, val_loss=0.859]\u001b[A\n"," 11%|█         | 24/224 [04:44\u003c39:46, 11.93s/it, val_loss=0.859]\u001b[A\n"," 11%|█         | 24/224 [05:00\u003c39:46, 11.93s/it, val_loss=0.839]\u001b[A\n"," 11%|█         | 25/224 [05:00\u003c43:13, 13.03s/it, val_loss=0.839]\u001b[A\n"," 11%|█         | 25/224 [05:07\u003c43:13, 13.03s/it, val_loss=0.591]\u001b[A\n"," 12%|█▏        | 26/224 [05:07\u003c37:42, 11.43s/it, val_loss=0.591]\u001b[A\n"," 12%|█▏        | 26/224 [05:17\u003c37:42, 11.43s/it, val_loss=0.612]\u001b[A\n"," 12%|█▏        | 27/224 [05:17\u003c35:47, 10.90s/it, val_loss=0.612]\u001b[A\n"," 12%|█▏        | 27/224 [05:30\u003c35:47, 10.90s/it, val_loss=0.632]\u001b[A\n"," 12%|█▎        | 28/224 [05:30\u003c37:11, 11.39s/it, val_loss=0.632]\u001b[A\n"," 12%|█▎        | 28/224 [05:42\u003c37:11, 11.39s/it, val_loss=0.881]\u001b[A\n"," 13%|█▎        | 29/224 [05:42\u003c38:05, 11.72s/it, val_loss=0.881]\u001b[A\n"," 13%|█▎        | 29/224 [05:59\u003c38:05, 11.72s/it, val_loss=0.527]\u001b[A\n"," 13%|█▎        | 30/224 [05:59\u003c42:31, 13.15s/it, val_loss=0.527]\u001b[A\n"," 13%|█▎        | 30/224 [06:11\u003c42:31, 13.15s/it, val_loss=0.723]\u001b[A\n"," 14%|█▍        | 31/224 [06:11\u003c41:51, 13.01s/it, val_loss=0.723]\u001b[A\n"," 14%|█▍        | 31/224 [06:22\u003c41:51, 13.01s/it, val_loss=0.897]\u001b[A\n"," 14%|█▍        | 32/224 [06:22\u003c39:34, 12.37s/it, val_loss=0.897]\u001b[A\n"," 14%|█▍        | 32/224 [06:33\u003c39:34, 12.37s/it, val_loss=0.545]\u001b[A\n"," 15%|█▍        | 33/224 [06:33\u003c38:03, 11.96s/it, val_loss=0.545]\u001b[A\n"," 15%|█▍        | 33/224 [06:43\u003c38:03, 11.96s/it, val_loss=0.597]\u001b[A\n"," 15%|█▌        | 34/224 [06:43\u003c35:46, 11.30s/it, val_loss=0.597]\u001b[A\n"," 15%|█▌        | 34/224 [06:55\u003c35:46, 11.30s/it, val_loss=0.729]\u001b[A\n"," 16%|█▌        | 35/224 [06:55\u003c36:38, 11.63s/it, val_loss=0.729]\u001b[A\n"," 16%|█▌        | 35/224 [07:08\u003c36:38, 11.63s/it, val_loss=0.675]\u001b[A\n"," 16%|█▌        | 36/224 [07:08\u003c37:34, 11.99s/it, val_loss=0.675]\u001b[A\n"," 16%|█▌        | 36/224 [07:16\u003c37:34, 11.99s/it, val_loss=0.647]\u001b[A\n"," 17%|█▋        | 37/224 [07:16\u003c33:33, 10.77s/it, val_loss=0.647]\u001b[A\n"," 17%|█▋        | 37/224 [07:29\u003c33:33, 10.77s/it, val_loss=0.834]\u001b[A\n"," 17%|█▋        | 38/224 [07:29\u003c35:25, 11.43s/it, val_loss=0.834]\u001b[A\n"," 17%|█▋        | 38/224 [07:44\u003c35:25, 11.43s/it, val_loss=0.411]\u001b[A\n"," 17%|█▋        | 39/224 [07:44\u003c38:23, 12.45s/it, val_loss=0.411]\u001b[A\n"," 17%|█▋        | 39/224 [07:59\u003c38:23, 12.45s/it, val_loss=0.467]\u001b[A\n"," 18%|█▊        | 40/224 [07:59\u003c40:32, 13.22s/it, val_loss=0.467]\u001b[A\n"," 18%|█▊        | 40/224 [08:11\u003c40:32, 13.22s/it, val_loss=1.06] \u001b[A\n"," 18%|█▊        | 41/224 [08:11\u003c39:44, 13.03s/it, val_loss=1.06]\u001b[A\n"," 18%|█▊        | 41/224 [08:23\u003c39:44, 13.03s/it, val_loss=0.501]\u001b[A\n"," 19%|█▉        | 42/224 [08:23\u003c38:36, 12.73s/it, val_loss=0.501]\u001b[A\n"," 19%|█▉        | 42/224 [08:36\u003c38:36, 12.73s/it, val_loss=0.534]\u001b[A\n"," 19%|█▉        | 43/224 [08:36\u003c37:55, 12.57s/it, val_loss=0.534]\u001b[A\n"," 19%|█▉        | 43/224 [08:48\u003c37:55, 12.57s/it, val_loss=0.936]\u001b[A\n"," 20%|█▉        | 44/224 [08:48\u003c37:55, 12.64s/it, val_loss=0.936]\u001b[A\n"," 20%|█▉        | 44/224 [09:02\u003c37:55, 12.64s/it, val_loss=0.652]\u001b[A\n"," 20%|██        | 45/224 [09:02\u003c38:27, 12.89s/it, val_loss=0.652]\u001b[A\n"," 20%|██        | 45/224 [09:12\u003c38:27, 12.89s/it, val_loss=0.631]\u001b[A\n"," 21%|██        | 46/224 [09:12\u003c35:58, 12.13s/it, val_loss=0.631]\u001b[A\n"," 21%|██        | 46/224 [09:26\u003c35:58, 12.13s/it, val_loss=0.422]\u001b[A\n"," 21%|██        | 47/224 [09:26\u003c37:24, 12.68s/it, val_loss=0.422]\u001b[A\n"," 21%|██        | 47/224 [09:42\u003c37:24, 12.68s/it, val_loss=0.461]\u001b[A\n"," 21%|██▏       | 48/224 [09:42\u003c39:59, 13.63s/it, val_loss=0.461]\u001b[A\n"," 21%|██▏       | 48/224 [09:55\u003c39:59, 13.63s/it, val_loss=0.898]\u001b[A\n"," 22%|██▏       | 49/224 [09:55\u003c38:51, 13.32s/it, val_loss=0.898]\u001b[A\n"," 22%|██▏       | 49/224 [10:06\u003c38:51, 13.32s/it, val_loss=0.557]\u001b[A\n"," 22%|██▏       | 50/224 [10:06\u003c36:46, 12.68s/it, val_loss=0.557]\u001b[A\n"," 22%|██▏       | 50/224 [10:18\u003c36:46, 12.68s/it, val_loss=0.599]\u001b[A\n"," 23%|██▎       | 51/224 [10:18\u003c36:13, 12.56s/it, val_loss=0.599]\u001b[A\n"," 23%|██▎       | 51/224 [10:33\u003c36:13, 12.56s/it, val_loss=0.915]\u001b[A\n"," 23%|██▎       | 52/224 [10:33\u003c38:13, 13.33s/it, val_loss=0.915]\u001b[A\n"," 23%|██▎       | 52/224 [10:44\u003c38:13, 13.33s/it, val_loss=1.15] \u001b[A\n"," 24%|██▎       | 53/224 [10:44\u003c36:02, 12.65s/it, val_loss=1.15]\u001b[A\n"," 24%|██▎       | 53/224 [10:57\u003c36:02, 12.65s/it, val_loss=0.334]\u001b[A\n"," 24%|██▍       | 54/224 [10:57\u003c35:47, 12.63s/it, val_loss=0.334]\u001b[A\n"," 24%|██▍       | 54/224 [11:09\u003c35:47, 12.63s/it, val_loss=0.445]\u001b[A\n"," 25%|██▍       | 55/224 [11:09\u003c34:57, 12.41s/it, val_loss=0.445]\u001b[A\n"," 25%|██▍       | 55/224 [11:22\u003c34:57, 12.41s/it, val_loss=0.782]\u001b[A\n"," 25%|██▌       | 56/224 [11:22\u003c35:09, 12.55s/it, val_loss=0.782]\u001b[A\n"," 25%|██▌       | 56/224 [11:34\u003c35:09, 12.55s/it, val_loss=0.623]\u001b[A\n"," 25%|██▌       | 57/224 [11:34\u003c34:22, 12.35s/it, val_loss=0.623]\u001b[A\n"," 25%|██▌       | 57/224 [11:44\u003c34:22, 12.35s/it, val_loss=0.445]\u001b[A\n"," 26%|██▌       | 58/224 [11:44\u003c32:50, 11.87s/it, val_loss=0.445]\u001b[A\n"," 26%|██▌       | 58/224 [11:57\u003c32:50, 11.87s/it, val_loss=0.508]\u001b[A\n"," 26%|██▋       | 59/224 [11:57\u003c33:06, 12.04s/it, val_loss=0.508]\u001b[A\n"," 26%|██▋       | 59/224 [12:10\u003c33:06, 12.04s/it, val_loss=0.496]\u001b[A\n"," 27%|██▋       | 60/224 [12:10\u003c33:54, 12.41s/it, val_loss=0.496]\u001b[A\n"," 27%|██▋       | 60/224 [12:23\u003c33:54, 12.41s/it, val_loss=0.455]\u001b[A\n"," 27%|██▋       | 61/224 [12:23\u003c33:53, 12.48s/it, val_loss=0.455]\u001b[A\n"," 27%|██▋       | 61/224 [12:35\u003c33:53, 12.48s/it, val_loss=0.649]\u001b[A\n"," 28%|██▊       | 62/224 [12:35\u003c33:49, 12.53s/it, val_loss=0.649]\u001b[A\n"," 28%|██▊       | 62/224 [12:48\u003c33:49, 12.53s/it, val_loss=0.436]\u001b[A\n"," 28%|██▊       | 63/224 [12:48\u003c33:18, 12.41s/it, val_loss=0.436]\u001b[A\n"," 28%|██▊       | 63/224 [12:57\u003c33:18, 12.41s/it, val_loss=0.528]\u001b[A\n"," 29%|██▊       | 64/224 [12:57\u003c30:51, 11.57s/it, val_loss=0.528]\u001b[A\n"," 29%|██▊       | 64/224 [13:13\u003c30:51, 11.57s/it, val_loss=0.509]\u001b[A\n"," 29%|██▉       | 65/224 [13:13\u003c34:28, 13.01s/it, val_loss=0.509]\u001b[A\n"," 29%|██▉       | 65/224 [13:24\u003c34:28, 13.01s/it, val_loss=1.44] \u001b[A\n"," 29%|██▉       | 66/224 [13:24\u003c32:36, 12.39s/it, val_loss=1.44]\u001b[A\n"," 29%|██▉       | 66/224 [13:37\u003c32:36, 12.39s/it, val_loss=0.779]\u001b[A\n"," 30%|██▉       | 67/224 [13:37\u003c32:23, 12.38s/it, val_loss=0.779]\u001b[A\n"," 30%|██▉       | 67/224 [13:49\u003c32:23, 12.38s/it, val_loss=0.661]\u001b[A\n"," 30%|███       | 68/224 [13:49\u003c32:06, 12.35s/it, val_loss=0.661]\u001b[A\n"," 30%|███       | 68/224 [13:58\u003c32:06, 12.35s/it, val_loss=0.496]\u001b[A\n"," 31%|███       | 69/224 [13:58\u003c29:14, 11.32s/it, val_loss=0.496]\u001b[A\n"," 31%|███       | 69/224 [14:09\u003c29:14, 11.32s/it, val_loss=0.579]\u001b[A\n"," 31%|███▏      | 70/224 [14:09\u003c29:01, 11.31s/it, val_loss=0.579]\u001b[A\n"," 31%|███▏      | 70/224 [14:20\u003c29:01, 11.31s/it, val_loss=0.947]\u001b[A\n"," 32%|███▏      | 71/224 [14:20\u003c28:14, 11.07s/it, val_loss=0.947]\u001b[A\n"," 32%|███▏      | 71/224 [14:32\u003c28:14, 11.07s/it, val_loss=0.69] \u001b[A\n"," 32%|███▏      | 72/224 [14:32\u003c28:48, 11.37s/it, val_loss=0.69]\u001b[A\n"," 32%|███▏      | 72/224 [14:42\u003c28:48, 11.37s/it, val_loss=0.594]\u001b[A\n"," 33%|███▎      | 73/224 [14:42\u003c27:45, 11.03s/it, val_loss=0.594]\u001b[A\n"," 33%|███▎      | 73/224 [14:53\u003c27:45, 11.03s/it, val_loss=0.58] \u001b[A\n"," 33%|███▎      | 74/224 [14:53\u003c27:19, 10.93s/it, val_loss=0.58]\u001b[A\n"," 33%|███▎      | 74/224 [15:02\u003c27:19, 10.93s/it, val_loss=0.556]\u001b[A\n"," 33%|███▎      | 75/224 [15:02\u003c26:07, 10.52s/it, val_loss=0.556]\u001b[A\n"," 33%|███▎      | 75/224 [15:17\u003c26:07, 10.52s/it, val_loss=0.722]\u001b[A\n"," 34%|███▍      | 76/224 [15:17\u003c28:54, 11.72s/it, val_loss=0.722]\u001b[A\n"," 34%|███▍      | 76/224 [15:27\u003c28:54, 11.72s/it, val_loss=0.626]\u001b[A\n"," 34%|███▍      | 77/224 [15:27\u003c27:14, 11.12s/it, val_loss=0.626]\u001b[A\n"," 34%|███▍      | 77/224 [15:38\u003c27:14, 11.12s/it, val_loss=0.375]\u001b[A\n"," 35%|███▍      | 78/224 [15:38\u003c27:14, 11.19s/it, val_loss=0.375]\u001b[A\n"," 35%|███▍      | 78/224 [15:48\u003c27:14, 11.19s/it, val_loss=0.819]\u001b[A\n"," 35%|███▌      | 79/224 [15:48\u003c25:52, 10.71s/it, val_loss=0.819]\u001b[A\n"," 35%|███▌      | 79/224 [16:01\u003c25:52, 10.71s/it, val_loss=0.38] \u001b[A\n"," 36%|███▌      | 80/224 [16:01\u003c27:57, 11.65s/it, val_loss=0.38]\u001b[A\n"," 36%|███▌      | 80/224 [16:11\u003c27:57, 11.65s/it, val_loss=0.651]\u001b[A\n"," 36%|███▌      | 81/224 [16:11\u003c26:39, 11.19s/it, val_loss=0.651]\u001b[A\n"," 36%|███▌      | 81/224 [16:24\u003c26:39, 11.19s/it, val_loss=0.968]\u001b[A\n"," 37%|███▋      | 82/224 [16:24\u003c27:39, 11.69s/it, val_loss=0.968]\u001b[A\n"," 37%|███▋      | 82/224 [16:39\u003c27:39, 11.69s/it, val_loss=0.623]\u001b[A\n"," 37%|███▋      | 83/224 [16:39\u003c29:25, 12.52s/it, val_loss=0.623]\u001b[A\n"," 37%|███▋      | 83/224 [16:52\u003c29:25, 12.52s/it, val_loss=0.391]\u001b[A\n"," 38%|███▊      | 84/224 [16:52\u003c29:39, 12.71s/it, val_loss=0.391]\u001b[A\n"," 38%|███▊      | 84/224 [17:07\u003c29:39, 12.71s/it, val_loss=0.493]\u001b[A\n"," 38%|███▊      | 85/224 [17:07\u003c31:02, 13.40s/it, val_loss=0.493]\u001b[A\n"," 38%|███▊      | 85/224 [17:18\u003c31:02, 13.40s/it, val_loss=0.811]\u001b[A\n"," 38%|███▊      | 86/224 [17:18\u003c29:12, 12.70s/it, val_loss=0.811]\u001b[A\n"," 38%|███▊      | 86/224 [17:30\u003c29:12, 12.70s/it, val_loss=0.769]\u001b[A\n"," 39%|███▉      | 87/224 [17:30\u003c28:29, 12.48s/it, val_loss=0.769]\u001b[A\n"," 39%|███▉      | 87/224 [17:43\u003c28:29, 12.48s/it, val_loss=0.466]\u001b[A\n"," 39%|███▉      | 88/224 [17:43\u003c28:41, 12.66s/it, val_loss=0.466]\u001b[A\n"," 39%|███▉      | 88/224 [17:56\u003c28:41, 12.66s/it, val_loss=0.463]\u001b[A\n"," 40%|███▉      | 89/224 [17:56\u003c28:26, 12.64s/it, val_loss=0.463]\u001b[A\n"," 40%|███▉      | 89/224 [18:11\u003c28:26, 12.64s/it, val_loss=0.579]\u001b[A\n"," 40%|████      | 90/224 [18:11\u003c29:56, 13.40s/it, val_loss=0.579]\u001b[A\n"," 40%|████      | 90/224 [18:23\u003c29:56, 13.40s/it, val_loss=0.464]\u001b[A\n"," 41%|████      | 91/224 [18:23\u003c28:42, 12.95s/it, val_loss=0.464]\u001b[A\n"," 41%|████      | 91/224 [18:35\u003c28:42, 12.95s/it, val_loss=0.57] \u001b[A\n"," 41%|████      | 92/224 [18:35\u003c27:55, 12.69s/it, val_loss=0.57]\u001b[A\n"," 41%|████      | 92/224 [18:48\u003c27:55, 12.69s/it, val_loss=0.527]\u001b[A\n"," 42%|████▏     | 93/224 [18:48\u003c28:15, 12.95s/it, val_loss=0.527]\u001b[A\n"," 42%|████▏     | 93/224 [18:59\u003c28:15, 12.95s/it, val_loss=0.617]\u001b[A\n"," 42%|████▏     | 94/224 [18:59\u003c26:15, 12.12s/it, val_loss=0.617]\u001b[A\n"," 42%|████▏     | 94/224 [19:12\u003c26:15, 12.12s/it, val_loss=0.687]\u001b[A\n"," 42%|████▏     | 95/224 [19:12\u003c26:37, 12.39s/it, val_loss=0.687]\u001b[A\n"," 42%|████▏     | 95/224 [19:27\u003c26:37, 12.39s/it, val_loss=0.719]\u001b[A\n"," 43%|████▎     | 96/224 [19:27\u003c28:35, 13.40s/it, val_loss=0.719]\u001b[A\n"," 43%|████▎     | 96/224 [19:39\u003c28:35, 13.40s/it, val_loss=0.395]\u001b[A\n"," 43%|████▎     | 97/224 [19:39\u003c27:12, 12.86s/it, val_loss=0.395]\u001b[A\n"," 43%|████▎     | 97/224 [19:51\u003c27:12, 12.86s/it, val_loss=0.546]\u001b[A\n"," 44%|████▍     | 98/224 [19:51\u003c26:23, 12.57s/it, val_loss=0.546]\u001b[A\n"," 44%|████▍     | 98/224 [20:00\u003c26:23, 12.57s/it, val_loss=0.593]\u001b[A\n"," 44%|████▍     | 99/224 [20:00\u003c23:51, 11.45s/it, val_loss=0.593]\u001b[A\n"," 44%|████▍     | 99/224 [20:12\u003c23:51, 11.45s/it, val_loss=0.467]\u001b[A\n"," 45%|████▍     | 100/224 [20:12\u003c24:05, 11.66s/it, val_loss=0.467]\u001b[A\n"," 45%|████▍     | 100/224 [20:22\u003c24:05, 11.66s/it, val_loss=0.512]\u001b[A\n"," 45%|████▌     | 101/224 [20:22\u003c23:13, 11.33s/it, val_loss=0.512]\u001b[A\n"," 45%|████▌     | 101/224 [20:36\u003c23:13, 11.33s/it, val_loss=0.635]\u001b[A\n"," 46%|████▌     | 102/224 [20:36\u003c24:18, 11.96s/it, val_loss=0.635]\u001b[A\n"," 46%|████▌     | 102/224 [20:46\u003c24:18, 11.96s/it, val_loss=0.639]\u001b[A\n"," 46%|████▌     | 103/224 [20:46\u003c22:53, 11.35s/it, val_loss=0.639]\u001b[A\n"," 46%|████▌     | 103/224 [20:54\u003c22:53, 11.35s/it, val_loss=0.661]\u001b[A\n"," 46%|████▋     | 104/224 [20:54\u003c21:07, 10.56s/it, val_loss=0.661]\u001b[A\n"," 46%|████▋     | 104/224 [21:05\u003c21:07, 10.56s/it, val_loss=0.637]\u001b[A\n"," 47%|████▋     | 105/224 [21:05\u003c21:11, 10.69s/it, val_loss=0.637]\u001b[A\n"," 47%|████▋     | 105/224 [21:19\u003c21:11, 10.69s/it, val_loss=0.582]\u001b[A\n"," 47%|████▋     | 106/224 [21:19\u003c22:46, 11.58s/it, val_loss=0.582]\u001b[A\n"," 47%|████▋     | 106/224 [21:29\u003c22:46, 11.58s/it, val_loss=0.653]\u001b[A\n"," 48%|████▊     | 107/224 [21:29\u003c21:47, 11.17s/it, val_loss=0.653]\u001b[A\n"," 48%|████▊     | 107/224 [21:40\u003c21:47, 11.17s/it, val_loss=0.636]\u001b[A\n"," 48%|████▊     | 108/224 [21:40\u003c21:23, 11.07s/it, val_loss=0.636]\u001b[A\n"," 48%|████▊     | 108/224 [21:57\u003c21:23, 11.07s/it, val_loss=0.676]\u001b[A\n"," 49%|████▊     | 109/224 [21:57\u003c24:39, 12.87s/it, val_loss=0.676]\u001b[A\n"," 49%|████▊     | 109/224 [22:09\u003c24:39, 12.87s/it, val_loss=0.758]\u001b[A\n"," 49%|████▉     | 110/224 [22:09\u003c23:49, 12.54s/it, val_loss=0.758]\u001b[A\n"," 49%|████▉     | 110/224 [22:21\u003c23:49, 12.54s/it, val_loss=0.667]\u001b[A\n"," 50%|████▉     | 111/224 [22:21\u003c23:12, 12.32s/it, val_loss=0.667]\u001b[A\n"," 50%|████▉     | 111/224 [22:35\u003c23:12, 12.32s/it, val_loss=0.55] \u001b[A\n"," 50%|█████     | 112/224 [22:35\u003c23:48, 12.76s/it, val_loss=0.55]\u001b[A\n"," 50%|█████     | 112/224 [22:48\u003c23:48, 12.76s/it, val_loss=0.868]\u001b[A\n"," 50%|█████     | 113/224 [22:48\u003c23:44, 12.83s/it, val_loss=0.868]\u001b[A\n"," 50%|█████     | 113/224 [23:03\u003c23:44, 12.83s/it, val_loss=0.739]\u001b[A\n"," 51%|█████     | 114/224 [23:03\u003c24:45, 13.51s/it, val_loss=0.739]\u001b[A\n"," 51%|█████     | 114/224 [23:15\u003c24:45, 13.51s/it, val_loss=0.684]\u001b[A\n"," 51%|█████▏    | 115/224 [23:15\u003c23:57, 13.19s/it, val_loss=0.684]\u001b[A\n"," 51%|█████▏    | 115/224 [23:31\u003c23:57, 13.19s/it, val_loss=0.547]\u001b[A\n"," 52%|█████▏    | 116/224 [23:31\u003c25:03, 13.92s/it, val_loss=0.547]\u001b[A\n"," 52%|█████▏    | 116/224 [23:41\u003c25:03, 13.92s/it, val_loss=0.48] \u001b[A\n"," 52%|█████▏    | 117/224 [23:41\u003c22:57, 12.87s/it, val_loss=0.48]\u001b[A\n"," 52%|█████▏    | 117/224 [23:54\u003c22:57, 12.87s/it, val_loss=0.513]\u001b[A\n"," 53%|█████▎    | 118/224 [23:54\u003c22:33, 12.76s/it, val_loss=0.513]\u001b[A\n"," 53%|█████▎    | 118/224 [24:06\u003c22:33, 12.76s/it, val_loss=0.519]\u001b[A\n"," 53%|█████▎    | 119/224 [24:06\u003c22:04, 12.62s/it, val_loss=0.519]\u001b[A\n"," 53%|█████▎    | 119/224 [24:18\u003c22:04, 12.62s/it, val_loss=0.882]\u001b[A\n"," 54%|█████▎    | 120/224 [24:18\u003c21:20, 12.31s/it, val_loss=0.882]\u001b[A\n"," 54%|█████▎    | 120/224 [24:28\u003c21:20, 12.31s/it, val_loss=0.447]\u001b[A\n"," 54%|█████▍    | 121/224 [24:28\u003c20:08, 11.74s/it, val_loss=0.447]\u001b[A\n"," 54%|█████▍    | 121/224 [24:38\u003c20:08, 11.74s/it, val_loss=0.787]\u001b[A\n"," 54%|█████▍    | 122/224 [24:38\u003c19:15, 11.33s/it, val_loss=0.787]\u001b[A\n"," 54%|█████▍    | 122/224 [24:50\u003c19:15, 11.33s/it, val_loss=0.56] \u001b[A\n"," 55%|█████▍    | 123/224 [24:50\u003c19:26, 11.55s/it, val_loss=0.56]\u001b[A\n"," 55%|█████▍    | 123/224 [25:04\u003c19:26, 11.55s/it, val_loss=0.496]\u001b[A\n"," 55%|█████▌    | 124/224 [25:04\u003c20:18, 12.19s/it, val_loss=0.496]\u001b[A\n"," 55%|█████▌    | 124/224 [25:14\u003c20:18, 12.19s/it, val_loss=0.507]\u001b[A\n"," 56%|█████▌    | 125/224 [25:14\u003c19:06, 11.58s/it, val_loss=0.507]\u001b[A\n"," 56%|█████▌    | 125/224 [25:27\u003c19:06, 11.58s/it, val_loss=0.338]\u001b[A\n"," 56%|█████▋    | 126/224 [25:27\u003c19:39, 12.04s/it, val_loss=0.338]\u001b[A\n"," 56%|█████▋    | 126/224 [25:36\u003c19:39, 12.04s/it, val_loss=0.66] \u001b[A\n"," 57%|█████▋    | 127/224 [25:36\u003c17:58, 11.11s/it, val_loss=0.66]\u001b[A\n"," 57%|█████▋    | 127/224 [25:46\u003c17:58, 11.11s/it, val_loss=0.62]\u001b[A\n"," 57%|█████▋    | 128/224 [25:46\u003c16:59, 10.62s/it, val_loss=0.62]\u001b[A\n"," 57%|█████▋    | 128/224 [26:00\u003c16:59, 10.62s/it, val_loss=0.408]\u001b[A\n"," 58%|█████▊    | 129/224 [26:00\u003c18:34, 11.73s/it, val_loss=0.408]\u001b[A\n"," 58%|█████▊    | 129/224 [26:12\u003c18:34, 11.73s/it, val_loss=0.733]\u001b[A\n"," 58%|█████▊    | 130/224 [26:12\u003c18:28, 11.79s/it, val_loss=0.733]\u001b[A\n"," 58%|█████▊    | 130/224 [26:25\u003c18:28, 11.79s/it, val_loss=0.588]\u001b[A\n"," 58%|█████▊    | 131/224 [26:25\u003c18:37, 12.01s/it, val_loss=0.588]\u001b[A\n"," 58%|█████▊    | 131/224 [26:39\u003c18:37, 12.01s/it, val_loss=0.655]\u001b[A\n"," 59%|█████▉    | 132/224 [26:39\u003c19:39, 12.82s/it, val_loss=0.655]\u001b[A\n"," 59%|█████▉    | 132/224 [26:52\u003c19:39, 12.82s/it, val_loss=0.697]\u001b[A\n"," 59%|█████▉    | 133/224 [26:52\u003c19:30, 12.87s/it, val_loss=0.697]\u001b[A\n"," 59%|█████▉    | 133/224 [27:07\u003c19:30, 12.87s/it, val_loss=0.681]\u001b[A\n"," 60%|█████▉    | 134/224 [27:07\u003c20:08, 13.43s/it, val_loss=0.681]\u001b[A\n"," 60%|█████▉    | 134/224 [27:18\u003c20:08, 13.43s/it, val_loss=1.08] \u001b[A\n"," 60%|██████    | 135/224 [27:18\u003c18:48, 12.68s/it, val_loss=1.08]\u001b[A\n"," 60%|██████    | 135/224 [27:31\u003c18:48, 12.68s/it, val_loss=0.764]\u001b[A\n"," 61%|██████    | 136/224 [27:31\u003c18:34, 12.67s/it, val_loss=0.764]\u001b[A\n"," 61%|██████    | 136/224 [27:41\u003c18:34, 12.67s/it, val_loss=0.628]\u001b[A\n"," 61%|██████    | 137/224 [27:41\u003c17:26, 12.03s/it, val_loss=0.628]\u001b[A\n"," 61%|██████    | 137/224 [27:51\u003c17:26, 12.03s/it, val_loss=0.576]\u001b[A\n"," 62%|██████▏   | 138/224 [27:51\u003c16:25, 11.46s/it, val_loss=0.576]\u001b[A\n"," 62%|██████▏   | 138/224 [28:09\u003c16:25, 11.46s/it, val_loss=0.737]\u001b[A\n"," 62%|██████▏   | 139/224 [28:09\u003c18:44, 13.23s/it, val_loss=0.737]\u001b[A\n"," 62%|██████▏   | 139/224 [28:20\u003c18:44, 13.23s/it, val_loss=0.359]\u001b[A\n"," 62%|██████▎   | 140/224 [28:20\u003c17:48, 12.72s/it, val_loss=0.359]\u001b[A\n"," 62%|██████▎   | 140/224 [28:35\u003c17:48, 12.72s/it, val_loss=0.648]\u001b[A\n"," 63%|██████▎   | 141/224 [28:35\u003c18:20, 13.26s/it, val_loss=0.648]\u001b[A\n"," 63%|██████▎   | 141/224 [28:48\u003c18:20, 13.26s/it, val_loss=0.549]\u001b[A\n"," 63%|██████▎   | 142/224 [28:48\u003c18:00, 13.18s/it, val_loss=0.549]\u001b[A\n"," 63%|██████▎   | 142/224 [29:02\u003c18:00, 13.18s/it, val_loss=0.466]\u001b[A\n"," 64%|██████▍   | 143/224 [29:02\u003c18:08, 13.43s/it, val_loss=0.466]\u001b[A\n"," 64%|██████▍   | 143/224 [29:13\u003c18:08, 13.43s/it, val_loss=0.384]\u001b[A\n"," 64%|██████▍   | 144/224 [29:13\u003c16:54, 12.68s/it, val_loss=0.384]\u001b[A\n"," 64%|██████▍   | 144/224 [29:23\u003c16:54, 12.68s/it, val_loss=0.619]\u001b[A\n"," 65%|██████▍   | 145/224 [29:23\u003c15:58, 12.13s/it, val_loss=0.619]\u001b[A\n"," 65%|██████▍   | 145/224 [29:32\u003c15:58, 12.13s/it, val_loss=0.552]\u001b[A\n"," 65%|██████▌   | 146/224 [29:32\u003c14:23, 11.07s/it, val_loss=0.552]\u001b[A\n"," 65%|██████▌   | 146/224 [29:42\u003c14:23, 11.07s/it, val_loss=0.482]\u001b[A\n"," 66%|██████▌   | 147/224 [29:42\u003c13:37, 10.62s/it, val_loss=0.482]\u001b[A\n"," 66%|██████▌   | 147/224 [29:52\u003c13:37, 10.62s/it, val_loss=0.592]\u001b[A\n"," 66%|██████▌   | 148/224 [29:52\u003c13:18, 10.51s/it, val_loss=0.592]\u001b[A\n"," 66%|██████▌   | 148/224 [30:02\u003c13:18, 10.51s/it, val_loss=0.683]\u001b[A\n"," 67%|██████▋   | 149/224 [30:02\u003c13:04, 10.45s/it, val_loss=0.683]\u001b[A\n"," 67%|██████▋   | 149/224 [30:15\u003c13:04, 10.45s/it, val_loss=0.503]\u001b[A\n"," 67%|██████▋   | 150/224 [30:15\u003c13:41, 11.10s/it, val_loss=0.503]\u001b[A\n"," 67%|██████▋   | 150/224 [30:29\u003c13:41, 11.10s/it, val_loss=0.525]\u001b[A\n"," 67%|██████▋   | 151/224 [30:29\u003c14:37, 12.02s/it, val_loss=0.525]\u001b[A\n"," 67%|██████▋   | 151/224 [30:38\u003c14:37, 12.02s/it, val_loss=0.726]\u001b[A\n"," 68%|██████▊   | 152/224 [30:38\u003c13:31, 11.27s/it, val_loss=0.726]\u001b[A\n"," 68%|██████▊   | 152/224 [30:51\u003c13:31, 11.27s/it, val_loss=0.704]\u001b[A\n"," 68%|██████▊   | 153/224 [30:51\u003c13:56, 11.79s/it, val_loss=0.704]\u001b[A\n"," 68%|██████▊   | 153/224 [31:04\u003c13:56, 11.79s/it, val_loss=0.703]\u001b[A\n"," 69%|██████▉   | 154/224 [31:04\u003c13:56, 11.95s/it, val_loss=0.703]\u001b[A\n"," 69%|██████▉   | 154/224 [31:18\u003c13:56, 11.95s/it, val_loss=0.599]\u001b[A\n"," 69%|██████▉   | 155/224 [31:18\u003c14:25, 12.54s/it, val_loss=0.599]\u001b[A\n"," 69%|██████▉   | 155/224 [31:28\u003c14:25, 12.54s/it, val_loss=0.49] \u001b[A\n"," 70%|██████▉   | 156/224 [31:28\u003c13:20, 11.77s/it, val_loss=0.49]\u001b[A\n"," 70%|██████▉   | 156/224 [31:41\u003c13:20, 11.77s/it, val_loss=0.619]\u001b[A\n"," 70%|███████   | 157/224 [31:41\u003c13:41, 12.26s/it, val_loss=0.619]\u001b[A\n"," 70%|███████   | 157/224 [31:53\u003c13:41, 12.26s/it, val_loss=0.642]\u001b[A\n"," 71%|███████   | 158/224 [31:53\u003c13:24, 12.19s/it, val_loss=0.642]\u001b[A\n"," 71%|███████   | 158/224 [32:04\u003c13:24, 12.19s/it, val_loss=0.366]\u001b[A\n"," 71%|███████   | 159/224 [32:04\u003c12:54, 11.91s/it, val_loss=0.366]\u001b[A\n"," 71%|███████   | 159/224 [32:17\u003c12:54, 11.91s/it, val_loss=0.714]\u001b[A\n"," 71%|███████▏  | 160/224 [32:17\u003c13:05, 12.28s/it, val_loss=0.714]\u001b[A\n"," 71%|███████▏  | 160/224 [32:28\u003c13:05, 12.28s/it, val_loss=0.622]\u001b[A\n"," 72%|███████▏  | 161/224 [32:28\u003c12:19, 11.74s/it, val_loss=0.622]\u001b[A\n"," 72%|███████▏  | 161/224 [32:42\u003c12:19, 11.74s/it, val_loss=0.637]\u001b[A\n"," 72%|███████▏  | 162/224 [32:42\u003c12:44, 12.33s/it, val_loss=0.637]\u001b[A\n"," 72%|███████▏  | 162/224 [32:54\u003c12:44, 12.33s/it, val_loss=0.53] \u001b[A\n"," 73%|███████▎  | 163/224 [32:54\u003c12:39, 12.45s/it, val_loss=0.53]\u001b[A\n"," 73%|███████▎  | 163/224 [33:04\u003c12:39, 12.45s/it, val_loss=0.506]\u001b[A\n"," 73%|███████▎  | 164/224 [33:04\u003c11:40, 11.68s/it, val_loss=0.506]\u001b[A\n"," 73%|███████▎  | 164/224 [33:15\u003c11:40, 11.68s/it, val_loss=0.799]\u001b[A\n"," 74%|███████▎  | 165/224 [33:15\u003c11:15, 11.46s/it, val_loss=0.799]\u001b[A\n"," 74%|███████▎  | 165/224 [33:29\u003c11:15, 11.46s/it, val_loss=0.795]\u001b[A\n"," 74%|███████▍  | 166/224 [33:29\u003c11:52, 12.28s/it, val_loss=0.795]\u001b[A\n"," 74%|███████▍  | 166/224 [33:46\u003c11:52, 12.28s/it, val_loss=0.4]  \u001b[A\n"," 75%|███████▍  | 167/224 [33:46\u003c12:48, 13.49s/it, val_loss=0.4]\u001b[A\n"," 75%|███████▍  | 167/224 [33:57\u003c12:48, 13.49s/it, val_loss=0.595]\u001b[A\n"," 75%|███████▌  | 168/224 [33:57\u003c12:04, 12.94s/it, val_loss=0.595]\u001b[A\n"," 75%|███████▌  | 168/224 [34:11\u003c12:04, 12.94s/it, val_loss=0.49] \u001b[A\n"," 75%|███████▌  | 169/224 [34:11\u003c12:01, 13.12s/it, val_loss=0.49]\u001b[A\n"," 75%|███████▌  | 169/224 [34:21\u003c12:01, 13.12s/it, val_loss=0.558]\u001b[A\n"," 76%|███████▌  | 170/224 [34:21\u003c10:53, 12.11s/it, val_loss=0.558]\u001b[A\n"," 76%|███████▌  | 170/224 [34:34\u003c10:53, 12.11s/it, val_loss=0.625]\u001b[A\n"," 76%|███████▋  | 171/224 [34:34\u003c11:00, 12.46s/it, val_loss=0.625]\u001b[A\n"," 76%|███████▋  | 171/224 [34:49\u003c11:00, 12.46s/it, val_loss=0.57] \u001b[A\n"," 77%|███████▋  | 172/224 [34:49\u003c11:21, 13.10s/it, val_loss=0.57]\u001b[A\n"," 77%|███████▋  | 172/224 [35:00\u003c11:21, 13.10s/it, val_loss=0.697]\u001b[A\n"," 77%|███████▋  | 173/224 [35:00\u003c10:39, 12.54s/it, val_loss=0.697]\u001b[A\n"," 77%|███████▋  | 173/224 [35:10\u003c10:39, 12.54s/it, val_loss=0.588]\u001b[A\n"," 78%|███████▊  | 174/224 [35:10\u003c09:50, 11.81s/it, val_loss=0.588]\u001b[A\n"," 78%|███████▊  | 174/224 [35:23\u003c09:50, 11.81s/it, val_loss=0.57] \u001b[A\n"," 78%|███████▊  | 175/224 [35:23\u003c09:52, 12.10s/it, val_loss=0.57]\u001b[A\n"," 78%|███████▊  | 175/224 [35:31\u003c09:52, 12.10s/it, val_loss=0.69]\u001b[A\n"," 79%|███████▊  | 176/224 [35:31\u003c08:50, 11.05s/it, val_loss=0.69]\u001b[A\n"," 79%|███████▊  | 176/224 [35:40\u003c08:50, 11.05s/it, val_loss=0.536]\u001b[A\n"," 79%|███████▉  | 177/224 [35:40\u003c08:05, 10.33s/it, val_loss=0.536]\u001b[A\n"," 79%|███████▉  | 177/224 [35:51\u003c08:05, 10.33s/it, val_loss=0.576]\u001b[A\n"," 79%|███████▉  | 178/224 [35:51\u003c08:07, 10.60s/it, val_loss=0.576]\u001b[A\n"," 79%|███████▉  | 178/224 [36:09\u003c08:07, 10.60s/it, val_loss=0.597]\u001b[A\n"," 80%|███████▉  | 179/224 [36:09\u003c09:34, 12.78s/it, val_loss=0.597]\u001b[A\n"," 80%|███████▉  | 179/224 [36:22\u003c09:34, 12.78s/it, val_loss=0.794]\u001b[A\n"," 80%|████████  | 180/224 [36:22\u003c09:26, 12.87s/it, val_loss=0.794]\u001b[A\n"," 80%|████████  | 180/224 [36:37\u003c09:26, 12.87s/it, val_loss=0.512]\u001b[A\n"," 81%|████████  | 181/224 [36:37\u003c09:38, 13.45s/it, val_loss=0.512]\u001b[A\n"," 81%|████████  | 181/224 [36:47\u003c09:38, 13.45s/it, val_loss=0.353]\u001b[A\n"," 81%|████████▏ | 182/224 [36:47\u003c08:48, 12.59s/it, val_loss=0.353]\u001b[A\n"," 81%|████████▏ | 182/224 [37:01\u003c08:48, 12.59s/it, val_loss=0.972]\u001b[A\n"," 82%|████████▏ | 183/224 [37:01\u003c08:43, 12.76s/it, val_loss=0.972]\u001b[A\n"," 82%|████████▏ | 183/224 [37:13\u003c08:43, 12.76s/it, val_loss=0.407]\u001b[A\n"," 82%|████████▏ | 184/224 [37:13\u003c08:24, 12.60s/it, val_loss=0.407]\u001b[A\n"," 82%|████████▏ | 184/224 [37:28\u003c08:24, 12.60s/it, val_loss=0.971]\u001b[A\n"," 83%|████████▎ | 185/224 [37:28\u003c08:38, 13.29s/it, val_loss=0.971]\u001b[A\n"," 83%|████████▎ | 185/224 [37:39\u003c08:38, 13.29s/it, val_loss=0.839]\u001b[A\n"," 83%|████████▎ | 186/224 [37:39\u003c08:07, 12.82s/it, val_loss=0.839]\u001b[A\n"," 83%|████████▎ | 186/224 [37:50\u003c08:07, 12.82s/it, val_loss=0.624]\u001b[A\n"," 83%|████████▎ | 187/224 [37:50\u003c07:32, 12.24s/it, val_loss=0.624]\u001b[A\n"," 83%|████████▎ | 187/224 [38:02\u003c07:32, 12.24s/it, val_loss=0.723]\u001b[A\n"," 84%|████████▍ | 188/224 [38:02\u003c07:15, 12.09s/it, val_loss=0.723]\u001b[A\n"," 84%|████████▍ | 188/224 [38:17\u003c07:15, 12.09s/it, val_loss=0.706]\u001b[A\n"," 84%|████████▍ | 189/224 [38:17\u003c07:33, 12.96s/it, val_loss=0.706]\u001b[A\n"," 84%|████████▍ | 189/224 [38:26\u003c07:33, 12.96s/it, val_loss=0.646]\u001b[A\n"," 85%|████████▍ | 190/224 [38:26\u003c06:42, 11.85s/it, val_loss=0.646]\u001b[A\n"," 85%|████████▍ | 190/224 [38:41\u003c06:42, 11.85s/it, val_loss=0.788]\u001b[A\n"," 85%|████████▌ | 191/224 [38:41\u003c07:00, 12.74s/it, val_loss=0.788]\u001b[A\n"," 85%|████████▌ | 191/224 [38:53\u003c07:00, 12.74s/it, val_loss=0.481]\u001b[A\n"," 86%|████████▌ | 192/224 [38:53\u003c06:38, 12.46s/it, val_loss=0.481]\u001b[A\n"," 86%|████████▌ | 192/224 [39:05\u003c06:38, 12.46s/it, val_loss=0.649]\u001b[A\n"," 86%|████████▌ | 193/224 [39:05\u003c06:25, 12.44s/it, val_loss=0.649]\u001b[A\n"," 86%|████████▌ | 193/224 [39:17\u003c06:25, 12.44s/it, val_loss=0.54] \u001b[A\n"," 87%|████████▋ | 194/224 [39:17\u003c06:03, 12.13s/it, val_loss=0.54]\u001b[A\n"," 87%|████████▋ | 194/224 [39:31\u003c06:03, 12.13s/it, val_loss=0.55]\u001b[A\n"," 87%|████████▋ | 195/224 [39:31\u003c06:09, 12.74s/it, val_loss=0.55]\u001b[A\n"," 87%|████████▋ | 195/224 [39:42\u003c06:09, 12.74s/it, val_loss=0.567]\u001b[A\n"," 88%|████████▊ | 196/224 [39:42\u003c05:42, 12.22s/it, val_loss=0.567]\u001b[A\n"," 88%|████████▊ | 196/224 [39:52\u003c05:42, 12.22s/it, val_loss=0.639]\u001b[A\n"," 88%|████████▊ | 197/224 [39:52\u003c05:12, 11.58s/it, val_loss=0.639]\u001b[A\n"," 88%|████████▊ | 197/224 [40:04\u003c05:12, 11.58s/it, val_loss=0.698]\u001b[A\n"," 88%|████████▊ | 198/224 [40:04\u003c05:02, 11.62s/it, val_loss=0.698]\u001b[A\n"," 88%|████████▊ | 198/224 [40:15\u003c05:02, 11.62s/it, val_loss=0.484]\u001b[A\n"," 89%|████████▉ | 199/224 [40:15\u003c04:47, 11.48s/it, val_loss=0.484]\u001b[A\n"," 89%|████████▉ | 199/224 [40:28\u003c04:47, 11.48s/it, val_loss=0.664]\u001b[A\n"," 89%|████████▉ | 200/224 [40:28\u003c04:49, 12.06s/it, val_loss=0.664]\u001b[A\n"," 89%|████████▉ | 200/224 [40:42\u003c04:49, 12.06s/it, val_loss=0.895]\u001b[A\n"," 90%|████████▉ | 201/224 [40:42\u003c04:49, 12.60s/it, val_loss=0.895]\u001b[A\n"," 90%|████████▉ | 201/224 [40:52\u003c04:49, 12.60s/it, val_loss=0.682]\u001b[A\n"," 90%|█████████ | 202/224 [40:52\u003c04:20, 11.85s/it, val_loss=0.682]\u001b[A\n"," 90%|█████████ | 202/224 [41:03\u003c04:20, 11.85s/it, val_loss=0.435]\u001b[A\n"," 91%|█████████ | 203/224 [41:03\u003c04:04, 11.63s/it, val_loss=0.435]\u001b[A\n"," 91%|█████████ | 203/224 [41:17\u003c04:04, 11.63s/it, val_loss=0.581]\u001b[A\n"," 91%|█████████ | 204/224 [41:17\u003c04:07, 12.35s/it, val_loss=0.581]\u001b[A\n"," 91%|█████████ | 204/224 [41:28\u003c04:07, 12.35s/it, val_loss=0.605]\u001b[A\n"," 92%|█████████▏| 205/224 [41:28\u003c03:47, 11.97s/it, val_loss=0.605]\u001b[A\n"," 92%|█████████▏| 205/224 [41:41\u003c03:47, 11.97s/it, val_loss=0.719]\u001b[A\n"," 92%|█████████▏| 206/224 [41:41\u003c03:37, 12.08s/it, val_loss=0.719]\u001b[A\n"," 92%|█████████▏| 206/224 [41:50\u003c03:37, 12.08s/it, val_loss=0.52] \u001b[A\n"," 92%|█████████▏| 207/224 [41:50\u003c03:12, 11.35s/it, val_loss=0.52]\u001b[A\n"," 92%|█████████▏| 207/224 [42:05\u003c03:12, 11.35s/it, val_loss=0.458]\u001b[A\n"," 93%|█████████▎| 208/224 [42:05\u003c03:16, 12.28s/it, val_loss=0.458]\u001b[A\n"," 93%|█████████▎| 208/224 [42:17\u003c03:16, 12.28s/it, val_loss=0.548]\u001b[A\n"," 93%|█████████▎| 209/224 [42:17\u003c03:02, 12.18s/it, val_loss=0.548]\u001b[A\n"," 93%|█████████▎| 209/224 [42:32\u003c03:02, 12.18s/it, val_loss=0.619]\u001b[A\n"," 94%|█████████▍| 210/224 [42:32\u003c03:03, 13.13s/it, val_loss=0.619]\u001b[A\n"," 94%|█████████▍| 210/224 [42:42\u003c03:03, 13.13s/it, val_loss=0.692]\u001b[A\n"," 94%|█████████▍| 211/224 [42:42\u003c02:38, 12.21s/it, val_loss=0.692]\u001b[A\n"," 94%|█████████▍| 211/224 [42:51\u003c02:38, 12.21s/it, val_loss=0.641]\u001b[A\n"," 95%|█████████▍| 212/224 [42:51\u003c02:14, 11.23s/it, val_loss=0.641]\u001b[A\n"," 95%|█████████▍| 212/224 [43:02\u003c02:14, 11.23s/it, val_loss=0.645]\u001b[A\n"," 95%|█████████▌| 213/224 [43:02\u003c02:03, 11.22s/it, val_loss=0.645]\u001b[A\n"," 95%|█████████▌| 213/224 [43:17\u003c02:03, 11.22s/it, val_loss=0.54] \u001b[A\n"," 96%|█████████▌| 214/224 [43:17\u003c02:01, 12.19s/it, val_loss=0.54]\u001b[A\n"," 96%|█████████▌| 214/224 [43:28\u003c02:01, 12.19s/it, val_loss=0.652]\u001b[A\n"," 96%|█████████▌| 215/224 [43:28\u003c01:45, 11.74s/it, val_loss=0.652]\u001b[A\n"," 96%|█████████▌| 215/224 [43:39\u003c01:45, 11.74s/it, val_loss=0.676]\u001b[A\n"," 96%|█████████▋| 216/224 [43:39\u003c01:33, 11.74s/it, val_loss=0.676]\u001b[A\n"," 96%|█████████▋| 216/224 [43:51\u003c01:33, 11.74s/it, val_loss=0.929]\u001b[A\n"," 97%|█████████▋| 217/224 [43:51\u003c01:21, 11.63s/it, val_loss=0.929]\u001b[A\n"," 97%|█████████▋| 217/224 [44:00\u003c01:21, 11.63s/it, val_loss=0.799]\u001b[A\n"," 97%|█████████▋| 218/224 [44:00\u003c01:05, 10.96s/it, val_loss=0.799]\u001b[A\n"," 97%|█████████▋| 218/224 [44:12\u003c01:05, 10.96s/it, val_loss=0.382]\u001b[A\n"," 98%|█████████▊| 219/224 [44:12\u003c00:55, 11.18s/it, val_loss=0.382]\u001b[A\n"," 98%|█████████▊| 219/224 [44:21\u003c00:55, 11.18s/it, val_loss=0.379]\u001b[A\n"," 98%|█████████▊| 220/224 [44:21\u003c00:42, 10.71s/it, val_loss=0.379]\u001b[A\n"," 98%|█████████▊| 220/224 [44:31\u003c00:42, 10.71s/it, val_loss=0.648]\u001b[A\n"," 99%|█████████▊| 221/224 [44:31\u003c00:30, 10.33s/it, val_loss=0.648]\u001b[A\n"," 99%|█████████▊| 221/224 [44:42\u003c00:30, 10.33s/it, val_loss=0.63] \u001b[A\n"," 99%|█████████▉| 222/224 [44:42\u003c00:21, 10.63s/it, val_loss=0.63]\u001b[A\n"," 99%|█████████▉| 222/224 [44:50\u003c00:21, 10.63s/it, val_loss=1.04]\u001b[A\n","100%|█████████▉| 223/224 [44:50\u003c00:09,  9.83s/it, val_loss=1.04]\u001b[A\n","100%|█████████▉| 223/224 [44:59\u003c00:09,  9.83s/it, val_loss=0.351]\u001b[A\n","100%|██████████| 224/224 [44:59\u003c00:00, 12.05s/it, val_loss=0.351]\n"]},{"data":{"text/plain":["{'val_loss': 0.6259005373458366}"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1026/1026 [5:20:17\u003c00:00, 18.73s/it, loss=0.677, lr=4.03e-6]\n"]},{"data":{"text/plain":["{'train_loss': 0.6360619584500051}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[" ==========end epoch ==========\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 224/224 [44:52\u003c00:00, 12.02s/it, val_loss=0.372]\n"]},{"data":{"text/plain":["{'val_loss': 0.6189149831244454}"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["save model weight\n","# ============ start epoch:2 ============== #\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|████▊     | 498/1026 [1:50:28\u003c1:56:20, 13.22s/it, loss=0.707, lr=2.56e-6]"]},{"name":"stdout","output_type":"stream","text":["===========steps： 498 ==========\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/224 [00:00\u003c?, ?it/s]\u001b[A\n","  0%|          | 0/224 [00:10\u003c?, ?it/s, val_loss=0.483]\u001b[A\n","  0%|          | 1/224 [00:10\u003c39:27, 10.62s/it, val_loss=0.483]\u001b[A\n","  0%|          | 1/224 [00:21\u003c39:27, 10.62s/it, val_loss=0.634]\u001b[A\n","  1%|          | 2/224 [00:21\u003c39:01, 10.55s/it, val_loss=0.634]\u001b[A\n","  1%|          | 2/224 [00:34\u003c39:01, 10.55s/it, val_loss=0.532]\u001b[A\n","  1%|▏         | 3/224 [00:34\u003c43:19, 11.76s/it, val_loss=0.532]\u001b[A\n","  1%|▏         | 3/224 [00:46\u003c43:19, 11.76s/it, val_loss=0.601]\u001b[A\n","  2%|▏         | 4/224 [00:46\u003c43:00, 11.73s/it, val_loss=0.601]\u001b[A\n","  2%|▏         | 4/224 [00:59\u003c43:00, 11.73s/it, val_loss=0.522]\u001b[A\n","  2%|▏         | 5/224 [00:59\u003c45:41, 12.52s/it, val_loss=0.522]\u001b[A\n","  2%|▏         | 5/224 [01:14\u003c45:41, 12.52s/it, val_loss=0.665]\u001b[A\n","  3%|▎         | 6/224 [01:14\u003c48:27, 13.34s/it, val_loss=0.665]\u001b[A\n","  3%|▎         | 6/224 [01:25\u003c48:27, 13.34s/it, val_loss=0.856]\u001b[A\n","  3%|▎         | 7/224 [01:25\u003c44:38, 12.34s/it, val_loss=0.856]\u001b[A\n","  3%|▎         | 7/224 [01:39\u003c44:38, 12.34s/it, val_loss=0.535]\u001b[A\n","  4%|▎         | 8/224 [01:39\u003c46:43, 12.98s/it, val_loss=0.535]\u001b[A\n","  4%|▎         | 8/224 [01:45\u003c46:43, 12.98s/it, val_loss=0.211]\u001b[A\n","  4%|▍         | 9/224 [01:45\u003c39:10, 10.93s/it, val_loss=0.211]\u001b[A\n","  4%|▍         | 9/224 [02:00\u003c39:10, 10.93s/it, val_loss=0.8]  \u001b[A\n","  4%|▍         | 10/224 [02:00\u003c43:06, 12.09s/it, val_loss=0.8]\u001b[A\n","  4%|▍         | 10/224 [02:11\u003c43:06, 12.09s/it, val_loss=0.746]\u001b[A\n","  5%|▍         | 11/224 [02:11\u003c41:14, 11.62s/it, val_loss=0.746]\u001b[A\n","  5%|▍         | 11/224 [02:23\u003c41:14, 11.62s/it, val_loss=0.58] \u001b[A\n","  5%|▌         | 12/224 [02:23\u003c41:51, 11.84s/it, val_loss=0.58]\u001b[A\n","  5%|▌         | 12/224 [02:33\u003c41:51, 11.84s/it, val_loss=0.475]\u001b[A\n","  6%|▌         | 13/224 [02:33\u003c39:18, 11.18s/it, val_loss=0.475]\u001b[A\n","  6%|▌         | 13/224 [02:45\u003c39:18, 11.18s/it, val_loss=0.543]\u001b[A\n","  6%|▋         | 14/224 [02:45\u003c39:55, 11.41s/it, val_loss=0.543]\u001b[A\n","  6%|▋         | 14/224 [02:53\u003c39:55, 11.41s/it, val_loss=0.587]\u001b[A\n","  7%|▋         | 15/224 [02:53\u003c36:26, 10.46s/it, val_loss=0.587]\u001b[A\n","  7%|▋         | 15/224 [03:05\u003c36:26, 10.46s/it, val_loss=0.643]\u001b[A\n","  7%|▋         | 16/224 [03:05\u003c38:30, 11.11s/it, val_loss=0.643]\u001b[A\n","  7%|▋         | 16/224 [03:21\u003c38:30, 11.11s/it, val_loss=0.532]\u001b[A\n","  8%|▊         | 17/224 [03:21\u003c42:36, 12.35s/it, val_loss=0.532]\u001b[A\n","  8%|▊         | 17/224 [03:29\u003c42:36, 12.35s/it, val_loss=0.634]\u001b[A\n","  8%|▊         | 18/224 [03:29\u003c38:18, 11.16s/it, val_loss=0.634]\u001b[A\n","  8%|▊         | 18/224 [03:42\u003c38:18, 11.16s/it, val_loss=0.833]\u001b[A\n","  8%|▊         | 19/224 [03:42\u003c39:42, 11.62s/it, val_loss=0.833]\u001b[A\n","  8%|▊         | 19/224 [03:55\u003c39:42, 11.62s/it, val_loss=0.584]\u001b[A\n","  9%|▉         | 20/224 [03:55\u003c41:29, 12.20s/it, val_loss=0.584]\u001b[A\n","  9%|▉         | 20/224 [04:07\u003c41:29, 12.20s/it, val_loss=0.382]\u001b[A\n","  9%|▉         | 21/224 [04:07\u003c40:32, 11.98s/it, val_loss=0.382]\u001b[A\n","  9%|▉         | 21/224 [04:20\u003c40:32, 11.98s/it, val_loss=0.637]\u001b[A\n"," 10%|▉         | 22/224 [04:20\u003c41:06, 12.21s/it, val_loss=0.637]\u001b[A\n"," 10%|▉         | 22/224 [04:29\u003c41:06, 12.21s/it, val_loss=0.346]\u001b[A\n"," 10%|█         | 23/224 [04:29\u003c38:00, 11.35s/it, val_loss=0.346]\u001b[A\n"," 10%|█         | 23/224 [04:42\u003c38:00, 11.35s/it, val_loss=0.905]\u001b[A\n"," 11%|█         | 24/224 [04:42\u003c39:16, 11.78s/it, val_loss=0.905]\u001b[A\n"," 11%|█         | 24/224 [04:57\u003c39:16, 11.78s/it, val_loss=0.907]\u001b[A\n"," 11%|█         | 25/224 [04:57\u003c42:59, 12.96s/it, val_loss=0.907]\u001b[A\n"," 11%|█         | 25/224 [05:05\u003c42:59, 12.96s/it, val_loss=0.574]\u001b[A\n"," 12%|█▏        | 26/224 [05:05\u003c37:37, 11.40s/it, val_loss=0.574]\u001b[A\n"," 12%|█▏        | 26/224 [05:15\u003c37:37, 11.40s/it, val_loss=0.625]\u001b[A\n"," 12%|█▏        | 27/224 [05:15\u003c35:35, 10.84s/it, val_loss=0.625]\u001b[A\n"," 12%|█▏        | 27/224 [05:27\u003c35:35, 10.84s/it, val_loss=0.57] \u001b[A\n"," 12%|█▎        | 28/224 [05:27\u003c37:04, 11.35s/it, val_loss=0.57]\u001b[A\n"," 12%|█▎        | 28/224 [05:40\u003c37:04, 11.35s/it, val_loss=0.882]\u001b[A\n"," 13%|█▎        | 29/224 [05:40\u003c37:59, 11.69s/it, val_loss=0.882]\u001b[A\n"," 13%|█▎        | 29/224 [05:56\u003c37:59, 11.69s/it, val_loss=0.498]\u001b[A\n"," 13%|█▎        | 30/224 [05:56\u003c41:51, 12.94s/it, val_loss=0.498]\u001b[A\n"," 13%|█▎        | 30/224 [06:08\u003c41:51, 12.94s/it, val_loss=0.767]\u001b[A\n"," 14%|█▍        | 31/224 [06:08\u003c41:14, 12.82s/it, val_loss=0.767]\u001b[A\n"," 14%|█▍        | 31/224 [06:19\u003c41:14, 12.82s/it, val_loss=0.977]\u001b[A\n"," 14%|█▍        | 32/224 [06:19\u003c39:00, 12.19s/it, val_loss=0.977]\u001b[A\n"," 14%|█▍        | 32/224 [06:30\u003c39:00, 12.19s/it, val_loss=0.463]\u001b[A\n"," 15%|█▍        | 33/224 [06:30\u003c37:24, 11.75s/it, val_loss=0.463]\u001b[A\n"," 15%|█▍        | 33/224 [06:39\u003c37:24, 11.75s/it, val_loss=0.655]\u001b[A\n"," 15%|█▌        | 34/224 [06:39\u003c35:19, 11.16s/it, val_loss=0.655]\u001b[A\n"," 15%|█▌        | 34/224 [06:52\u003c35:19, 11.16s/it, val_loss=0.625]\u001b[A\n"," 16%|█▌        | 35/224 [06:52\u003c36:31, 11.60s/it, val_loss=0.625]\u001b[A\n"," 16%|█▌        | 35/224 [07:04\u003c36:31, 11.60s/it, val_loss=0.681]\u001b[A\n"," 16%|█▌        | 36/224 [07:04\u003c37:10, 11.87s/it, val_loss=0.681]\u001b[A\n"," 16%|█▌        | 36/224 [07:12\u003c37:10, 11.87s/it, val_loss=0.581]\u001b[A\n"," 17%|█▋        | 37/224 [07:12\u003c33:13, 10.66s/it, val_loss=0.581]\u001b[A\n"," 17%|█▋        | 37/224 [07:25\u003c33:13, 10.66s/it, val_loss=0.764]\u001b[A\n"," 17%|█▋        | 38/224 [07:25\u003c35:17, 11.39s/it, val_loss=0.764]\u001b[A\n"," 17%|█▋        | 38/224 [07:40\u003c35:17, 11.39s/it, val_loss=0.36] \u001b[A\n"," 17%|█▋        | 39/224 [07:40\u003c37:50, 12.27s/it, val_loss=0.36]\u001b[A\n"," 17%|█▋        | 39/224 [07:55\u003c37:50, 12.27s/it, val_loss=0.451]\u001b[A\n"," 18%|█▊        | 40/224 [07:55\u003c40:18, 13.15s/it, val_loss=0.451]\u001b[A\n"," 18%|█▊        | 40/224 [08:08\u003c40:18, 13.15s/it, val_loss=1.11] \u001b[A\n"," 18%|█▊        | 41/224 [08:08\u003c39:35, 12.98s/it, val_loss=1.11]\u001b[A\n"," 18%|█▊        | 41/224 [08:19\u003c39:35, 12.98s/it, val_loss=0.548]\u001b[A\n"," 19%|█▉        | 42/224 [08:19\u003c38:19, 12.64s/it, val_loss=0.548]\u001b[A\n"," 19%|█▉        | 42/224 [08:32\u003c38:19, 12.64s/it, val_loss=0.543]\u001b[A\n"," 19%|█▉        | 43/224 [08:32\u003c37:44, 12.51s/it, val_loss=0.543]\u001b[A\n"," 19%|█▉        | 43/224 [08:44\u003c37:44, 12.51s/it, val_loss=1.17] \u001b[A\n"," 20%|█▉        | 44/224 [08:44\u003c37:31, 12.51s/it, val_loss=1.17]\u001b[A\n"," 20%|█▉        | 44/224 [08:57\u003c37:31, 12.51s/it, val_loss=0.561]\u001b[A\n"," 20%|██        | 45/224 [08:57\u003c38:07, 12.78s/it, val_loss=0.561]\u001b[A\n"," 20%|██        | 45/224 [09:08\u003c38:07, 12.78s/it, val_loss=0.646]\u001b[A\n"," 21%|██        | 46/224 [09:08\u003c35:50, 12.08s/it, val_loss=0.646]\u001b[A\n"," 21%|██        | 46/224 [09:22\u003c35:50, 12.08s/it, val_loss=0.565]\u001b[A\n"," 21%|██        | 47/224 [09:22\u003c37:07, 12.58s/it, val_loss=0.565]\u001b[A\n"," 21%|██        | 47/224 [09:37\u003c37:07, 12.58s/it, val_loss=0.451]\u001b[A\n"," 21%|██▏       | 48/224 [09:37\u003c39:39, 13.52s/it, val_loss=0.451]\u001b[A\n"," 21%|██▏       | 48/224 [09:50\u003c39:39, 13.52s/it, val_loss=0.935]\u001b[A\n"," 22%|██▏       | 49/224 [09:50\u003c38:24, 13.17s/it, val_loss=0.935]\u001b[A\n"," 22%|██▏       | 49/224 [10:00\u003c38:24, 13.17s/it, val_loss=0.6]  \u001b[A\n"," 22%|██▏       | 50/224 [10:01\u003c36:06, 12.45s/it, val_loss=0.6]\u001b[A\n"," 22%|██▏       | 50/224 [10:12\u003c36:06, 12.45s/it, val_loss=0.615]\u001b[A\n"," 23%|██▎       | 51/224 [10:12\u003c35:14, 12.22s/it, val_loss=0.615]\u001b[A\n"," 23%|██▎       | 51/224 [10:27\u003c35:14, 12.22s/it, val_loss=0.833]\u001b[A\n"," 23%|██▎       | 52/224 [10:27\u003c37:31, 13.09s/it, val_loss=0.833]\u001b[A\n"," 23%|██▎       | 52/224 [10:38\u003c37:31, 13.09s/it, val_loss=1.23] \u001b[A\n"," 24%|██▎       | 53/224 [10:38\u003c35:30, 12.46s/it, val_loss=1.23]\u001b[A\n"," 24%|██▎       | 53/224 [10:51\u003c35:30, 12.46s/it, val_loss=0.314]\u001b[A\n"," 24%|██▍       | 54/224 [10:51\u003c35:15, 12.44s/it, val_loss=0.314]\u001b[A\n"," 24%|██▍       | 54/224 [11:03\u003c35:15, 12.44s/it, val_loss=0.568]\u001b[A\n"," 25%|██▍       | 55/224 [11:03\u003c34:41, 12.32s/it, val_loss=0.568]\u001b[A\n"," 25%|██▍       | 55/224 [11:15\u003c34:41, 12.32s/it, val_loss=0.818]\u001b[A\n"," 25%|██▌       | 56/224 [11:15\u003c34:43, 12.40s/it, val_loss=0.818]\u001b[A\n"," 25%|██▌       | 56/224 [11:27\u003c34:43, 12.40s/it, val_loss=0.647]\u001b[A\n"," 25%|██▌       | 57/224 [11:27\u003c33:52, 12.17s/it, val_loss=0.647]\u001b[A\n"," 25%|██▌       | 57/224 [11:38\u003c33:52, 12.17s/it, val_loss=0.401]\u001b[A\n"," 26%|██▌       | 58/224 [11:38\u003c32:34, 11.78s/it, val_loss=0.401]\u001b[A\n"," 26%|██▌       | 58/224 [11:50\u003c32:34, 11.78s/it, val_loss=0.515]\u001b[A\n"," 26%|██▋       | 59/224 [11:50\u003c32:58, 11.99s/it, val_loss=0.515]\u001b[A\n"," 26%|██▋       | 59/224 [12:04\u003c32:58, 11.99s/it, val_loss=0.603]\u001b[A\n"," 27%|██▋       | 60/224 [12:04\u003c33:47, 12.36s/it, val_loss=0.603]\u001b[A\n"," 27%|██▋       | 60/224 [12:16\u003c33:47, 12.36s/it, val_loss=0.555]\u001b[A\n"," 27%|██▋       | 61/224 [12:16\u003c33:53, 12.48s/it, val_loss=0.555]\u001b[A\n"," 27%|██▋       | 61/224 [12:29\u003c33:53, 12.48s/it, val_loss=0.659]\u001b[A\n"," 28%|██▊       | 62/224 [12:29\u003c33:39, 12.47s/it, val_loss=0.659]\u001b[A\n"," 28%|██▊       | 62/224 [12:41\u003c33:39, 12.47s/it, val_loss=0.437]\u001b[A\n"," 28%|██▊       | 63/224 [12:41\u003c32:58, 12.29s/it, val_loss=0.437]\u001b[A\n"," 28%|██▊       | 63/224 [12:50\u003c32:58, 12.29s/it, val_loss=0.502]\u001b[A\n"," 29%|██▊       | 64/224 [12:50\u003c30:43, 11.52s/it, val_loss=0.502]\u001b[A\n"," 29%|██▊       | 64/224 [13:07\u003c30:43, 11.52s/it, val_loss=0.47] \u001b[A\n"," 29%|██▉       | 65/224 [13:07\u003c34:19, 12.95s/it, val_loss=0.47]\u001b[A\n"," 29%|██▉       | 65/224 [13:17\u003c34:19, 12.95s/it, val_loss=1.24]\u001b[A\n"," 29%|██▉       | 66/224 [13:17\u003c32:24, 12.31s/it, val_loss=1.24]\u001b[A\n"," 29%|██▉       | 66/224 [13:30\u003c32:24, 12.31s/it, val_loss=0.736]\u001b[A\n"," 30%|██▉       | 67/224 [13:30\u003c32:20, 12.36s/it, val_loss=0.736]\u001b[A\n"," 30%|██▉       | 67/224 [13:42\u003c32:20, 12.36s/it, val_loss=0.554]\u001b[A\n"," 30%|███       | 68/224 [13:42\u003c31:47, 12.23s/it, val_loss=0.554]\u001b[A\n"," 30%|███       | 68/224 [13:50\u003c31:47, 12.23s/it, val_loss=0.407]\u001b[A\n"," 31%|███       | 69/224 [13:50\u003c28:42, 11.11s/it, val_loss=0.407]\u001b[A\n"," 31%|███       | 69/224 [14:01\u003c28:42, 11.11s/it, val_loss=0.43] \u001b[A\n"," 31%|███▏      | 70/224 [14:01\u003c28:25, 11.07s/it, val_loss=0.43]\u001b[A\n"," 31%|███▏      | 70/224 [14:12\u003c28:25, 11.07s/it, val_loss=1.01]\u001b[A\n"," 32%|███▏      | 71/224 [14:12\u003c27:39, 10.85s/it, val_loss=1.01]\u001b[A\n"," 32%|███▏      | 71/224 [14:23\u003c27:39, 10.85s/it, val_loss=0.65]\u001b[A\n"," 32%|███▏      | 72/224 [14:23\u003c28:09, 11.12s/it, val_loss=0.65]\u001b[A\n"," 32%|███▏      | 72/224 [14:33\u003c28:09, 11.12s/it, val_loss=0.651]\u001b[A\n"," 33%|███▎      | 73/224 [14:33\u003c27:11, 10.80s/it, val_loss=0.651]\u001b[A\n"," 33%|███▎      | 73/224 [14:44\u003c27:11, 10.80s/it, val_loss=0.627]\u001b[A\n"," 33%|███▎      | 74/224 [14:44\u003c27:00, 10.80s/it, val_loss=0.627]\u001b[A\n"," 33%|███▎      | 74/224 [14:54\u003c27:00, 10.80s/it, val_loss=0.543]\u001b[A\n"," 33%|███▎      | 75/224 [14:54\u003c25:53, 10.43s/it, val_loss=0.543]\u001b[A\n"," 33%|███▎      | 75/224 [15:08\u003c25:53, 10.43s/it, val_loss=0.716]\u001b[A\n"," 34%|███▍      | 76/224 [15:08\u003c28:34, 11.59s/it, val_loss=0.716]\u001b[A\n"," 34%|███▍      | 76/224 [15:18\u003c28:34, 11.59s/it, val_loss=0.63] \u001b[A\n"," 34%|███▍      | 77/224 [15:18\u003c27:05, 11.05s/it, val_loss=0.63]\u001b[A\n"," 34%|███▍      | 77/224 [15:29\u003c27:05, 11.05s/it, val_loss=0.38]\u001b[A\n"," 35%|███▍      | 78/224 [15:29\u003c27:13, 11.19s/it, val_loss=0.38]\u001b[A\n"," 35%|███▍      | 78/224 [15:39\u003c27:13, 11.19s/it, val_loss=0.675]\u001b[A\n"," 35%|███▌      | 79/224 [15:39\u003c25:44, 10.65s/it, val_loss=0.675]\u001b[A\n"," 35%|███▌      | 79/224 [15:53\u003c25:44, 10.65s/it, val_loss=0.359]\u001b[A\n"," 36%|███▌      | 80/224 [15:53\u003c27:54, 11.63s/it, val_loss=0.359]\u001b[A\n"," 36%|███▌      | 80/224 [16:03\u003c27:54, 11.63s/it, val_loss=0.71] \u001b[A\n"," 36%|███▌      | 81/224 [16:03\u003c26:44, 11.22s/it, val_loss=0.71]\u001b[A\n"," 36%|███▌      | 81/224 [16:16\u003c26:44, 11.22s/it, val_loss=0.77]\u001b[A\n"," 37%|███▋      | 82/224 [16:16\u003c27:34, 11.65s/it, val_loss=0.77]\u001b[A\n"," 37%|███▋      | 82/224 [16:30\u003c27:34, 11.65s/it, val_loss=0.629]\u001b[A\n"," 37%|███▋      | 83/224 [16:30\u003c29:21, 12.49s/it, val_loss=0.629]\u001b[A\n"," 37%|███▋      | 83/224 [16:43\u003c29:21, 12.49s/it, val_loss=0.384]\u001b[A\n"," 38%|███▊      | 84/224 [16:43\u003c29:41, 12.73s/it, val_loss=0.384]\u001b[A\n"," 38%|███▊      | 84/224 [16:58\u003c29:41, 12.73s/it, val_loss=0.426]\u001b[A\n"," 38%|███▊      | 85/224 [16:58\u003c30:52, 13.32s/it, val_loss=0.426]\u001b[A\n"," 38%|███▊      | 85/224 [17:09\u003c30:52, 13.32s/it, val_loss=0.833]\u001b[A\n"," 38%|███▊      | 86/224 [17:09\u003c29:09, 12.68s/it, val_loss=0.833]\u001b[A\n"," 38%|███▊      | 86/224 [17:21\u003c29:09, 12.68s/it, val_loss=0.653]\u001b[A\n"," 39%|███▉      | 87/224 [17:21\u003c28:32, 12.50s/it, val_loss=0.653]\u001b[A\n"," 39%|███▉      | 87/224 [17:34\u003c28:32, 12.50s/it, val_loss=0.368]\u001b[A\n"," 39%|███▉      | 88/224 [17:34\u003c28:35, 12.62s/it, val_loss=0.368]\u001b[A\n"," 39%|███▉      | 88/224 [17:47\u003c28:35, 12.62s/it, val_loss=0.411]\u001b[A\n"," 40%|███▉      | 89/224 [17:47\u003c28:13, 12.54s/it, val_loss=0.411]\u001b[A\n"," 40%|███▉      | 89/224 [18:01\u003c28:13, 12.54s/it, val_loss=0.496]\u001b[A\n"," 40%|████      | 90/224 [18:01\u003c29:32, 13.23s/it, val_loss=0.496]\u001b[A\n"," 40%|████      | 90/224 [18:13\u003c29:32, 13.23s/it, val_loss=0.516]\u001b[A\n"," 41%|████      | 91/224 [18:13\u003c28:05, 12.67s/it, val_loss=0.516]\u001b[A\n"," 41%|████      | 91/224 [18:25\u003c28:05, 12.67s/it, val_loss=0.558]\u001b[A\n"," 41%|████      | 92/224 [18:25\u003c27:28, 12.49s/it, val_loss=0.558]\u001b[A\n"," 41%|████      | 92/224 [18:38\u003c27:28, 12.49s/it, val_loss=0.475]\u001b[A\n"," 42%|████▏     | 93/224 [18:38\u003c27:58, 12.81s/it, val_loss=0.475]\u001b[A\n"," 42%|████▏     | 93/224 [18:48\u003c27:58, 12.81s/it, val_loss=0.576]\u001b[A\n"," 42%|████▏     | 94/224 [18:48\u003c25:56, 11.97s/it, val_loss=0.576]\u001b[A\n"," 42%|████▏     | 94/224 [19:01\u003c25:56, 11.97s/it, val_loss=0.701]\u001b[A\n"," 42%|████▏     | 95/224 [19:01\u003c26:21, 12.26s/it, val_loss=0.701]\u001b[A\n"," 42%|████▏     | 95/224 [19:17\u003c26:21, 12.26s/it, val_loss=0.677]\u001b[A\n"," 43%|████▎     | 96/224 [19:17\u003c28:20, 13.29s/it, val_loss=0.677]\u001b[A\n"," 43%|████▎     | 96/224 [19:28\u003c28:20, 13.29s/it, val_loss=0.329]\u001b[A\n"," 43%|████▎     | 97/224 [19:28\u003c26:56, 12.73s/it, val_loss=0.329]\u001b[A\n"," 43%|████▎     | 97/224 [19:40\u003c26:56, 12.73s/it, val_loss=0.499]\u001b[A\n"," 44%|████▍     | 98/224 [19:40\u003c26:09, 12.46s/it, val_loss=0.499]\u001b[A\n"," 44%|████▍     | 98/224 [19:49\u003c26:09, 12.46s/it, val_loss=0.577]\u001b[A\n"," 44%|████▍     | 99/224 [19:49\u003c23:45, 11.40s/it, val_loss=0.577]\u001b[A\n"," 44%|████▍     | 99/224 [20:01\u003c23:45, 11.40s/it, val_loss=0.463]\u001b[A\n"," 45%|████▍     | 100/224 [20:01\u003c23:56, 11.58s/it, val_loss=0.463]\u001b[A\n"," 45%|████▍     | 100/224 [20:12\u003c23:56, 11.58s/it, val_loss=0.544]\u001b[A\n"," 45%|████▌     | 101/224 [20:12\u003c22:59, 11.22s/it, val_loss=0.544]\u001b[A\n"," 45%|████▌     | 101/224 [20:25\u003c22:59, 11.22s/it, val_loss=0.749]\u001b[A\n"," 46%|████▌     | 102/224 [20:25\u003c24:12, 11.90s/it, val_loss=0.749]\u001b[A\n"," 46%|████▌     | 102/224 [20:35\u003c24:12, 11.90s/it, val_loss=0.531]\u001b[A\n"," 46%|████▌     | 103/224 [20:35\u003c22:46, 11.29s/it, val_loss=0.531]\u001b[A\n"," 46%|████▌     | 103/224 [20:44\u003c22:46, 11.29s/it, val_loss=0.664]\u001b[A\n"," 46%|████▋     | 104/224 [20:44\u003c20:55, 10.46s/it, val_loss=0.664]\u001b[A\n"," 46%|████▋     | 104/224 [20:54\u003c20:55, 10.46s/it, val_loss=0.65] \u001b[A\n"," 47%|████▋     | 105/224 [20:54\u003c20:57, 10.56s/it, val_loss=0.65]\u001b[A\n"," 47%|████▋     | 105/224 [21:08\u003c20:57, 10.56s/it, val_loss=0.488]\u001b[A\n"," 47%|████▋     | 106/224 [21:08\u003c22:45, 11.57s/it, val_loss=0.488]\u001b[A\n"," 47%|████▋     | 106/224 [21:18\u003c22:45, 11.57s/it, val_loss=0.63] \u001b[A\n"," 48%|████▊     | 107/224 [21:18\u003c21:44, 11.15s/it, val_loss=0.63]\u001b[A\n"," 48%|████▊     | 107/224 [21:29\u003c21:44, 11.15s/it, val_loss=0.577]\u001b[A\n"," 48%|████▊     | 108/224 [21:29\u003c21:18, 11.02s/it, val_loss=0.577]\u001b[A\n"," 48%|████▊     | 108/224 [21:46\u003c21:18, 11.02s/it, val_loss=0.709]\u001b[A\n"," 49%|████▊     | 109/224 [21:46\u003c24:33, 12.81s/it, val_loss=0.709]\u001b[A\n"," 49%|████▊     | 109/224 [21:57\u003c24:33, 12.81s/it, val_loss=0.683]\u001b[A\n"," 49%|████▉     | 110/224 [21:57\u003c23:27, 12.35s/it, val_loss=0.683]\u001b[A\n"," 49%|████▉     | 110/224 [22:09\u003c23:27, 12.35s/it, val_loss=0.634]\u001b[A\n"," 50%|████▉     | 111/224 [22:09\u003c22:46, 12.09s/it, val_loss=0.634]\u001b[A\n"," 50%|████▉     | 111/224 [22:23\u003c22:46, 12.09s/it, val_loss=0.481]\u001b[A\n"," 50%|█████     | 112/224 [22:23\u003c23:28, 12.58s/it, val_loss=0.481]\u001b[A\n"," 50%|█████     | 112/224 [22:35\u003c23:28, 12.58s/it, val_loss=0.71] \u001b[A\n"," 50%|█████     | 113/224 [22:35\u003c23:21, 12.62s/it, val_loss=0.71]\u001b[A"]}],"source":["# =====================\n","# Main\n","# =====================\n","\n","# setup\n","cfg = setup(Config)\n","\n","import transformers\n","from transformers import AutoConfig, AutoModel, AutoTokenizer\n","from transformers import AdamW, get_cosine_schedule_with_warmup\n","import tokenizers\n","import sentencepiece\n","%env TOKENIZERS_PARALLELISM=true\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","\n","# main\n","train = pd.read_csv(os.path.join(cfg.INPUT, 'train_full.csv'))\n","test = pd.read_csv(os.path.join(cfg.INPUT, 'test.csv'))\n","sub = pd.read_csv(os.path.join(cfg.INPUT, 'sample_submission.csv'))\n","\n","cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL_PATH)\n","cfg.tokenizer.save_pretrained(os.path.join(cfg.OUTPUT_EXP, 'tokenizer'))\n","train['label'] = train['discourse_effectiveness'].map({'Ineffective':0, 'Adequate':1, 'Effective':2})\n","train['type_label'] = train['discourse_type'] + '_' + train['discourse_effectiveness']\n","train, cfg.folds = get_groupstratifiedkfold(train, 'type_label', 'essay_id', cfg.num_fold, cfg.seed)\n","cfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'))\n","\n","train = preprocess_df(train, cfg.tokenizer, max_length=198, total_max_length=1024)\n","display(train.head())\n","score = training(cfg, train)\n","\n","if cfg.upload_from_colab and cfg.COLAB:\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","    dataset_create_new(dataset_name=Config.EXP, upload_dir=Config.OUTPUT_EXP)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PwTIVNWuwErN"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP6WONxN5lLLUPQCyXQVFlO","background_execution":"on","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"16c_PSkctEMBMLSiJBt5lcUyW8HaWheaJ","name":"exp042_essay.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}